{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import time\n",
    "from torch_scatter import scatter_max, scatter_mean, scatter_std\n",
    "from sklearn.model_selection import KFold\n",
    "import torchmetrics\n",
    "import lightning as L\n",
    "from torch_geometric.data import Batch, Data\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "from transformers import AutoModel, DebertaV2Tokenizer, AutoTokenizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_vocab = {v:k for k,v in tokenizer.vocab.items()}\n",
    "all_vocab_indices = list(id_vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# all_vocab_str = []\n",
    "# vector_keys = list(nlp.vocab.vectors.keys())\n",
    "# for i in range(len(vector_keys)):\n",
    "#     try:\n",
    "#         t = nlp.vocab.strings[vector_keys[i]]\n",
    "#         all_vocab_str.append(t)\n",
    "#     except:\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# Create a set of all English characters, numbers, and punctuation\n",
    "allowed_chars = string.ascii_letters + string.digits + string.punctuation\n",
    "all_chars = set(allowed_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'Data\\ReducedEmbeddings\\deberta_larg_reduced_embeddings_64.npy', 'rb') as f:\n",
    "    embeddings = np.load(f)\n",
    "embeddings = torch.from_numpy(embeddings)\n",
    "all_vocab_str = []\n",
    "for i in range(len(id_vocab)):\n",
    "    all_vocab_str.append(id_vocab[i])\n",
    "    \n",
    "# embeddings = (embeddings - torch.min(embeddings)) / (torch.max(embeddings)-torch.min(embeddings))\n",
    "token_vocab_dict = dict(zip(all_vocab_str, embeddings))\n",
    "# del all_vocab_str\n",
    "# del all_vocab_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(r'Data\\ReducedEmbeddings\\polarity_debertav3_tokens_gpt_mini_emb.npy', 'rb') as f:\n",
    "    polarities_subjectivities= np.load(f)\n",
    "polarities_subjectivities = torch.from_numpy(polarities_subjectivities)\n",
    "polarity_vocab_dict = dict(zip(all_vocab_str, polarities_subjectivities))\n",
    "polarity_vocab_dict['<n>'] = torch.tensor([0.0, 0.0])\n",
    "len(token_vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128001, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarities_subjectivities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1085 tensor([0.7000, 0.6000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_vocab_str)):\n",
    "    if 'nice' in all_vocab_str[i]:\n",
    "        print(i, polarities_subjectivities[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# Create a set of all English characters, numbers, and punctuation\n",
    "allowed_chars = string.ascii_letters + string.digits + string.punctuation + ' \\t'\n",
    "all_chars = set(allowed_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 256\n",
    "folder_path = r'C:\\Users\\fardin\\Projects\\CGNet\\Data\\TextClassification\\IMDB'\n",
    "# t_tokenizer = TweetTokenizer()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_ratio = 1.0\n",
    "test_df = pd.read_csv(r'data\\TextClassification\\IMDB\\test.csv')\n",
    "test_df['Topic'] = test_df['label']\n",
    "# test_df['Content'] = [test_df.text[i].values[0] + \" \\n \" + test_df.text[i].values[1] for i in range(len(test_df))]\n",
    "test_df['Content'] = test_df['text']\n",
    "# test_df['Content'] = test_df['Content']\n",
    "test_df.drop(['label', 'text'], axis=1, inplace=True)\n",
    "test_df.dropna(inplace=True)\n",
    "test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "test_df = test_df.iloc[:int(keep_ratio*test_df.shape[0])]\n",
    "train_df = pd.read_csv(r'data\\TextClassification\\IMDB\\train.csv')\n",
    "train_df['Topic'] = train_df['label']\n",
    "# train_df['Content'] = [train_df.text[i].values[0] + \" \\n \" + train_df.text[i].values[1] for i in range(len(train_df))]\n",
    "train_df['Content'] = train_df['text']\n",
    "train_df.drop(['label', 'text'], axis=1, inplace=True)\n",
    "train_df.dropna(inplace=True)\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "train_df = train_df.iloc[:int(keep_ratio*train_df.shape[0])]\n",
    "sst_classes = [\"Negative\", \"Positive\"]\n",
    "df = pd.DataFrame(np.concatenate([train_df.values, test_df.values]), columns=train_df.columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13704\n",
      "CPU times: total: 1.09 s\n",
      "Wall time: 1.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_lengths = np.array([len(df.Content[i]) for i in df.index])\n",
    "print(np.max(doc_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_list = df.Topic.unique()\n",
    "class_id = {c:i for i, c in enumerate(sst_classes)}\n",
    "id_class = {i:c for i, c in enumerate(sst_classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_dict = {c:i for i, c in enumerate(allowed_chars)}\n",
    "# if '\\x01' not in vocab_dict:\n",
    "#     vocab_dict['\\x01'] = len(vocab_dict)\n",
    "# char_Set = set(vocab_dict.keys())\n",
    "# len(char_Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13704\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a histogram\n",
    "plt.hist(doc_lengths, bins='auto', edgecolor='black')\n",
    "plt.xlim([0,14000])\n",
    "# Add labels and title\n",
    "plt.xlabel('String Lengths in Character')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of String Lengths')\n",
    "print(doc_lengths.max())\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 31s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_token_lengths = np.array([len(tokenizer.tokenize(df.Content[i])) for i in df.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3088\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a histogram\n",
    "plt.hist(doc_token_lengths, bins='auto', edgecolor='black')\n",
    "plt.xlim([0,2000])\n",
    "# Add labels and title\n",
    "plt.xlabel('String Lengths in Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of String Lengths')\n",
    "print(doc_token_lengths.max())\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'too'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'▁Too'.strip('▁').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "vocabs_lists = list(token_vocab_dict.keys())\n",
    "term_frequencies = {t:1 for t in vocabs_lists}\n",
    "temp_term_frequencies = {}\n",
    "\n",
    "char_set = set()\n",
    "for doc in train_df.Content.values:\n",
    "    tokens_list = tokenizer.tokenize(doc)\n",
    "    char_set.update(set(' '.join(tokens_list)))\n",
    "    char_set.update(set(doc))\n",
    "    new_tokens = {t.strip('▁').lower() for t in tokens_list}\n",
    "    for t in new_tokens:\n",
    "        if t not in temp_term_frequencies:\n",
    "            temp_term_frequencies[t] = 0\n",
    "        temp_term_frequencies[t] += 1\n",
    "        \n",
    "# stripped_key_pairs = {}\n",
    "# stripped_key_frequencies = {}\n",
    "# for k in vocabs_lists:\n",
    "#     stripped_k = k.strip('▁').lower()\n",
    "#     stripped_key_pairs[k] = stripped_k\n",
    "    # if stripped_k not in stripped_key_frequencies:\n",
    "    #     stripped_key_frequencies[stripped_k] = 0\n",
    "    # stripped_key_frequencies[stripped_k] += v\n",
    "    \n",
    "for k, v in term_frequencies.items():\n",
    "    stripped_token = k.strip('▁').lower()\n",
    "    term_frequencies[k] = temp_term_frequencies[stripped_token] if stripped_token in temp_term_frequencies else 1\n",
    "    # token_frequencies[k] = temp_token_frequencies[stripped_key_pairs[k]]\n",
    "    \n",
    "# del stripped_key_pairs\n",
    "# del stripped_key_frequencies\n",
    "del temp_term_frequencies\n",
    "    \n",
    "for doc in test_df.Content.values:\n",
    "    char_set.update(set(' '.join(tokenizer.tokenize(doc))))\n",
    "    char_set.update(set(doc))\n",
    "len(char_set)\n",
    "\n",
    "allowed_chars = string.ascii_letters + string.digits + string.punctuation + ' '\n",
    "all_chars = set(''.join(char_set).join(allowed_chars))\n",
    "vocab_dict = {c:i for i, c in enumerate(all_chars)}\n",
    "if '\\x01' not in vocab_dict:\n",
    "    vocab_dict['\\x01'] = len(vocab_dict)\n",
    "char_Set = set(vocab_dict.keys())\n",
    "num_embedding = len(vocab_dict)\n",
    "vocab_dict_rev = dict(zip(list(vocab_dict.values()), list(vocab_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def subsampling_equation_sigmoid(x: torch.Tensor):\n",
    "#     x = 0.9*F.sigmoid(0.06*(70-x))\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tensor(1), tensor(3), tensor(4), tensor(4), tensor(5)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(torch.tensor([1,4,3,5,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.00001\n",
    "total_token_count = np.array(list(term_frequencies.values())).sum()\n",
    "one_tensor = torch.tensor(1)\n",
    "def subsampling_equation_linear(x: torch.Tensor):\n",
    "    f_x = x/total_token_count\n",
    "    x = torch.min(one_tensor, torch.sqrt_(threshold/f_x))\n",
    "    return x\n",
    "\n",
    "def subsampling_equation_sigmoid(x: torch.Tensor):\n",
    "    f_x = x/total_token_count\n",
    "    x = 1-0.95*F.sigmoid(0.05*((f_x/threshold)-90))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0365597667011765e-05"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_token_count\n",
    "676/total_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsample: tensor([0.9262])\n"
     ]
    }
   ],
   "source": [
    "print(f'subsample: {subsampling_equation_sigmoid(torch.from_numpy(np.array([term_frequencies[\"were\"]])))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "676"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_frequencies[\"▁is\"]\n",
    "term_frequencies[\"king\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsample: tensor([0.9873])\n"
     ]
    }
   ],
   "source": [
    "print(f'subsample: {subsampling_equation_sigmoid(torch.from_numpy(np.array([term_frequencies[\"king\"]])))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterandTokenLevelCustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, num_classes, char_dict, token_dict, sentiment_dict, tokenizer, token_frequencies, sampling_equation, shuffle=True, batch_size=128) -> None:\n",
    "        super().__init__()\n",
    "        # y = y[indices].values\n",
    "        # y = torch.from_numpy(np.array([class_id[c] for c in y], dtype=np.longlong))\n",
    "        \n",
    "        # print(f'self.num_sections1: {len(y) // batch_size}')\n",
    "        # self.sampling_equation = sampling_equation\n",
    "        if len(y) % batch_size != 0:\n",
    "            self.shortage = ((len(y) // batch_size)+1)*batch_size - len(y)\n",
    "            empty_labels = [i%2 for i in range(self.shortage)]\n",
    "            empty_strings = [id_class[l] for l in empty_labels]\n",
    "            \n",
    "            # print(f'y1 - {y.shape}: {y}')\n",
    "            y = np.concatenate([y, empty_labels])\n",
    "            # print(f'y2 - {y.shape}: {y}')\n",
    "            # print(f'X1 - {X.shape}: {X}')\n",
    "            X = np.concatenate([X, empty_strings])\n",
    "        #     print(f'X2 - {X.shape}: {X}')\n",
    "        \n",
    "        # print(f'self.num_sections2: {len(y) // batch_size}')\n",
    "        \n",
    "        y = torch.from_numpy(y)\n",
    "        self.shuffle = shuffle\n",
    "        self.y = torch.nn.functional.one_hot(y, num_classes=num_classes).float()\n",
    "        self.X = X\n",
    "        self.char_dict = char_dict\n",
    "        self.char_Set = set(char_dict.keys())\n",
    "        self.vocab_size = len(self.char_dict)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_dict = token_dict\n",
    "        self.sentiment_dict = sentiment_dict\n",
    "        self.token_frequencies = token_frequencies\n",
    "        self.max_token_count = 0\n",
    "        \n",
    "        \n",
    "        self.all_data = []\n",
    "        self.token_lengths = []\n",
    "        self.token_embeddign_ids = []\n",
    "        \n",
    "        self.sum_a = 0\n",
    "        \n",
    "        for doc in tqdm(self.X):\n",
    "            g_data = self.content_to_graph(doc, sampling_equation)\n",
    "            self.all_data.append(g_data)\n",
    "        \n",
    "        \n",
    "        self.num_sections = len(y) // batch_size\n",
    "        self.x_lengths = np.array([self.all_data[i].character_length for i in range(len(self.all_data))])\n",
    "        self.x_len_args = np.argsort(self.x_lengths)[::-1]\n",
    "        \n",
    "        self.section_ranges = np.linspace(0, len(self.x_len_args), self.num_sections+1)\n",
    "        self.section_ranges = [(int(self.section_ranges[i-1]), int(self.section_ranges[i])) for i in range(1, len(self.section_ranges))]\n",
    "\n",
    "        self.position_j = 0\n",
    "        self.section_i = 0\n",
    "        self.epoch = 0\n",
    "        self.each_section_i = np.zeros((self.num_sections, ), dtype=int)\n",
    "        \n",
    "        self.sections, self.section_size = self.split_into_k_groups(self.x_len_args, self.x_lengths, self.num_sections)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        index = self.get_section_index()\n",
    "        return self.all_data[index], self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def get_section_index(self):\n",
    "        # if self.shuffle:\n",
    "            \n",
    "        #     t_range = self.section_ranges[self.section_i]\n",
    "        #     target_index = np.random.randint(t_range[0], t_range[1])\n",
    "        # else:\n",
    "        #     t_range = self.section_ranges[self.section_i]\n",
    "        #     target_index = t_range[0] + self.each_section_i[self.section_i]\n",
    "        #     self.each_section_i[self.section_i] = (self.each_section_i[self.section_i] + 1) % (t_range[1] - t_range[0])\n",
    "        # print()\n",
    "        # print(f'self.section_i: {self.section_i},   self.position_j: {self.position_j}')\n",
    "        target_index = self.sections[self.section_i, self.position_j]\n",
    "        \n",
    "        self.position_j = (self.position_j + 1) % self.section_size\n",
    "        if self.position_j == 0:\n",
    "            self.section_i = (self.section_i + 1) % self.num_sections\n",
    "            if self.shuffle and self.section_i == 0:\n",
    "                self.sections, self.section_size = self.split_into_k_groups(self.x_len_args, self.x_lengths, self.num_sections)\n",
    "                # random_positions = np.random.choice(np.arange(0, self.section_size), size=self.section_size, replace=False)\n",
    "        # return self.x_len_args[target_index]\n",
    "        return target_index\n",
    "\n",
    "    def reset_params(self):\n",
    "        self.section_i = 0\n",
    "        self.position_j = 0\n",
    "        self.each_section_i = np.zeros((self.num_sections, ), dtype=int)\n",
    "        \n",
    "    def split_into_k_groups(self, len_sorted_args, lengths:np.array, k):\n",
    "        if self.shuffle and self.epoch > 0:\n",
    "            randomize_sections = np.concatenate([np.random.choice(np.arange(r[0], r[1]), size=r[1]-r[0], replace=False) for r in self.section_ranges])\n",
    "            len_sorted_args = len_sorted_args[randomize_sections]\n",
    "        \n",
    "        nums = lengths[len_sorted_args]\n",
    "        groups_size = len(len_sorted_args) // k\n",
    "        \n",
    "        \n",
    "        groups = [[] for _ in range(k)]\n",
    "        group_sums = np.zeros(k, dtype=int)\n",
    "        group_sizes = np.zeros(k, dtype=int)\n",
    "        \n",
    "        # print(f'groups_size: {groups_size}')\n",
    "        # print(f'len(len_sorted_args): {len(len_sorted_args)}')\n",
    "        # print(f'k: {k}')\n",
    "        for i, num in enumerate(nums):\n",
    "            candidate_indices = np.where(group_sizes<groups_size)[0]\n",
    "            # print(f'candidate_indices: {candidate_indices}')\n",
    "            min_group_idx = candidate_indices[np.argmin(group_sums[candidate_indices])]\n",
    "            groups[min_group_idx].append(len_sorted_args[i])\n",
    "            group_sums[min_group_idx] += num\n",
    "            group_sizes[min_group_idx] += 1\n",
    "        self.epoch += 1\n",
    "        \n",
    "        groups = np.array(groups)\n",
    "        group_sums_argsort = np.argsort(group_sums)[::-1]\n",
    "        groups = groups[group_sums_argsort]\n",
    "        \n",
    "        # check_x = self.X[groups]\n",
    "        # check_x_lens = [np.sum(np.array([len(sx) for sx in rx])) for rx in check_x]\n",
    "        # print(f'check_x: {check_x}')\n",
    "        \n",
    "        \n",
    "        return np.array(groups), groups_size\n",
    "        \n",
    "    def content_to_graph(self, doc, sampling_equation):\n",
    "        # tokens = self.tokenizer(''.join(c for c in doc if c in self.char_Set))\n",
    "        # tokens = [t.text for t in tokens]\n",
    "        tokens = self.tokenizer(doc)\n",
    "        if len(tokens) == 0:\n",
    "            tokens = ['empty']\n",
    "                        \n",
    "        token_lengths = [len(t) for t in tokens]\n",
    "        tokens.append('\\x01')\n",
    "        \n",
    "        token_lengths.append(len(tokens[-1])-1)\n",
    "        token_lengths = torch.from_numpy(np.array(token_lengths, dtype=np.longlong))+1\n",
    "        token_embs = [self.token_dict[t] if t in self.token_dict else torch.zeros((64, ), dtype=torch.float32) for t in tokens]\n",
    "        token_sentiments = [self.sentiment_dict[t] if t in self.sentiment_dict else (0.0, 0.0) for t in tokens]\n",
    "        token_embs = torch.from_numpy(np.array(token_embs, dtype=np.float32))\n",
    "        token_sentiments = torch.from_numpy(np.array(token_sentiments, dtype=np.float32))\n",
    "        doc = ' '.join(tokens)\n",
    "        characters = torch.from_numpy(np.array([self.char_dict[t] for t in doc], dtype=np.longlong))\n",
    "        token_positions = torch.arange(len(token_lengths), dtype=torch.long)\n",
    "        token_indices = torch.repeat_interleave(token_positions, token_lengths)\n",
    "        token_subsampling_probabilities = sampling_equation(torch.from_numpy(np.array([self.token_frequencies[t] if t in self.token_frequencies else 1 for t in tokens])))\n",
    "        num_tokens = len(token_lengths)\n",
    "        if num_tokens > self.max_token_count:\n",
    "            self.max_token_count = num_tokens\n",
    "        g_data = Data(x=characters,\n",
    "                        token_positions=token_positions,\n",
    "                        character_length = len(characters),\n",
    "                        num_tokens = num_tokens,\n",
    "                        token_indices=token_indices,\n",
    "                        token_lengths=token_lengths,\n",
    "                        token_embeddings=token_embs,\n",
    "                        token_sentiments=token_sentiments,\n",
    "                        token_subsampling_probabilities=token_subsampling_probabilities)\n",
    "        return g_data\n",
    " \n",
    "    def caluculate_batch_token_positions(self, num_tokens, character_length, token_indices):\n",
    "        cumsum_vals = torch.cumsum(num_tokens, dim=0).roll(1)\n",
    "        cumsum_vals[0] = 0\n",
    "        additions = torch.repeat_interleave(cumsum_vals, character_length)\n",
    "        cumulative_token_indices = token_indices + additions\n",
    "        return cumulative_token_indices       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "class CharacterandTokenLevelDataLoader(DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size: int = 1,\n",
    "        shuffle: bool = False,\n",
    "        follow_batch: List[str] | None = None,\n",
    "        exclude_keys: List[str] | None = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(CharacterandTokenLevelDataLoader, self).__init__(\n",
    "            dataset, batch_size, shuffle, follow_batch, exclude_keys, **kwargs\n",
    "        )\n",
    "\n",
    "    def __iter__(self):\n",
    "        base_iterator = super(CharacterandTokenLevelDataLoader, self).__iter__()\n",
    "        for batch in base_iterator:\n",
    "            cumsum_vals = torch.cumsum(batch[0].num_tokens, dim=0).roll(1)\n",
    "            cumsum_vals[0] = 0\n",
    "            additions = torch.repeat_interleave(cumsum_vals, batch[0].character_length)\n",
    "            batch[0].cumulative_token_indices = batch[0].token_indices + additions\n",
    "            yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(df.Content.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3760, 1.2731, 3.5556, 8.2565, 3.0525, 4.3947, 8.9001, 0.7448, 9.5223,\n",
      "        6.1687, 9.8460, 1.3974, 8.8606, 4.4414, 6.8289, 1.4518, 0.5102, 0.1452,\n",
      "        9.4172, 3.9890, 1.9018, 4.3660, 0.6702, 0.1788, 9.1955, 2.1227, 9.5538,\n",
      "        9.0033, 5.3526, 9.5499, 2.9036, 7.0387, 6.2835, 2.4924, 9.0960, 3.7956,\n",
      "        5.2009, 8.1702, 5.5275, 7.7821, 7.1771, 5.1397, 2.7790, 3.0315, 2.3583,\n",
      "        3.9104, 2.5695, 4.6973, 4.8258, 7.7537, 6.9126, 2.6768, 6.3562, 2.2382,\n",
      "        5.5057, 5.4118, 4.6455, 4.7651, 5.3609, 6.1031, 4.4532, 2.6402, 3.9951,\n",
      "        2.2412])\n"
     ]
    }
   ],
   "source": [
    "print(token_vocab_dict['charm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6000, 0.7000], dtype=torch.float64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_vocab_dict['charm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 54/25088 [00:01<07:42, 54.14it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25088/25088 [06:37<00:00, 63.11it/s]\n",
      "100%|██████████| 25088/25088 [06:37<00:00, 63.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 50min 32s\n",
      "Wall time: 13min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_dataset = CharacterandTokenLevelCustomDataset(train_df.Content.values, train_df.Topic.values, len(class_id), vocab_dict, token_vocab_dict, polarity_vocab_dict, tokenizer.tokenize, token_frequencies=term_frequencies, sampling_equation=subsampling_equation_sigmoid, batch_size=batch_size)\n",
    "test_dataset = CharacterandTokenLevelCustomDataset(test_df.Content.values, test_df.Topic.values, len(class_id), vocab_dict, token_vocab_dict, polarity_vocab_dict, tokenizer.tokenize, token_frequencies=term_frequencies, sampling_equation=subsampling_equation_sigmoid, batch_size=batch_size)\n",
    "max_token_count = max(train_dataset.max_token_count, test_dataset.max_token_count)\n",
    "train_dataloader = CharacterandTokenLevelDataLoader(train_dataset, batch_size=batch_size, drop_last=False, shuffle=True)\n",
    "test_dataloader = CharacterandTokenLevelDataLoader(test_dataset, batch_size=batch_size, drop_last=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_lengths = np.array([train_dataset[i][0].num_tokens for i in range(len(train_dataset))])\n",
    "# test_lengths = np.array([test_dataset[i][0].num_tokens for i in range(len(test_dataset))])\n",
    "# print(np.max(train_lengths))\n",
    "# print(np.max(test_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[7584], token_positions=[1367], character_length=[1], num_tokens=[1], token_indices=[7584], token_lengths=[1367], token_embeddings=[1367, 64], token_sentiments=[1367, 2], token_subsampling_probabilities=[1367])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁\" The ▁belief ▁in ▁'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([vocab_dict_rev[t.item()] for t in X[0].x[:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7500, 0.0994, 0.9890,  ..., 0.9796, 0.0977, 0.9896])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].token_subsampling_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Topic                                                      0\n",
       "Content    So I finally saw the film \"My Left Foot\" last ...\n",
       "Name: 15926, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[np.argwhere(['hardy' in c and 'obscure' in c for c in  df.Content.values]).squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ▁* * * Tip : ▁Have ▁It ▁Read ▁To ▁You , ▁Here s ▁How * * * < br ▁/ > < br ▁/ > 1 ) ▁Copy ▁And ▁Paste ▁This ▁To ▁Notepad ▁( NOT ▁WORD ) ▁2 ) ▁Go ▁To . ▁START > ALL ▁PROGRAMS > ACC ESS O RIES > ACC ESSA BIL TY > NAR RATOR < br ▁/ > < br ▁/ > having ▁your ▁testicles ▁ironed . < br ▁/ > < br ▁/ > When ▁Jonathan ▁Ross ▁started ▁his ▁career ▁he ▁was ▁on ▁a ▁show ▁call ▁\" The ▁Last ▁Resort \" ▁now ▁a ▁days ▁he ▁is ▁the ▁first ▁resort ▁to ▁host ▁anything ▁and ▁anything . ▁TV ▁Award ▁Shows ▁that ▁half ▁the ▁time ▁he ▁is ▁up ▁for ▁nominations ▁in , ▁Comic ▁relief , ▁chat ▁shows , ▁quiz ▁shows , ▁game ▁shows , ▁charity ▁shows , ▁Brighton . ▁Just ▁when ▁you ▁at ▁you ▁wits ▁end ▁and ▁think ▁you ▁can ▁find ▁salvation ▁in ▁the ▁wireless ▁the ▁lisp ing ▁twang ▁of ▁good ▁old ▁J . R . ▁Hits ▁you ▁like ▁a ▁freight ▁train ▁going ▁none ▁stop ▁from ▁Texas ▁to ▁downtown ▁N . Y . ▁That ▁has ▁lost ▁a ▁hour ▁and ▁is ▁trying ▁to ▁make ▁it ▁up . < br ▁/ > < br ▁/ > About ▁this ▁show ▁( F NW JR ) . < br ▁/ > < br ▁/ > It s ▁a ▁normal ▁chat ▁show ▁format ▁with ▁J . R . ▁As ▁host ▁and ▁a ▁house ▁band ▁that ▁concise st ▁of ▁four ▁gay ▁men ▁( ha ▁ha ▁ha , ▁ow ▁my ▁aching ▁sides . ) ▁and ▁season ▁one ▁had ▁Andy ▁Davis , ▁but ▁he ▁left ▁or ▁was ▁fired ▁to ▁give ▁way ▁to ▁Ross ' s ▁Ego . < br ▁/ > < br ▁/ > Ross ▁will ▁more ▁less ▁use ▁his ▁guests ▁as ▁props ▁and ▁you ▁really ▁don ' t ▁hear ▁them ▁speak ▁because ▁of ▁his ▁\" It s ▁my ▁ball ▁and ▁I ' ll ▁take ▁it ▁home \" ▁attitude , ▁you ▁also ▁see ▁that ▁the ▁bigger ▁the ▁guest ▁the ▁more ▁he ▁is ▁willing ▁to ▁lie ▁and ▁suck ▁up ▁to ▁them , ▁to ▁get ▁in ▁with ▁the ▁big ▁boys ▁( Like ▁the ▁weak ▁kid ▁at ▁school ▁who ▁hangs ▁round ▁with ▁the ▁bully ) . < br ▁/ > < br ▁/ > However ▁when ▁a ▁small ▁reality ▁T . V . ▁Star ▁comes ▁on ▁he ' ll ▁happily ▁humiliate ▁them , ▁asking ▁personal ▁questions ▁about ▁the ▁past ▁and ▁telling ▁them ▁about ▁their ▁lack ▁of ▁talent ▁to ▁get ▁the ▁laughs . ▁Sometimes ▁he ▁will ▁under ▁estimate ▁the ▁popularity ▁of ▁a ▁guest , ▁say ▁something ▁to ▁belittle ▁them ▁and ▁then ▁when ▁the ▁audience ▁act ▁shocked , ▁he ▁will ▁quickly ▁turn ▁and ▁start ▁making ▁himself ▁the ▁fall ▁guy , ▁the ▁best ▁example ▁of ▁this ▁was ▁when ▁\" Life ▁On ▁Mars \" ▁star ▁John ▁Sim m ▁came ▁on ▁and ▁he ▁said ▁how ▁does ▁someone ▁like ▁you ▁get ▁work , ▁your ▁OK ▁looking ▁but ▁not ▁Hollywood ▁good - looking ▁( Ba re ▁in ▁mind ▁the ▁Hugh ▁Jackman ▁and ▁Halle ▁Berry ▁was ▁in ▁the ▁green ▁room , ▁he ▁was ▁really ▁only ▁trying ▁to ▁suck ▁up ▁to ▁them ▁before ▁they ▁were ▁even ▁on ▁the ▁couch ) . ▁When ▁the ▁audience ▁acted ▁shock ▁Ross ▁quickly ▁said ▁\" What , ▁I ' m ▁bit ▁light ▁headed ▁from ▁wearing ▁that ▁corset , ▁I ▁don ' t ▁know ▁what ▁I ' m ▁saying \" . ▁If ▁he ▁don ' t ▁have ▁any ▁low ▁forms ▁of ▁TV ▁life ▁on ▁he ' ll ▁just ▁dig ▁at ▁the ▁four ▁gay ▁men ▁on ▁the ▁piano ▁with ▁jokes ▁more ▁out ▁of ▁date ▁than ▁his ▁fashion . < br ▁/ > < br ▁/ > It s ▁very ▁much ▁a ▁different ▁story ▁when ▁a ▁Hollywood ▁A - lister ▁or ▁big ▁TV ▁star ▁comes ▁on ▁the ▁show ▁in ▁that ▁he ' ll ▁tell ▁them ▁stories ▁to ▁humor ▁them . ▁When ▁some ▁actor ▁explains ▁that ▁he ▁was ▁in ▁a ▁support ▁band ▁then ▁Jonathon ▁Ross ▁will ▁say ▁something ▁like ▁\" Wow , ▁well ▁he ▁ever ▁I ▁go ▁to ▁see ▁a ▁band ▁i ▁was ▁try ▁to ▁look ▁interested ▁for ▁the ▁support ▁band , ▁to ▁make ▁them ▁feel ▁as ▁though ▁they ▁are ▁wanted \" ▁with ▁an ▁underline ▁message ▁being ▁\" please ▁like ▁me , ▁I ▁was ▁probably ▁one ▁of ▁the ▁people ▁that ▁cheered ▁you ▁when ▁you ▁was ▁in ▁your ▁band \" . ▁Top ▁this ▁off ▁with ▁an ▁audience ▁of ▁Ross ▁fans ▁so ▁hooked ▁on ▁every ▁bad ▁old ▁joke ▁and ▁bull ing , ▁it ▁really ▁makes ▁for ▁a ▁poor ▁show . < br ▁/ > < br ▁/ > Your ▁better ▁off ▁watching ▁US ▁chat ▁shows ▁instead , ▁they ▁are ▁more ▁scripted ▁but ▁not ▁anywhere ▁near ▁as ▁hard ▁to ▁watch . \u0001\n",
      "0 ▁Once ▁again , ▁I ' ve ▁been ▁duped ▁by ▁seemingly ▁intelligent ▁reviews ▁making ▁seemingly ▁intelligent ▁comments ▁about ▁an ▁obviously ▁crappy ▁movie . ▁I ▁actually ▁put ▁my ▁shoes ▁on , ▁got ▁in ▁my ▁car , ▁burned ▁expensive ▁gasoline ▁and ▁drove ▁to ▁the ▁nearest ▁rental ▁place ▁AFTER ▁reading ▁said ▁reviews ▁and ▁paid ▁the ▁requisite ▁4 ▁dollars ▁and ▁change ▁to ▁rent ▁this ▁thing . ▁I ' m ▁telling ▁you , ▁this ▁one ' s ▁not ▁worth ▁the ▁minuscule ▁kilo - calorie s ▁spent ▁on ▁lifting ▁one ' s ▁index ▁finger ▁to ▁switch ▁channels ▁on ▁a ▁TV ▁remote . ▁< br ▁/ > < br ▁/ > I ▁even ▁gave ▁it ▁a ▁few ▁more ▁minutes ▁after ▁seeing ▁all ▁the ▁tell - tale ▁signs ▁of ▁a ▁pedigree ▁dog - pile . ▁These ▁presented ▁as ▁clinical ▁symptoms ▁of ▁a ▁director ▁who ▁is ▁a . ▁going ▁senile ▁or , ▁b . ▁is ▁only ▁marginally ▁interested ▁in ▁the ▁film ▁he / she ▁is ▁obligated ▁to ▁create . ▁I ▁saw ▁similar ▁deterioration ▁with ▁John ▁Carpenter ' s ▁string ▁of ▁ridiculous ▁caricature ' s ▁over ▁the ▁past ▁number ▁of ▁years . < br ▁/ > < br ▁/ > Here ▁are ▁a ▁couple ▁of ▁scenes ▁as ▁incriminating ▁evidence . ▁The ▁priest ▁is ▁having ▁a ▁disturbing ▁dream . . . supposedly ▁a ▁harbinger ▁of ▁nastiness ▁to ▁come ▁since ▁he ▁seems ▁hell ▁bent ▁on ▁opening ▁the ▁archaeological ▁feature ▁which ▁houses ▁the ▁demon . ▁The ▁dream ▁is ▁a ▁goofy ▁collage ▁of ▁disjointed ▁images ▁right ▁out ▁of ▁the ▁Twilight ▁Zone ' s ▁stock ▁footage . ▁A ▁ticking ▁clock ▁care ens ▁through ▁the ▁dream ▁scape ' s ▁blackness ▁implying , ▁what ? , ▁the ▁unfathomable ▁mystery ▁of ▁Time ? . . . . big ▁deal ! ▁A ▁disembodied ▁head , ▁painted ▁in ▁demon ▁features ▁with ▁convenience ▁store ▁quality ▁Halloween ▁make - up , ▁flicker s ▁back ▁and ▁forth ▁in ▁a ▁convulsive ▁frenzy . ▁Every ▁time ▁I ▁see ▁this ▁effect , ▁a ▁big ▁fat ▁rip - off ▁from ▁Jacob ' s ▁Ladder , ▁it ▁pisses ▁me ▁off . ▁This , ▁in ▁itself , ▁almost ▁instantly ▁discredit s ▁a ▁film . ▁< br ▁/ > < br ▁/ > The ▁whole ▁build - up ▁of ▁the ▁archaeological ▁dig ▁itself ▁is ▁laughable . ▁Everything ▁is ▁so ▁obvious . . . so ▁tired ▁and ▁over - wrought . . . the ▁only ▁possible ▁response ▁is ▁boredom . ▁At ▁one ▁point ▁in ▁the ▁dig , ▁the ▁priest ▁comments ▁on ▁finding ▁the ▁statues ▁of ▁Angels ▁surrounding ▁a ▁sarcophagus . . . they ' re ▁all ▁pointing ▁down ▁toward ▁the ▁crypt ▁with ▁their ▁weapons . ▁He ▁queries ▁\" Look ▁at ▁these ▁surrounding ▁statues . . . . It ' s ▁as ▁if ▁they ▁are ▁holding . . something . . down ! \" ▁This ▁is ▁supposed ▁to ▁build ▁tension . . . critical ▁mass . . but ▁it ▁doesn ' t ▁even ▁come ▁close ! ▁How ▁can ▁there ▁be ▁suspense ▁if ▁you ▁treat ▁the ▁audience ▁like ▁a ▁bunch ▁of ▁morons ▁having ▁to ▁EXPLAIN ▁the ▁suspense ▁as ▁you ▁go ▁along . ▁The ▁imagery ▁is ▁over - done ▁in ▁the ▁first ▁place ▁but ▁the ▁added ▁comments ▁only ▁add ▁insult ▁to ▁injury ▁in ▁my ▁opinion . ▁Soon ▁thereafter , ▁the ▁tomb ▁is ▁\" decorated \" ▁with ▁the ▁remains ▁of ▁the ▁soldiers ▁placed ▁there ▁to ▁guard ▁the ▁main ▁atrium ▁( another ▁shameless ▁rip - off ▁of ▁The ▁Keep , ▁btw ) . ▁Who , ▁for ▁crying ▁out ▁loud , ▁did ▁the ▁make - up ▁effects ▁for ▁this ▁film ? ? ! ▁The ▁blood ▁actually ▁had ▁that ▁pinkish ▁quality ▁one ▁might ▁see ▁in ▁70 ' s ▁T roma ville ▁flicks . ▁At ▁this ▁point ▁I ▁became ▁almost ▁convinced ▁that ▁they ▁simply ▁forgot ▁the ▁make - up ▁and ▁had ▁to ▁go ▁to ▁Wal - Mart ▁in ▁the ▁interest ▁of ▁time ▁and ▁money . < br ▁/ > < br ▁/ > DON ' T ▁listen ▁to ▁glowing ▁comments ▁on ▁this ▁one ! ▁I ' ll ▁be ▁keeping ▁a ▁suspicious ▁eye ▁on ▁Schrader ▁too . ▁Looks ▁like ▁it ▁might ▁be ▁time ▁to ▁hang ▁up ▁his ▁gloves . ▁Perhaps ▁a ▁close ▁friend ▁will ▁offer ▁a ▁gentle ▁admonition ▁to ▁quit ▁while ▁there ' s ▁still ▁dignity ▁in ▁memory ▁of ▁films ▁gone ▁by . \u0001\n",
      "0 ▁Like ▁many ▁people ▁in ▁my ▁general ▁age ▁range , ▁I ▁remember ▁going ▁to ▁see ▁this ▁movie ▁as ▁a ▁kid ▁in ▁' 98 ▁and ▁coming ▁out ▁of ▁the ▁theatre ▁practically ▁in ▁tears . ▁It ▁seemed , ▁at ▁the ▁time , ▁to ▁be ▁one ▁of ▁the ▁most ▁important , ▁awe - inspiring ▁cinematic ▁experiences ▁of ▁our ▁generation . ▁At ▁once ▁riveting , ▁action - packed , ▁funny , ▁heartbreaking , ▁and ▁truly ▁inspirational , ▁Arm eg ed de on ▁really ▁did ▁have ▁everything ▁going ▁on , ▁right ▁down ▁to ▁the ▁catchy ▁Aerosmith ▁theme ▁song ▁and ▁sappy ▁tear - jerk er ▁of ▁an ▁ending . < br ▁/ > < br ▁/ > Sweet ▁Jeb us . ▁What ▁were ▁we ▁smoking ? ▁I ▁watched ▁it ▁for ▁the ▁first ▁time ▁in ▁years ▁last ▁night ▁on ▁one ▁of ▁the ▁movie ▁channels , ▁and . . . ▁I ▁cannot ▁even ▁describe ▁it . ▁This ▁is , ▁truly , ▁one ▁of ▁the ▁worst ▁movies ▁ever ▁made . ▁Where ▁to ▁even ▁begin ? ▁Leaving ▁aside ▁the ▁plethora ▁of ▁LAUGH ABLE ▁scientific ▁errors ▁( ' personnel ▁trackers ' ▁on ▁astronauts ? ▁yeah , ▁sure , ▁thanks ▁for ▁that , ▁Billy ▁Bob ) , ▁I ' d ▁have ▁to ▁say ▁the ▁worst ▁thing ▁about ▁it ▁was ▁the ▁remarkable ▁- ▁dare ▁I ▁even ▁say ▁unmatched ▁- ▁way ▁in ▁which ▁it ▁combined ▁crappy ▁writing ▁with ▁crappy ▁acting . ▁There ▁are ▁too ▁many ▁examples ▁of ▁this ▁to ▁even ▁begin ▁listing ▁here , ▁but ▁one ▁in ▁particular ▁springs ▁to ▁mind ▁- ▁the ▁scene ▁where ▁Bruce ▁Willis ▁is ▁telling ▁the ▁Fed s ▁exactly ▁where ▁to ▁go ▁to ▁track ▁down ▁each ▁of ▁the ▁oh - so - charm ingly - ro guish ▁members ▁of ▁his ▁oil ▁drilling ▁team ▁( ' check ▁every ▁bar ▁in ▁New ▁Orleans ' , ▁' the ▁craps ▁tables ▁in ▁Vegas ' , ▁' the ▁only ▁black ▁guy ▁on ▁a ▁motorcycle ▁in ▁Sturgis ' . . . ▁all ▁to ▁the ▁tune ▁of ▁' Come ▁Together ' . . . ▁it ▁reminded ▁me ▁a ▁bit ▁of ▁the ▁\" NEWS ▁TEAM ! ▁AS SEM BLE ! \" ▁scene ▁from ▁Anchor man , ▁except ▁serious ) . ▁Ben ▁Affleck ▁proves , ▁once ▁again , ▁that ▁he ▁is ▁by ▁far ▁the ▁most ▁overpaid ▁actor ▁in ▁Hollywood , ▁having ▁less ▁depth , ▁range , ▁and ▁overall ▁talent ▁than ▁anyone ▁else ▁in ▁the ▁business . ▁Not ▁that ▁Bruce ▁Willis , ▁Liv ▁Tyler , ▁OR ▁ANYONE ▁ELSE ▁IN ▁THIS ▁GOD FOR SA KEN ▁PIECE ▁OF ▁GAR B AGE ▁was ▁much ▁better . < br ▁/ > < br ▁/ > ( I ▁have ▁to ▁say , ▁though , ▁I ▁got ▁a ▁kick ▁out ▁of ▁seeing ▁a ▁pre - star dom ▁Owen ▁Wilson ▁get ▁killed ▁off ▁half - way ▁through . . . ▁is ▁this ▁the ▁only ▁movie ▁where ▁his ▁character ▁dies ? ) ▁< br ▁/ > < br ▁/ > Peter ▁Storm are ▁is ▁perfect ▁as ▁THE ▁MOST ▁STEREO TY P ICAL ▁UN SHA VEN ▁Russian ▁COSMO N AUT ▁YOU ▁HAVE ▁EVER ▁SCENE . ▁( Then ▁again , ▁Peter ▁Storm are ▁does ▁seem ▁to ▁have ▁a ▁talent ▁for ▁playing ▁over - the - top ▁Euro types . ) ▁It ▁really ▁was ▁quite ▁amusing ▁how , ▁almost ▁IMMEDIATELY ▁after ▁the ▁Americans ▁dock ▁with ▁the ▁Russian ▁Space ▁Station ▁( which ▁is ▁actually ▁called ▁that ▁in ▁the ▁movie ) , ▁Ben ▁Affleck ▁succeeds ▁in ▁singlehandedly ▁causing ▁the ▁whole ▁joint ▁to ▁explode ▁in ▁spectacular ▁Hollywood ▁fashion . ▁I ▁also ▁love ▁the ▁fact ▁that , ▁in ▁the ▁end , ▁Paris ▁is ▁the ▁only ▁place ▁on ▁Earth ▁to ▁get ▁destroyed , ▁and ▁that ▁absolutely ▁no ▁one ▁seems ▁to ▁care . ▁And ▁on ▁top ▁of ▁all ▁that , ▁it ▁at ▁points ▁literally ▁turns ▁into ▁simultaneous ▁ads ▁for ▁Lockheed ▁Martin ▁AND ▁Kerr ▁McGee . ▁Oh ▁how ▁proud ▁I ▁am ▁to ▁be ▁an ▁American . < br ▁/ > < br ▁/ > There ' s ▁plenty ▁of ▁other ▁stuff ▁to ▁rant ▁about , ▁but ▁I ▁won ' t . . . ▁suffice ▁it ▁to ▁say ▁that ▁this ▁is ▁a ▁really , ▁really , ▁REALLY ▁terrible ▁movie , ▁that ▁I ▁feel ▁ashamed ▁to ▁have ▁ever ▁genuinely ▁liked . < br ▁/ > < br ▁/ > I ▁give ▁it ▁two ▁stars ▁just ▁for ▁the ▁mock ability ▁factor . \u0001\n",
      "1 ▁I ' m ▁not ▁sure ▁why ▁I ▁picked ▁for ▁a ▁borrow ▁from ▁mom ▁for ▁\" Nurse ▁Betty \" . ▁I ▁think ▁just ▁because ▁I ▁had ▁heard ▁a ▁little ▁bit ▁of ▁this ▁movie . ▁But ▁I ' m ▁glad ▁I ▁did . ▁\" Nurse ▁Betty \" ▁is ▁an ▁original ▁and ▁clever ▁movie ▁that ▁has ▁humor ▁and ▁a ▁darker ▁side . ▁< br ▁/ > < br ▁/ > This ▁was ▁one ▁of ▁Renee ' s ▁first ▁big ▁one ' s ▁before ▁hitting ▁it ▁major ▁in ▁Hollywood . ▁I ▁can ▁see ▁why , ▁she ▁is ▁an ▁incredible ▁actress . ▁The ▁scene ▁where ▁she ▁finally ▁realizes ▁what ▁had ▁happened ▁and ▁she ' s ▁on ▁the ▁set ▁of ▁her ▁favorite ▁soap ▁opera , ▁you ▁can ▁see ▁pain , ▁confusion , ▁fear , ▁and ▁embarrassment ▁on ▁her ▁face . ▁Just ▁to ▁let ▁you ▁in ▁on ▁the ▁movie , ▁she ▁plays ▁Betty . ▁A ▁shy ▁and ▁insecure ▁woman ▁who ▁stands ▁by ▁her ▁abusive ▁husband , ▁she ' s ▁a ▁waitress , ▁and ▁is ▁in ▁love ▁with ▁soap ▁operas , ▁especially ▁one ▁where ▁a ▁certain ▁cute ▁doctor , ▁Dr . ▁Dave ▁Revell . ▁When ▁she ▁happens ▁to ▁see ▁her ▁husband ' s ▁murder ▁accidentally ▁in ▁separate ▁room , ▁the ▁murders ▁she ▁notices ▁are ▁two ▁customers ▁she ▁just ▁had , ▁Morgan ▁Freeman ▁and ▁Chris ▁Rock . ▁She ▁then ▁just ▁loose s ▁her ▁mind ▁and ▁leaves ▁town ▁after ▁talking ▁to ▁he ▁police ▁and ▁says ▁she ▁needs ▁to ▁find ▁her ▁former ▁fiancée , ▁Dr . ▁Dave ▁Revell . ▁So , ▁she ▁travels ▁along ▁the ▁country ▁to ▁California ▁to ▁find ▁Dr . ▁Revell , ▁and ▁wants ▁a ▁job ▁as ▁a ▁nurse ▁to ▁work ▁with ▁Dave , ▁she ' s ▁seen ▁the ▁show ▁so ▁many ▁times , ▁somehow ▁she ' s ▁just ▁awesome ▁at ▁being ▁a ▁nurse ▁when ▁she ▁saves ▁a ▁woman ' s ▁brother . ▁Despite ▁everyone ▁telling ▁her ▁that ▁she ▁is ▁delusional , ▁she ▁just ▁looks ▁at ▁them ▁like ▁as ▁if ▁they ▁were ▁the ▁crazy ▁one ' s . ▁When ▁she ▁meets ▁the ▁actor ▁who ▁plays ▁Dave ▁Revell , ▁George ( his ▁real ▁name ) ▁thinks ▁she ' s ▁just ▁a ▁crazy ▁fan ▁trying ▁to ▁get ▁on ▁the ▁show . ▁She ▁just ▁looks ▁at ▁him ▁with ▁confusion ▁and ▁believes ▁that ▁he ▁and ▁her ▁belong ▁together . < br ▁/ > < br ▁/ > Re nee ▁was ▁terrific , ▁she ▁was ▁so ▁believable ▁on ▁loosing ▁her ▁mind ▁in ▁the ▁movie . ▁She ▁has ▁come ▁such ▁a ▁long ▁way , ▁and ▁with er ▁you ▁want ▁to ▁admit ▁it ▁or ▁not , ▁she ' s ▁adorable ▁and ▁a ▁great ▁actress . < br ▁/ > < br ▁/ > Morgan ▁Freeman ▁plays ▁one ▁of ▁the ▁assassin ' s , ▁Charlie , ▁who ▁is ▁the ▁father ▁of ▁the ▁two . ▁He ▁is ▁so ▁charmed ▁and ▁smitten ed ▁by ▁Betty ▁and ▁while ▁chasing ▁her ▁around ▁the ▁country , ▁he ▁becomes ▁almost ▁just ▁infatuated ▁with ▁Betty ▁to ▁the ▁point ▁where ▁he ▁almost ▁falls ▁in ▁love ▁with ▁her . ▁He ▁and ▁his ▁son ▁Wesley ▁must ▁find ▁Betty ▁when ▁they ▁find ▁out ▁she ▁was ▁there ▁at ▁the ▁murder ▁scene ▁and ▁could ▁give ▁away ▁their ▁identities . ▁When ▁Charlie ▁sees ▁Betty ▁and ▁catches ▁her ▁finally , ▁she ' s ▁scarred ▁at ▁first , ▁but ▁calms ▁down ▁and ▁they ▁know ▁they ▁have ▁a ▁real ▁connection . ▁It ▁was ▁a ▁beautifully ▁played ▁scene , ▁my ▁opinion ▁is ▁that ▁Morgan ▁gave ▁a ▁stronger ▁performance . ▁He ' s ▁just ▁great . < br ▁/ > < br ▁/ > A ▁surprisingly ▁decent ▁performance ▁by ▁Chris ▁Rock , ▁the ▁son , ▁Wesley . ▁He ▁is ▁so ▁\" gun \" - ho ▁about ▁just ▁getting ▁the ▁job ▁done ▁in ▁a ▁rush ▁and ▁taking ▁care ▁of ▁business . ▁I ▁loved ▁his ▁comedic ▁performance ▁at ▁the ▁end ▁where ▁he ▁and ▁the ▁gang ▁he ' s ▁holding ▁hostage ▁by ▁gun ▁point ▁are ▁just ▁watching ▁the ▁soap ▁opera ' s ▁together . ▁Classic . ▁\" Nurse ▁Betty \" ▁is ▁a ▁great ▁movie ▁that ▁I ' d ▁recommend ▁for ▁a ▁good ▁laugh ▁and ▁just ▁in ▁all ▁a ▁nice ▁honest ▁little ▁movie ▁I ▁think ▁anyone ▁could ▁enjoy . < br ▁/ > < br ▁/ > 9 / 10 \u0001\n",
      "0 ▁There ' s ▁something ▁intriguing ▁about ▁disaster ▁movies . ▁The ▁simple , ▁primal ▁premise ▁can ▁lead ▁to ▁several ▁great ▁stories . ▁Granted , ▁most ▁disaster ▁movies ▁tend ▁to ▁explore ▁familiar ▁territory ▁instead ▁but ▁I ▁can ▁usually ▁live ▁with ▁that . < br ▁/ > < br ▁/ > Unfortunately , ▁Flood ▁probably ▁marks ▁the ▁low ▁point ▁in ▁the ▁history ▁of ▁this ▁sub - genre . ▁Robert ▁Carlyle ▁is ▁undoubtedly ▁the ▁star ▁of ▁the ▁movie , ▁even ▁though ▁screen ▁time ▁is ▁split ▁between ▁different ▁locations ▁and ▁characters . ▁He ▁gives ▁a ▁barely ▁decent ▁performance . ▁As ▁well , ▁Joanne ▁Whalley ▁is ▁very ▁uneven . ▁Veteran ▁actor ▁Tom ▁Courtenay ▁( he ▁played ▁in ▁Doctor ▁Zhi va go ▁for ▁heaven ' s ▁sake ) ▁is ▁particularly ▁bad . ▁I ▁mean , ▁his ▁timing ▁is ▁completely ▁off ▁most ▁of ▁the ▁time ▁and ▁his ▁characterization ▁is ▁extremely ▁poor . ▁What ▁an ▁embarrassing ▁performance ▁for ▁that ▁man . ▁The ▁rest ▁of ▁the ▁cast ▁ranges ▁from ▁decent ▁to ▁really ▁bad ▁with ▁one ▁exception : ▁Jessa lyn ▁Gil sig , ▁whom ▁I ▁thought ▁might ▁be ▁there ▁as ▁a ▁plot ▁device / eye ▁candy ▁gives ▁by ▁far ▁the ▁most ▁convincing ▁performance . ▁Doesn ' t ▁mean ▁much ▁considering ▁how ▁bad ▁everybody ▁else ▁is ▁but ▁still ▁nice ▁to ▁see ▁that ▁she ▁cared . < br ▁/ > < br ▁/ > The ▁script ▁is ▁really ▁bad , ▁confusing ▁and ▁cliché . ▁Some ▁of ▁the ▁worse ▁lines ▁I ▁have ▁heard ▁in ▁quite ▁some ▁time ▁are ▁delivered ▁by ▁the ▁actors ▁one ▁after ▁the ▁other . You ' ve ▁seen ▁this ▁story ▁a ▁thousand ▁times . ▁It ▁employs ▁every ▁dramatic ▁hook ▁and ▁tear - jerk ers ▁you ' ve ▁seen ▁in ▁\" Out break \" , ▁\" Ar mageddon \" , ▁the ▁Poseidon ▁movies ▁( original ▁and ▁remake ) ▁and ▁many ▁others . < br ▁/ > < br ▁/ > The ▁direction ▁is ▁awful . ▁No ▁sense ▁of ▁timing , ▁nothing ▁inspired . ▁The ▁shots ▁are ▁bland , ▁dialog ▁and ▁action ▁both ▁fail ▁to ▁flow . ▁Editing ▁is ▁bad ▁but ▁how ▁do ▁you ▁edit ▁such ▁a ▁mess ? ▁Without ▁a ▁doubt , ▁this ▁movie ▁tried ▁to ▁rely ▁way ▁too ▁much ▁on ▁( rather ▁poor ) ▁CGI . ▁The ▁human ▁factor , ▁the ▁drama ▁and ▁struggles ▁of ▁the ▁characters ▁are ▁glossed ▁over . ▁Scenes ▁where ▁the ▁characters ▁must ▁actually ▁face ▁the ▁flood ▁are ▁rare ▁and ▁poorly ▁done . ▁The ▁made - for - TV ▁feel ▁gives ▁nausea . ▁Some ▁guy ▁is ▁supposed ▁to ▁go ▁down ▁a ▁rope ▁from ▁an ▁helicopter ? ▁No ▁problem , ▁let ' s ▁show ▁him ▁inside ▁a ▁helicopter ▁and ▁make ▁a ▁really ▁poor ▁cut / editing ▁job ▁and ▁have ▁the ▁next ▁frame ▁with ▁him ▁safely ▁on ▁the ▁ground , ▁in ▁the ▁most ▁obvious ▁way ▁possible . < br ▁/ > < br ▁/ > The ▁movie ▁score ▁is ▁rather ▁poor . ▁All ▁over ▁the ▁place , ▁no ▁timing . < br ▁/ > < br ▁/ > The ▁ending ▁is ▁probably ▁the ▁worse ▁I ▁have ▁seen ▁in ▁quite ▁some ▁time . ▁Very ▁much ▁like ▁they ▁ran ▁out ▁of ▁ideas . ▁Scrap ▁that , ▁you ▁can ' t ▁run ▁out ▁of ▁something ▁if ▁you ▁never ▁had ▁it ▁in ▁the ▁first ▁place . ▁Must ▁have ▁ran ▁out ▁of ▁budget . < br ▁/ > < br ▁/ > This ▁is ▁a ▁really ▁amateur ▁job . ▁I ▁give ▁it ▁a ▁2 ▁for ▁using ▁London ▁as ▁a ▁location , ▁which ▁is ▁a ▁nice ▁change , ▁for ▁Gil sig ▁being ▁actually ▁decent ▁in ▁a ▁key ▁support ▁role ▁and ▁for ▁the ▁few ▁CGI ▁shots ▁that ▁were ▁decent ▁( those ▁of ▁the ▁water ▁closing ▁in ▁on ▁London ▁and ▁the ▁gates ) . < br ▁/ > < br ▁/ > Do ▁yourself ▁a ▁favor ▁and ▁check ▁out ▁Day ▁After ▁Tomorrow ▁or ▁just ▁about ▁any ▁disaster ▁movie ▁before ▁this ▁one . ▁This ▁includes ▁older ▁classics ▁like ▁The ▁Tower ing ▁Inferno . \u0001\n"
     ]
    }
   ],
   "source": [
    "for i in range(15,20):\n",
    "    char_ids = X[i].x\n",
    "    text_for_ids = ''.join([vocab_dict_rev[ci.item()] for ci in char_ids])\n",
    "    print(torch.argmax(y[i]).item(), text_for_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rand = torch.randn((64, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9990)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rand.var(unbiased=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[406094], token_positions=[75621], character_length=[256], num_tokens=[256], token_indices=[406094], token_lengths=[75621], token_embeddings=[75621, 64], token_sentiments=[75621, 2], token_subsampling_probabilities=[75621], batch=[406094], ptr=[257], cumulative_token_indices=[406094])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = torch.randn((len(X.token_positions), 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([75621])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.repeat_interleave(torch.arange(len(X.num_tokens)), X.num_tokens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  ..., 20, 21, 22])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.token_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7994, -0.5107,  0.3645,  ...,  0.6588,  1.5570, -0.5494],\n",
       "        [-0.5500,  1.2228, -0.3744,  ...,  0.4838, -1.4837,  0.7082],\n",
       "        [ 0.0379,  0.0511,  0.0942,  ..., -0.2066,  0.7047, -0.4536],\n",
       "        ...,\n",
       "        [-0.4543, -0.9508,  1.2477,  ..., -1.0361,  0.8070,  1.4626],\n",
       "        [-0.0239,  1.1046, -0.7905,  ...,  1.3271, -0.2336, -0.4396],\n",
       "        [ 0.1513, -1.0826, -0.6217,  ..., -1.3772,  0.4925,  0.8410]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATv2Conv, SimpleConv\n",
    "\n",
    "# Normalization on each feature of all tokens, for this we used batch norm class but with tokens at batch dimention\n",
    "class GCNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, *args, **kwargs):\n",
    "        super(GCNN, self).__init__(*args, **kwargs)\n",
    "        self.gnn = GATv2Conv(hidden_dim, hidden_dim//8, heads=4, add_self_loops=False)\n",
    "        self.conv = nn.Conv1d(hidden_dim, hidden_dim//2, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(hidden_dim//2, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim//2)\n",
    "        \n",
    "    def forward(self, x, edge_data, return_attention_weights = False):\n",
    "        x1, edge_weights = self.gnn(x, edge_data, return_attention_weights=return_attention_weights) \n",
    "        x2 = F.relu(self.conv(x.T).T)\n",
    "        x1 = F.leaky_relu_(self.bn1(x1))\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        return x, edge_weights, edge_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx, to_undirected\n",
    "\n",
    "class GenGraph(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, virtual_nodes, lattice_step, lattice_pattern=None, *args, **kwargs):\n",
    "        super(GenGraph, self).__init__(*args, **kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.virtual_nodes = virtual_nodes\n",
    "        self.lattice_step = lattice_step\n",
    "        self.lp = lattice_pattern if lattice_pattern is None else torch.tensor(lattice_pattern)\n",
    "        self.virtual_node_embeddings = nn.Embedding(self.virtual_nodes, hidden_dim)\n",
    "        \n",
    "    def gen_graph(self, x, token_subsampling_probabilities, total_token_counts, token_counts, random_edges, lattice_edges, lattice_start_distance=2):\n",
    "        random_links, lattice_links, tc_range = self.calculate_graph(x, total_token_counts, token_counts, random_edges, lattice_edges, lattice_start_distance)\n",
    "        v_n_e_counts = total_token_counts*self.virtual_nodes\n",
    "        base_numel = random_links.numel() + lattice_links.numel()*2\n",
    "        edge_indices = torch.empty((2, base_numel + v_n_e_counts*2), dtype=torch.int64, device=x.device)\n",
    "        self.fill_lattice_and_random_edges(edge_indices, random_links, lattice_links, tc_range)\n",
    "            \n",
    "        if self.virtual_nodes > 0:\n",
    "            virtual_nodes_range = torch.arange(self.virtual_nodes, device=x.device).view(1, -1)\n",
    "            virtual_nodes_ids = torch.repeat_interleave(virtual_nodes_range, len(token_counts), dim=0)\n",
    "            v_n_idx = (virtual_nodes_ids + torch.arange(0, len(token_counts)*self.virtual_nodes, self.virtual_nodes, device=x.device).view(-1, 1) + total_token_counts )\n",
    "            virtual_edge_ids = torch.repeat_interleave(v_n_idx.view(-1), token_counts.view(-1, 1).expand(len(token_counts), self.virtual_nodes).reshape(-1), dim=0).view(1, -1)\n",
    "            \n",
    "            embs = self.virtual_node_embeddings(virtual_nodes_ids.T).view(-1, self.hidden_dim)\n",
    "            x_extended = torch.cat([x, embs], dim=0)\n",
    "            x_index = torch.arange(total_token_counts, device=x.device).repeat(self.virtual_nodes).view(1, -1)\n",
    "            edge_indices[:, base_numel:base_numel+v_n_e_counts] = torch.cat([x_index, virtual_edge_ids], dim=0)\n",
    "            edge_indices[:, base_numel+v_n_e_counts:] = torch.cat([virtual_edge_ids, x_index], dim=0)\n",
    "            x = x_extended\n",
    "        \n",
    "        edge_indices = self.subsample_edges(edge_indices, token_subsampling_probabilities)\n",
    "        return Batch.from_data_list([Data(x=x, edge_index=edge_indices)])\n",
    "        \n",
    "    def re_gen_graph(self, x, edge_indices, token_subsampling_probabilities, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance=2):\n",
    "        random_links, lattice_links, tc_range = self.calculate_graph(x, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance)\n",
    "        base_numel = random_links.numel() + lattice_links.numel()*2\n",
    "        \n",
    "        self.fill_lattice_and_random_edges(edge_indices, random_links, lattice_links, tc_range)\n",
    "        # for i in range(base.shape[1]):\n",
    "        #     edge_indices[:, i*base.shape[0]:(i+1)*base.shape[0]] = torch.cat([tc_range, base[:,i].view(1,-1)], dim=0)\n",
    "        edge_indices = self.subsample_edges(edge_indices, token_subsampling_probabilities)\n",
    "        return Batch.from_data_list([Data(x=x, edge_index=edge_indices)])\n",
    "    \n",
    "    def replace_unimportant_edges(self, edge_weights, x, edge_indices, token_subsampling_probabilities, total_token_counts, token_counts, random_edges, lattice_edges, p_keep=1, lattice_start_distance=2):\n",
    "        v_n_e_counts = total_token_counts*self.virtual_nodes\n",
    "        # if v_n_e_counts>0:\n",
    "        #     important_indices = torch.topk(edge_weights[:-2*v_n_e_counts].view(-1, total_token_coutns), p_keep, dim=0).indices\n",
    "        # else:\n",
    "        #     print(f'edge_weights.shape: {edge_weights.shape}')\n",
    "        #     print(f'total_token_coutns: {total_token_coutns}')\n",
    "        #     print(f'p_keep: {p_keep}')\n",
    "        #     important_indices = torch.topk(edge_weights.view(-1, total_token_coutns), p_keep, dim=0).indices\n",
    "        # important_indices = torch.topk(edge_weights[:-1*total_token_coutns].view(-1, total_token_coutns), 1, dim=0).indices.squeeze()\n",
    "        # print(f'edge_weights.shape: {edge_weights.shape}')\n",
    "        # print(f'edge_indices.shape: {edge_indices.shape}')\n",
    "        # print(f'1: edge_weights: {edge_weights.shape}')\n",
    "        important_indices = torch.topk(edge_weights.squeeze(), p_keep*total_token_counts, dim=0).indices\n",
    "        # print(f'2: important_indices: {important_indices.shape}')\n",
    "        # print(f'2.5: \\n {edge_weights} \\n\\n {important_indices}')\n",
    "\n",
    "        # important_indices = torch.arange(total_token_counts, dtype=torch.int64, device=x.device)\n",
    "        # important_indices = important_indices.view(-1)\n",
    "        random_links, lattice_links, tc_range = self.calculate_graph(x, total_token_counts, token_counts, random_edges, lattice_edges, lattice_start_distance)\n",
    "        # print(f'3: random_links: {random_links.shape}, lattice_links: {lattice_links.shape}, tc_range: {tc_range.shape},')\n",
    "        base_numel = random_links.numel() + lattice_links.numel()*2\n",
    "        # print(f'4: base_numel: {base_numel}')\n",
    "        \n",
    "        new_edge_index = torch.empty((2, base_numel + important_indices.shape[0] + 2*v_n_e_counts), dtype=torch.int64, device=x.device)\n",
    "        # print(f'5: new_edge_index: {new_edge_index.shape}')\n",
    "        # print(f'new_edge_index.shape 1: {new_edge_index.shape}, base_numel + important_indices.shape[0] + 2*v_n_e_counts: {base_numel + important_indices.shape[0] + 2*v_n_e_counts}')\n",
    "        self.fill_lattice_and_random_edges(new_edge_index, random_links, lattice_links, tc_range)\n",
    "        # print(f'6: new_edge_index: {new_edge_index.shape}, random_links: {random_links.shape}, lattice_links: {lattice_links.shape}, tc_range: {tc_range.shape}')\n",
    "        # print(f'new_edge_index.shape 2: {new_edge_index.shape}, edge_indices: {edge_indices.shape}, important_indices shape: {important_indices.shape}, important_indices max: {important_indices.max()}')\n",
    "        new_edge_index[:, base_numel:base_numel+important_indices.shape[0]] = edge_indices[:, important_indices]\n",
    "        # print(f'7: new_edge_index: {new_edge_index.shape}')\n",
    "\n",
    "        if(self.virtual_nodes>0):\n",
    "            new_edge_index[:, -2*v_n_e_counts:] = edge_indices[:, -2*v_n_e_counts:]\n",
    "            \n",
    "        # for i in range(base.shape[1]):\n",
    "        #     new_edge_index[:, i*base.shape[0]:(i+1)*base.shape[0]] = torch.cat([tc_range, base[:,i].view(1,-1)], dim=0)\n",
    "        # print(f'7.5: \\n {new_edge_index} \\n\\n {token_subsampling_probabilities}')\n",
    "        new_edge_index = self.subsample_edges(new_edge_index, token_subsampling_probabilities)\n",
    "        return Batch.from_data_list([Data(x=x, edge_index=new_edge_index)])\n",
    "    \n",
    "    # def replace_unimportant_edges(self, edge_weights, x, edge_index, token_subsampling_probabilities, total_token_coutns, token_counts, random_edges, lattice_edges, p_keep=1, lattice_start_distance=2, seed=-1):\n",
    "    #     v_n_e_counts = total_token_coutns*self.virtual_nodes\n",
    "    #     if v_n_e_counts>0:\n",
    "    #         important_indices = torch.topk(edge_weights[:-2*v_n_e_counts].view(-1, total_token_coutns), p_keep, dim=0).indices\n",
    "    #     else:\n",
    "    #         print(f'edge_weights.shape: {edge_weights.shape}')\n",
    "    #         print(f'total_token_coutns: {total_token_coutns}')\n",
    "    #         print(f'p_keep: {p_keep}')\n",
    "    #         important_indices = torch.topk(edge_weights.view(-1, total_token_coutns), p_keep, dim=0).indices\n",
    "    #     # important_indices = torch.topk(edge_weights[:-1*total_token_coutns].view(-1, total_token_coutns), 1, dim=0).indices.squeeze()\n",
    "\n",
    "    #     important_indices = torch.arange(total_token_coutns, dtype=torch.int64, device=x.device) + important_indices*total_token_coutns\n",
    "    #     important_indices = important_indices.view(-1)\n",
    "    #     random_links, lattice_links, tc_range = self.calculate_graph(x, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance, seed)\n",
    "    #     base_numel = random_links.numel() + lattice_links.numel()*2\n",
    "        \n",
    "    #     new_edge_index = torch.empty((2, base_numel + important_indices.shape[0] + 2*v_n_e_counts), dtype=torch.int64, device=x.device)\n",
    "    #     self.fill_lattice_and_random_edges(new_edge_index, random_links, lattice_links, tc_range)\n",
    "    #     new_edge_index[:, base_numel:base_numel+important_indices.shape[0]] = edge_index[:, important_indices]\n",
    "    #     if(self.virtual_nodes>0):\n",
    "    #         new_edge_index[:, -2*v_n_e_counts:] = edge_index[:, -2*v_n_e_counts:]\n",
    "            \n",
    "    #     # for i in range(base.shape[1]):\n",
    "    #     #     new_edge_index[:, i*base.shape[0]:(i+1)*base.shape[0]] = torch.cat([tc_range, base[:,i].view(1,-1)], dim=0)\n",
    "    #     new_edge_index = self.subsample_edges(new_edge_index, token_subsampling_probabilities)\n",
    "    #     return Batch.from_data_list([Data(x=x, edge_index=new_edge_index)])\n",
    "    \n",
    "    \n",
    "         \n",
    "    def calculate_graph(self, x, total_token_counts, token_counts, random_edges, lattice_edges, lattice_start_distance):\n",
    "\n",
    "        tc_extended = torch.repeat_interleave(token_counts, token_counts, dim=0).view(-1,1)\n",
    "        tc_lower_bound = torch.empty((len(token_counts)+1), dtype=torch.long, device=x.device) #torch.cuda.IntTensor(len(token_counts)+1) #\n",
    "        tc_lower_bound[0] = 0\n",
    "        tc_lower_bound[1:] = torch.cumsum(token_counts, dim=0)\n",
    "        tc_lower_bound_extended = torch.repeat_interleave(tc_lower_bound[:-1], token_counts, dim=0).view(-1,1)\n",
    "        tc_range = torch.arange(tc_lower_bound[-1], device=x.device).view(-1,1)\n",
    "        # torch.arange(tc_lower_bound[-1], dtype=torch.int32, device=x.device).view(-1,1)\n",
    "        \n",
    "        random_ints = torch.randint(0, 2*total_token_counts, (total_token_counts, random_edges), device=x.device) # torch.cuda.IntTensor(len(token_lengths), random_edges).random_()\n",
    "        lattice = self.lp.to(x.device) if self.lp is not None else torch.arange(lattice_start_distance, max(lattice_start_distance, self.lattice_step*lattice_edges+1), self.lattice_step, device=x.device).view(1, -1)\n",
    "        \n",
    "\n",
    "        # exponentials = torch.pow(2, torch.arange(1, self.exp_edges+1, device=x.device)).view(1, -1)\n",
    "        tc_local_range = tc_range - tc_lower_bound_extended\n",
    "        random_links = (((random_ints % (tc_extended - 1))+1 + tc_local_range) % tc_extended)+tc_lower_bound_extended\n",
    "        lattice_links = ((lattice + tc_local_range) % tc_extended)+tc_lower_bound_extended\n",
    "        # base = torch.cat([base1, base2], dim=1)\n",
    "        tc_range = tc_range.view(1,-1)\n",
    "        return random_links, lattice_links, tc_range\n",
    "    \n",
    "    def fill_lattice_and_random_edges(self, edge_indices, random_links, lattice_links, tc_range):\n",
    "        for i in range(0, lattice_links.shape[1]*2, 2):\n",
    "            edge_indices[:, i*lattice_links.shape[0]:(i+1)*lattice_links.shape[0]] = torch.cat([lattice_links[:,i//2].view(1,-1), tc_range], dim=0)\n",
    "            edge_indices[:, (i+1)*lattice_links.shape[0]:(i+2)*lattice_links.shape[0]] = edge_indices[:, i*lattice_links.shape[0]:(i+1)*lattice_links.shape[0]][[1, 0]]\n",
    "            \n",
    "        for i in range(random_links.shape[1]):\n",
    "            j = i + lattice_links.shape[1]*2\n",
    "            edge_indices[:, j*random_links.shape[0]:(j+1)*random_links.shape[0]] = torch.cat([random_links[:,i].view(1,-1), tc_range], dim=0)\n",
    "            \n",
    "    def subsample_edges(self, edge_indices, token_subsampling_probabilities, keep_ratio=0.65):\n",
    "        \n",
    "        p = torch.rand(edge_indices.shape, dtype=torch.float, device=edge_indices.device)\n",
    "        to_keep = (p<token_subsampling_probabilities[edge_indices]).float()\n",
    "        to_keep = torch.topk(to_keep[0] + to_keep[1], (int)(edge_indices.shape[1]*keep_ratio), dim=0).indices\n",
    "        edge_indices = edge_indices[:, to_keep]\n",
    "        return edge_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6016, 0.7661, 0.2928, 0.7894, 0.5939],\n",
      "        [0.6280, 0.3027, 0.0625, 0.2144, 0.7388]])\n",
      "tensor([[0.3811, 0.9040, 0.6691, 0.8094, 0.1984],\n",
      "        [0.7413, 0.8390, 0.3304, 0.6647, 0.7376]])\n"
     ]
    }
   ],
   "source": [
    "base = torch.rand((2, 5), dtype=torch.float)\n",
    "p = torch.rand((2, 5), dtype=torch.float)\n",
    "print(base)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 1.],\n",
       "        [1., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_keep = (p<torch.tensor([[1, 0.5, 0.5, 0.0, 0.7], [1.0, 0.5, 1.0, 0.5, 0.0]])).float()\n",
    "to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 4])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_keep = torch.topk(to_keep[0] + to_keep[1], 3, dim=0).indices\n",
    "to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# to_keep = torch.topk(to_keep[0] + to_keep[1], 3, dim=0).indices\n",
    "# edge_indices = edge_indices[:, to_keep]\n",
    "# torch.topk(to_keep[0] + to_keep[1], (int)(edge_indices.shape[1]*keep_ratio), dim=0).indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment_Injection(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)        \n",
    "        self.conv1 = nn.Conv1d(2, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim*2, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "    def forward(self, x, token_sentiments):\n",
    "        x1 = F.relu_(self.bn1(self.conv1(token_sentiments.T).T))\n",
    "        x = F.relu_(self.conv2(torch.cat([x, x1], dim=1).T))\n",
    "        return x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATv2Conv, SimpleConv, summary\n",
    "\n",
    "class CNN_for_Text_No_Positional_Encoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embedding, pos_emb_size=8192, embedding_dim=64, hidden_dim=64, dropout=0.3, num_out_features=4, seed=-1, random_edges=4, lattice_edges=10, virtual_nodes=1, lattice_step=2, lattice_start_distance=2, inject_embedding_dim=64, isXaiTests=False, step_of_test = 0, num_tests=50, *args, **kwargs) -> None:\n",
    "        super(CNN_for_Text_No_Positional_Encoding, self).__init__(*args, **kwargs)\n",
    "        self.pos_emb_size = pos_emb_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.virtual_nodes = virtual_nodes\n",
    "        self.base_random_edges = random_edges\n",
    "        self.base_lattice_edges = lattice_edges\n",
    "        self.lattice_start_distance = lattice_start_distance\n",
    "        self.num_out_features = num_out_features\n",
    "        self.isXaiTests = int(isXaiTests)\n",
    "        self.num_tests = num_tests\n",
    "        self.step_of_test = step_of_test\n",
    "        # self.use_token_polarity = use_token_polarity\n",
    "        if seed>-1:\n",
    "            torch.manual_seed(seed)\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embedding, embedding_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(embedding_dim, hidden_dim, kernel_size=5, padding=2)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, padding=2)\n",
    "        \n",
    "        # if self.use_token_polarity[0]:\n",
    "        #     self.conv3 = nn.Conv1d(2*hidden_dim + 2, hidden_dim, kernel_size=3, padding=1)\n",
    "        # else:\n",
    "        self.conv3 = nn.Conv1d(2*hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv4 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        \n",
    "        # if self.use_token_polarity[1]:\n",
    "        self.sentiment1  = Sentiment_Injection(hidden_dim)\n",
    "        # if self.use_token_polarity[2]:\n",
    "        self.sentiment2  = Sentiment_Injection(hidden_dim)\n",
    "            \n",
    "        self.gcnn1 = GCNN(hidden_dim)\n",
    "        self.gcnn2 = GCNN(hidden_dim+inject_embedding_dim)\n",
    "        self.graph_generator = GenGraph(hidden_dim, virtual_nodes, lattice_step)\n",
    "        \n",
    "        k = 4\n",
    "        self.fc0 = nn.Linear(hidden_dim , hidden_dim+inject_embedding_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim+inject_embedding_dim , hidden_dim * k)\n",
    "        self.fc2 = nn.Linear(hidden_dim * (2+virtual_nodes) * k , 32)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(32, self.num_out_features)\n",
    "        self.max_length = 0\n",
    "    \n",
    "    def forward(self, x, edge_index, token_subsampling_probabilities, token_indices, token_sentiments, token_lengths, num_tokens, character_length, token_embeddings):\n",
    "        # cumulative_token_indices = token_indices if not self.isXaiTests else self.caluculate_batch_token_positions(num_tokens, character_length, token_indices)\n",
    "        cumulative_token_indices = self.caluculate_batch_token_positions(num_tokens, character_length, token_indices)\n",
    "        \n",
    "        # print(f'2: {x.shape}')\n",
    "        x = self.embedding(x)\n",
    "        # print(f'2.5: {x.shape}')\n",
    "        x = self.dropout(x)\n",
    "        # print(f'2.6: {x.shape}')\n",
    "        x = x.T\n",
    "        # print(f'2.7: {x.shape}')\n",
    "        # x = self.refine_shape(1, x, 0)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # print(f'2.8: {x.shape}')\n",
    "        x = self.refine_shape(1, x, self.hidden_dim, 0)\n",
    "        # print(f'2.8 refined: {x.shape}')\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.refine_shape(2, x, self.hidden_dim, 0)\n",
    "        # print(f'2.9: {x.shape}')\n",
    "        x = self.dropout(x)\n",
    "        # x = self.refine_shape(4, x, 0)\n",
    "        # print(f'3: {x.shape}')\n",
    "        x1 = scatter_max(x, cumulative_token_indices, dim=1)[0]\n",
    "        x2 = scatter_mean(x, cumulative_token_indices, dim=1)\n",
    "\n",
    "        # if self.use_token_polarity[0]:\n",
    "        #     x = torch.cat([x1, x2, g_data.token_sentiments.T], dim=0)\n",
    "        # else:\n",
    "        x = torch.cat([x1, x2], dim=0)\n",
    "            \n",
    "        # print(f'4: {x.shape}')\n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        # print(f'4.5: {x.shape}, self.hidden_dim: {self.hidden_dim}, self.is_tests_token_level: {self.step_of_test}')\n",
    "        # if self.isXaiTests and x.shape[0] != self.hidden_dim:\n",
    "        # x = torch.chunk(x, self.num_tests ** (1 - self.isXaiTests), dim=0)\n",
    "        x = self.refine_shape(3, x, self.hidden_dim, 0)\n",
    "        \n",
    "        # x = torch.chunk(x, (x.shape[0] // self.hidden_dim)**self.is_tests_token_level, dim=0)\n",
    "        # x = torch.cat(x, dim=1)\n",
    "        \n",
    "        # if self.isXaiTests and x.shape[0] != self.hidden_dim:\n",
    "        #     x = torch.chunk(x, self.num_tests, dim=0)\n",
    "        #     x = torch.cat(x, dim=1)\n",
    "        # x = x.reshape(self.hidden_dim, -1)\n",
    "        # print(\"abababdadasd\")\n",
    "        # print(f'5: {x.shape}, {edge_index.shape}, {cumulative_token_indices.shape}, {token_sentiments.shape}, {token_lengths.shape}, {num_tokens.shape}, {character_length.shape}, {token_embeddings.shape}')\n",
    "        # if self.use_token_polarity[1]:\n",
    "        x = self.sentiment1(x.T, token_sentiments)\n",
    "\n",
    "        # print(f'6: {x.shape}')\n",
    "        x = self.refine_shape(4, x, self.hidden_dim, 1)\n",
    "        # print(f'6 refined: {x.shape}')\n",
    "        rand_edges, lattice_edges = self.base_random_edges, self.base_lattice_edges\n",
    "            \n",
    "        graph = self.graph_generator.gen_graph(x, token_subsampling_probabilities, len(token_lengths), num_tokens, rand_edges, lattice_edges, lattice_start_distance=self.lattice_start_distance)\n",
    "        rand_edges = rand_edges-1\n",
    "        lattice_edges = lattice_edges-1\n",
    "        \n",
    "        # print(f'7: {graph.x.shape}')\n",
    "        \n",
    "        doc_token_index = torch.repeat_interleave(torch.arange(len(num_tokens), device=x.device), num_tokens)\n",
    "        x, edge_weights, edge_index = self.gcnn1(graph.x, graph.edge_index, return_attention_weights = True)\n",
    "        # print(f'7.1: {x.shape}')\n",
    "        x = self.refine_shape(5, x, self.hidden_dim, 1)\n",
    "        # print(f'7.1 2 : {len(torch.cat(edge_weights[1::2], dim=0))}')\n",
    "        edge_weights = self.refine_edge_weights(edge_weights)\n",
    "        edge_index = self.refine_edge_index(edge_index)\n",
    "        \n",
    "        # edge_weights = edge_weights[1].unsqueeze(-1)\n",
    "        # print(f'7.5 edge_weights: {edge_weights.shape}, - {graph.edge_index.shape[1]}, {edge_weights.shape[0]}')\n",
    "        # edge_weights = edge_weights[:min(graph.edge_index.shape[1], edge_weights.shape[0]), 0]\n",
    "        # edge_weights = edge_weights.squeeze()\n",
    "        \n",
    "        edge_weights = edge_weights.unsqueeze(-1)\n",
    "        # print(f'7.5 edge_weights: {edge_weights.shape}, - {edge_weights.shape[0]}')\n",
    "        edge_weights = edge_weights[:edge_weights.shape[0], 0]\n",
    "        \n",
    "        # print(f'7.6 edge_weights: {edge_weights.shape}')\n",
    "        # print(f'7.7: {x.shape}')\n",
    "        x = self.refine_shape(5, x, self.hidden_dim, 1)\n",
    "        # print(f'7.8 refined: {x.shape}')\n",
    "        \n",
    "        graph = self.graph_generator.replace_unimportant_edges(edge_weights, x, edge_index, token_subsampling_probabilities, len(token_lengths), num_tokens, rand_edges, lattice_edges, p_keep=2, lattice_start_distance=self.lattice_start_distance+1)\n",
    "        \n",
    "        # print(f'8: {graph.x.shape}')\n",
    "        \n",
    "        # if self.use_token_polarity[2]:\n",
    "        x = self.sentiment2(x, token_sentiments)\n",
    "          \n",
    "        # print(f'8.1: {x.shape}')\n",
    "        x = self.refine_shape(6, x, self.hidden_dim, 1)\n",
    "        # print(f'8.2 refined: {x.shape}')\n",
    "        \n",
    "        # print(f'9: {x.shape}')  \n",
    "        xa = graph.x[:token_embeddings.shape[0]]\n",
    "        xb = token_embeddings\n",
    "        x = torch.cat([xa, xb], dim=1)\n",
    "        # x = torch.cat([graph.x[:g_data.token_embeddings.shape[0]], g_data.token_embeddings], dim=1)\n",
    "        \n",
    "        # print(f'10: {x.shape}')  \n",
    "        x1 = F.relu(self.fc0(graph.x[token_embeddings.shape[0]:]))\n",
    "        x = torch.cat([x, x1], dim=0)\n",
    "        \n",
    "        # print(f'11: {x.shape}')  \n",
    "        sum1 = torch.sum(edge_weights) + torch.sum(edge_index)\n",
    "        x, edge_weights, edge_index = self.gcnn2(x, graph.edge_index)\n",
    "        edge_weights = self.refine_edge_weights(edge_weights)\n",
    "        sum1 = sum1 + torch.sum(edge_weights) + torch.sum(edge_index) \n",
    "        \n",
    "        # print(f'12: {x.shape}')  \n",
    "        x = F.elu_(self.fc1(x))\n",
    "        x1 = scatter_max(x[:len(token_lengths)], doc_token_index, dim=0)[0]\n",
    "        x2 = scatter_mean(x[:len(token_lengths)], doc_token_index, dim=0)\n",
    "        vn_embs = x[len(token_lengths):]\n",
    "        x_for_cat = [x1, x2]\n",
    "        x_for_cat.extend([vn_embs[i*x1.shape[0]:(i+1)*x1.shape[0]] for i in range(self.virtual_nodes)])\n",
    "        x = torch.cat(x_for_cat, dim=1)\n",
    "        \n",
    "        # print(f'13: {x.shape}')  \n",
    "        x = F.elu_(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc_out(x)\n",
    "        # print(f'14: {x.shape}')  \n",
    "        return x + sum1 * 0.0\n",
    "    \n",
    "    def caluculate_batch_token_positions(self, num_tokens, character_length, token_indices):\n",
    "        cumsum_vals = torch.cumsum(num_tokens, dim=0).roll(1)\n",
    "        cumsum_vals[0] = 0\n",
    "        additions = torch.repeat_interleave(cumsum_vals, character_length)\n",
    "        cumulative_token_indices = token_indices + additions\n",
    "        return cumulative_token_indices\n",
    "    \n",
    "    def refine_shape(self, test_step, x, num_chunks, section=0):\n",
    "        x = torch.chunk(x, (x.shape[section] // num_chunks)**(self.step_of_test==test_step), dim=0)\n",
    "        x = torch.cat(x, dim=1-section)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def refine_edge_weights(self, edge_weights):\n",
    "        edge_weights = edge_weights[1::2] + edge_weights[0::2] * 0\n",
    "        edge_weights = [edge_weights[i] for i in range(len(edge_weights))]\n",
    "        edge_weights = torch.cat(edge_weights, dim=0)\n",
    "        return edge_weights\n",
    "    \n",
    "    \n",
    "    def refine_edge_index(self, edge_index):\n",
    "        edge_index = torch.cat([edge_index[::2].reshape(1, -1), edge_index[1::2].reshape(1, -1)], dim=0)\n",
    "        return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 20],\n",
       "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 30]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([torch.arange(1000).reshape(100,10)[::2].reshape(1, -1),torch.arange(1000).reshape(100,10)[1::2].reshape(1, -1)], dim=0)[:, :11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for p1 in [False, True]:\n",
    "# #     for p2 in [False, True]:\n",
    "# #         for p3 in [False, True]:\n",
    "# # print(f'\\n{p1}, {p2}, {p3}: \\n')\n",
    "# classifier_torch_model = CNN_for_Text_No_Positional_Encoding(num_embedding=num_embedding, hidden_dim=64, embedding_dim=64, pos_emb_size=3080, dropout=0.2, num_out_features=len(class_id), seed=911, random_edges=4, lattice_edges=4, lattice_step=2, virtual_nodes=0, lattice_start_distance=2, isXaiTests=True, num_tests=len(X.num_tokens)).eval()\n",
    "# flopt_counter = FlopCounterMode(classifier_torch_model)\n",
    "# with flopt_counter:\n",
    "#     classifier_torch_model(X.x, torch.zeros((2, 0)), X.token_subsampling_probabilities, X.token_indices, X.token_sentiments, X.token_lengths, X.num_tokens, X.character_length, X.token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CnnGnnClassifierLightningModel(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        num_classes,\n",
    "        optimizer=None,\n",
    "        loss_func=None,\n",
    "        learning_rate=0.01,\n",
    "        batch_size=64,\n",
    "        lr_scheduler=None,\n",
    "        user_lr_scheduler=False,\n",
    "        min_lr=0.0,\n",
    "    ):\n",
    "        super(CnnGnnClassifierLightningModel, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "        self.min_lr = min_lr\n",
    "        # self.save_hyperparameters(ignore=[\"model\"])\n",
    "        self.save_hyperparameters(logger=False)\n",
    "        self.optimizer = self._get_optimizer(optimizer)\n",
    "        self.lr_scheduler = (\n",
    "            self._get_lr_scheduler(lr_scheduler) if user_lr_scheduler else None\n",
    "        )\n",
    "        self.loss_func = loss_func\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.model(x.x, torch.zeros((2, 0)), x.token_subsampling_probabilities, x.token_indices, x.token_sentiments, x.token_lengths, x.num_tokens, x.character_length, x.token_embeddings)\n",
    "\n",
    "    def on_train_epoch_start(self) -> None:\n",
    "        param_groups = next(iter(self.optimizer.param_groups))\n",
    "        if \"lr\" in param_groups and param_groups[\"lr\"] is not None:\n",
    "            current_learning_rate = float(param_groups[\"lr\"])\n",
    "            self.log(\n",
    "                \"lr\",\n",
    "                current_learning_rate,\n",
    "                batch_size=self.batch_size,\n",
    "                on_epoch=True,\n",
    "                on_step=False,\n",
    "            )\n",
    "\n",
    "    def training_step(self, batch, *args, **kwargs):\n",
    "        X, y = batch\n",
    "        X.to(self.device)\n",
    "        y.to(self.device)\n",
    "        \n",
    "        self.model.train()\n",
    "        y_out = self(X)\n",
    "\n",
    "        loss = self.loss_func(y_out.view(y.shape), y )\n",
    "        self.train_losses.append(loss.detach().item())\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            batch_size=self.batch_size,\n",
    "            on_epoch=True,\n",
    "            on_step=True,\n",
    "        )\n",
    "        \n",
    "        self.train_acc(torch.argmax(y_out, dim=1), torch.argmax(y, dim=1))\n",
    "        self.log('train_acc', self.train_acc, prog_bar=True, on_epoch=True, on_step=True, batch_size=self.batch_size)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, *args, **kwargs):\n",
    "        X, y = batch\n",
    "        X.to(self.device)\n",
    "        y.to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        y_out = self(X)\n",
    "        loss = self.loss_func(y_out.view(y.shape), y )\n",
    "        self.val_losses.append(loss.detach().item())\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            batch_size=self.batch_size,\n",
    "            on_epoch=True,\n",
    "            on_step=True,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.val_acc(torch.argmax(y_out, dim=1), torch.argmax(y, dim=1))\n",
    "        self.log('val_acc', self.val_acc, prog_bar=True, on_epoch=True, on_step=True, batch_size=self.batch_size)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.lr_scheduler is None:\n",
    "            return self.optimizer\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": self.optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": self.lr_scheduler,\n",
    "                \"monitor\": \"train_loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def update_learning_rate(self, learning_rate: float):\n",
    "        self.learning_rate = learning_rate\n",
    "        for g in self.optimizer.param_groups:\n",
    "            g[\"lr\"] = learning_rate\n",
    "\n",
    "    def _get_optimizer(self, optimizer):\n",
    "        return (\n",
    "            optimizer\n",
    "            if optimizer is not None\n",
    "            else torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        )\n",
    "\n",
    "    def _get_lr_scheduler(self, lr_scheduler):\n",
    "        return (\n",
    "            lr_scheduler\n",
    "            if lr_scheduler is not None\n",
    "            else torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer, patience=5, factor=0.5, mode=\"min\", min_lr=self.min_lr\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from torchmetrics import ConfusionMatrix\n",
    "\n",
    "def calculate_metrics(cl_model, dataloader):\n",
    "    cm = ConfusionMatrix(task=\"multiclass\", num_classes=len(class_id))\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    cl_model = cl_model.eval()\n",
    "    cl_model.to(device)\n",
    "    for X, y in tqdm(dataloader):\n",
    "        X = X.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_p = cl_model(X)\n",
    "            y_p = y_p.cpu()\n",
    "        y_pred.append(y_p)\n",
    "        y_true.append(y)\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "    y_true = torch.cat(y_true, dim=0)\n",
    "    y_pred2 = torch.argmax(y_pred, dim=1)\n",
    "    y_true2 = torch.argmax(y_true, dim=1)\n",
    "    print(f'classification report: \\n {classification_report(y_true2, y_pred2, digits=4)}')\n",
    "    print(f'confusion matrix:\\n {cm(y_pred2, y_true2)}')\n",
    "    print('================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fardin Rastakhiz @ 2023\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from lightning.pytorch.loggers import Logger, CSVLogger\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from typing import List\n",
    "from pytorch_lightning.core.saving import save_hparams_to_yaml\n",
    "\n",
    "class ModelManager(ABC):\n",
    "\n",
    "    def __init__(self,\n",
    "                 torch_model: torch.nn.Module,\n",
    "                 lightning_model,\n",
    "                 model_save_dir: str = '~/Desktop',\n",
    "                 log_dir: str = 'logs/',\n",
    "                 log_name: str = 'model_logs',\n",
    "                 device='cpu',\n",
    "                 max_epochs = 100,\n",
    "                 ckpt_path: str|None=None,\n",
    "                 accumulate_grad_batches=1):\n",
    "        self.torch_model = torch_model\n",
    "        self.lightning_model = lightning_model\n",
    "        self.log_dir = log_dir\n",
    "        self.log_name = log_name\n",
    "        self.model_save_dir = model_save_dir\n",
    "        self.device = device\n",
    "        self.accelerator = 'cpu' if self.device=='cpu' else 'gpu'\n",
    "        self.max_epochs = max_epochs\n",
    "        self.ckpt_path = ckpt_path\n",
    "\n",
    "        self.logger = self._create_logger()\n",
    "        self.callbacks = self._create_callbacks()\n",
    "        self.trainer: L.Trainer = self._create_trainer(accumulate_grad_batches)\n",
    "        self.tuner = Tuner(self.trainer)\n",
    "        self.tuning_result = None\n",
    "\n",
    "    def tune(self, data_manager=None, train_dataloaders=None, val_dataloaders=None, datamodule=None, draw_result=True, min_lr=0.0000001, max_lr=0.1):\n",
    "        self.tuning_result = self.tuner.lr_find(self.lightning_model, datamodule=data_manager, train_dataloaders=train_dataloaders,\n",
    "                         val_dataloaders=val_dataloaders, min_lr=min_lr,max_lr=max_lr, num_training=150)\n",
    "        if draw_result:\n",
    "            fig = self.tuning_result.plot(suggest=True)\n",
    "            fig.show()\n",
    "        self.update_learning_rate(self.tuning_result.suggestion())\n",
    "        return self.tuning_result.suggestion()\n",
    "    \n",
    "    def update_learning_rate(self, lr):\n",
    "        self.lightning_model.update_learning_rate(lr)\n",
    "\n",
    "    def fit(self, train_dataloaders=None, val_dataloaders=None, datamodule=None, max_epochs = -1, ckpt_path=None):\n",
    "        if ckpt_path is not None and ckpt_path != '':\n",
    "            self.ckpt_path = ckpt_path\n",
    "        if max_epochs>0:\n",
    "            self.trainer.fit_loop.max_epochs = max_epochs\n",
    "            # self.max_epochs = max_epochs\n",
    "            # self.trainer = self._create_trainer()\n",
    "        self.trainer.fit(self.lightning_model,\n",
    "                         datamodule=datamodule,\n",
    "                         train_dataloaders=train_dataloaders,\n",
    "                         val_dataloaders=val_dataloaders,\n",
    "                         ckpt_path = self.ckpt_path\n",
    "                         )\n",
    "\n",
    "    def validate(self, dataloaders=None, datamodule=None):\n",
    "        return self.trainer.validate(self.lightning_model,\n",
    "                             datamodule=datamodule,\n",
    "                             dataloaders=dataloaders)\n",
    "\n",
    "    def predict(self, dataloaders=None, datamodule=None):\n",
    "        return self.trainer.predict(self.lightning_model,\n",
    "                             datamodule=datamodule,\n",
    "                             dataloaders=dataloaders)\n",
    "\n",
    "    def _create_trainer(self, accumulate_grad_batches) -> L.Trainer:\n",
    "        return L.Trainer(\n",
    "            callbacks=self.callbacks,\n",
    "            max_epochs=self.max_epochs,\n",
    "            accelerator=self.accelerator,\n",
    "            logger=self.logger,\n",
    "            num_sanity_val_steps=0,\n",
    "            default_root_dir=self.model_save_dir,\n",
    "            accumulate_grad_batches=accumulate_grad_batches\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def _create_callbacks(self) -> List[Callback]:\n",
    "        pass\n",
    "\n",
    "    def _create_logger(self) -> Logger:\n",
    "        return CSVLogger(save_dir=self.log_dir, name=self.log_name)\n",
    "\n",
    "    @abstractmethod\n",
    "    def draw_summary(self, dataloader):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def plot_csv_logger(self, loss_names, eval_names):\n",
    "        pass\n",
    "    \n",
    "    def save_hyper_parameters(self):\n",
    "        mhparams = {\n",
    "            'start_lr': 0.045,\n",
    "            'ckpt_lrs' :  {51: 0.002, 65: 0.00058},\n",
    "            'last_lr' : 0.0003,\n",
    "            'ac_loss_factor': 0.0002,\n",
    "            'weight_decay': 0.0012\n",
    "        }\n",
    "        save_hparams_to_yaml(config_yaml=r'logs\\hetero_model_17_AG\\version_12\\hparams.yaml',\n",
    "                     hparams=mhparams)\n",
    "        \n",
    "    # def find_best_settings(data_manager,\n",
    "    #                        lrs: List[float]=[0.001], dropouts: List[float]=[0.2], \n",
    "    #                        weight_decays: List[float]=[0.00055], emb_factors: List[float]=[0.1], \n",
    "    #                        batch_sizes: List[int]=[128], log_name='find_best_settings'):\n",
    "    #     for lr in lrs:\n",
    "    #         for dropout in dropouts:\n",
    "    #             for wd in weight_decays:\n",
    "    #                 for emb_factor in emb_factors:\n",
    "    #                     for bs in batch_sizes:\n",
    "    #                         data_manager.update_batch_size(bs)\n",
    "    #                         torch_model = HeteroGcnGatModel1(300, 1, X1.metadata(), 128, dropout=dropout)\n",
    "    #                         lightning_model = HeteroBinaryLightningModel(torch_model,\n",
    "    #                                         torch.optim.Adam(torch_model.parameters(), lr=lr, weight_decay=wd),\n",
    "    #                                             loss_func=HeteroLoss1(exception_keys='word', enc_factor=emb_factor),\n",
    "    #                                             learning_rate=lr,\n",
    "    #                                             batch_size=bs,\n",
    "    #                                             user_lr_scheduler=True\n",
    "    #                                             ).to(device)\n",
    "    #                         model_manager = ClassifierModelManager(torch_model, lightning_model, log_name=log_name, device=device, num_train_epoch=10)\n",
    "    #                         model_manager.fit(datamodule=data_manager)\n",
    "    #                         model_manager.save_plot_csv_logger(name_prepend=f'{lr}_{dropout}_{wd}_{emb_factor}_{bs}', loss_names=['train_loss', 'val_loss'], eval_names=['train_acc_epoch', 'val_acc_epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fardin Rastakhiz @ 2023\n",
    "import torch\n",
    "# from scripts.managers.ModelManager import ModelManager\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from torch_geometric.nn import summary\n",
    "from lightning.pytorch.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from os import path\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, hinge_loss\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "class ClassifierModelManager(ModelManager):\n",
    "\n",
    "    def __init__(self,\n",
    "                 torch_model: torch.nn.Module,\n",
    "                 lightning_model,\n",
    "                 model_save_dir: str = '~/Desktop',\n",
    "                 log_dir: str = 'logs/',\n",
    "                 log_name: str = 'model_logs',\n",
    "                 device='cpu',\n",
    "                 num_train_epoch = 100,\n",
    "                 accumulate_grad_batches=1):\n",
    "        super(ClassifierModelManager, self).__init__(torch_model, lightning_model, model_save_dir, log_dir, log_name, device, num_train_epoch, accumulate_grad_batches=accumulate_grad_batches)\n",
    "\n",
    "    def _create_callbacks(self) -> List[Callback]:\n",
    "        return [\n",
    "            ModelCheckpoint(save_top_k=2, mode='max', monitor='val_acc', save_last=True),\n",
    "            # EarlyStopping(patience=50, mode='max', monitor='val_acc')\n",
    "        ]\n",
    "\n",
    "    def draw_summary(self, dataloader):\n",
    "        X, y = next(iter(dataloader))\n",
    "        print(summary(self.torch_model, X.to(self.device)))\n",
    "\n",
    "    def plot_csv_logger(self, loss_names=['train_loss', 'val_loss'], eval_names=['train_acc', 'val_acc']):\n",
    "        csv_path = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', 'metrics.csv')\n",
    "        metrics = pd.read_csv(csv_path)\n",
    "\n",
    "        aggregation_metrics = []\n",
    "        agg_col = 'epoch'\n",
    "        for i, dfg in metrics.groupby(agg_col):\n",
    "            agg = dict(dfg.mean())\n",
    "            agg[agg_col] = i\n",
    "            aggregation_metrics.append(agg)\n",
    "\n",
    "        df_metrics = pd.DataFrame(aggregation_metrics)\n",
    "        df_metrics[loss_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='loss')\n",
    "        df_metrics[eval_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='accuracy')\n",
    "        plt.show()\n",
    "\n",
    "    def save_plot_csv_logger(self, loss_names=['train_loss', 'val_loss'], eval_names=['train_acc', 'val_acc'], name_prepend: str=\"\"):\n",
    "        csv_path = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', 'metrics.csv')\n",
    "        metrics = pd.read_csv(csv_path)\n",
    "\n",
    "        aggregation_metrics = []\n",
    "        agg_col = 'epoch'\n",
    "        for i, dfg in metrics.groupby(agg_col):\n",
    "            agg = dict(dfg.mean())\n",
    "            agg[agg_col] = i\n",
    "            aggregation_metrics.append(agg)\n",
    "\n",
    "        df_metrics = pd.DataFrame(aggregation_metrics)\n",
    "        df_metrics[loss_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='loss')\n",
    "        \n",
    "        loss_png = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', f'{name_prepend}_loss_metric.png')\n",
    "        plt.savefig(loss_png)\n",
    "        \n",
    "        df_metrics[eval_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='accuracy')\n",
    "        \n",
    "        acc_png = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', f'{name_prepend}_acc_metric.png')\n",
    "        plt.savefig(acc_png)\n",
    "        \n",
    "        plt.close()\n",
    "    \n",
    "    def evaluate(self, eval_dataloader,\n",
    "                 give_confusion_matrix: bool=True, \n",
    "                 give_report: bool=True, \n",
    "                 give_f1_score: bool=False, \n",
    "                 give_accuracy_score: bool=False, \n",
    "                 give_precision_score: bool=False, \n",
    "                 give_recall_score: bool=False, \n",
    "                 give_hinge_loss: bool=False):\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        self.lightning_model.eval()\n",
    "        for X, y in eval_dataloader:\n",
    "            y_p = self.lightning_model(X.to(self.device))\n",
    "            if type(y_p) is tuple:\n",
    "                y_p = y_p[0]\n",
    "            y_pred.append((y_p>0).to(torch.int32).detach().to(y.device))\n",
    "            y_true.append(y.to(torch.int32))\n",
    "        y_true = torch.concat(y_true)\n",
    "        y_pred = torch.concat(y_pred)\n",
    "        if(give_confusion_matrix):\n",
    "            print(f'confusion_matrix: \\n{confusion_matrix(y_true, y_pred)}')\n",
    "        if(give_report):\n",
    "            print(classification_report(y_true, y_pred))\n",
    "        if(give_f1_score):\n",
    "            print(f'f1_score: {f1_score(y_true, y_pred)}')\n",
    "        if(give_accuracy_score):\n",
    "            print(f'accuracy_score: {accuracy_score(y_true, y_pred)}')\n",
    "        if(give_precision_score):\n",
    "            print(f'precision_score: {precision_score(y_true, y_pred)}')\n",
    "        if(give_recall_score):\n",
    "            print(f'recall_score: {recall_score(y_true, y_pred)}')\n",
    "        # if(give_hinge_loss):\n",
    "        #     print(f'hinge_loss: {hinge_loss(y_true, y_pred)}')\n",
    "                \n",
    "    def evaluate_best_models(self, lightning_type: L.LightningModule, eval_dataloader,\n",
    "                             give_confusion_matrix: bool=True, \n",
    "                             give_report: bool=True, \n",
    "                             give_f1_score: bool=False, \n",
    "                             give_accuracy_score: bool=False, \n",
    "                             give_precision_score: bool=False, \n",
    "                             give_recall_score: bool=False, \n",
    "                             give_hinge_loss: bool=False,\n",
    "                             multi_class: bool=False, **kwargs):\n",
    "        self.lightning_model = lightning_type.load_from_checkpoint(rf'{self.trainer.checkpoint_callback.best_model_path}', map_location=None, hparams_file=None, strict=True, **kwargs).eval()\n",
    "        self.save_evaluation(eval_dataloader, 'best_model', give_confusion_matrix, give_report,\n",
    "                             give_f1_score, give_accuracy_score, give_precision_score, give_recall_score, give_hinge_loss, multi_class)\n",
    "            \n",
    "    def save_evaluation(self, eval_dataloader, name_prepend: str='',\n",
    "                    give_confusion_matrix: bool=True, \n",
    "                    give_report: bool=True, \n",
    "                    give_f1_score: bool=False, \n",
    "                    give_accuracy_score: bool=False, \n",
    "                    give_precision_score: bool=False, \n",
    "                    give_recall_score: bool=False, \n",
    "                    give_hinge_loss: bool=False,\n",
    "                    multi_class: bool=False\n",
    "                    ):\n",
    "            \n",
    "            test_metrics_path = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', f'{name_prepend}_test_metrics.txt')\n",
    "            \n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            self.lightning_model.eval()\n",
    "            self.lightning_model.model.eval()\n",
    "            self.torch_model.eval()\n",
    "            self.trainer.model.eval()\n",
    "            for X, y in eval_dataloader:\n",
    "                with torch.no_grad():\n",
    "                    y_p = self.lightning_model(X.to(self.device))\n",
    "                if type(y_p) is tuple:\n",
    "                    y_p = y_p[0]\n",
    "                \n",
    "                if multi_class:\n",
    "                    y_pred.append(y_p.detach().to(y.device))\n",
    "                    y_true.append(y)\n",
    "                else:\n",
    "                    y_pred.append((y_p>0).to(torch.int32).detach().to(y.device))\n",
    "                    y_true.append(y.to(torch.int32))\n",
    "                    \n",
    "            y_true = torch.concat(y_true)\n",
    "            y_pred = torch.concat(y_pred)\n",
    "            print(y_true.shape)\n",
    "            print(y_pred.shape)\n",
    "            if multi_class:\n",
    "                y_true_num = torch.argmax(y_true, dim=1)\n",
    "                y_pred_num = torch.argmax(y_pred, dim=1)\n",
    "            else:\n",
    "                y_true_num = y_true\n",
    "                y_pred_num = y_pred\n",
    "                \n",
    "            print(y_true_num.shape)\n",
    "            print(y_pred_num.shape)\n",
    "            with open(test_metrics_path, 'at+') as f:\n",
    "                if(give_confusion_matrix):\n",
    "                    print(f'confusion_matrix: \\n{confusion_matrix(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_report):\n",
    "                    print(classification_report(y_true_num, y_pred_num), file=f)\n",
    "                if(give_f1_score):\n",
    "                    if multi_class:\n",
    "                        print(f'f1_score: {f1_score(y_true_num, y_pred_num, average=None)}', file=f)\n",
    "                    else:\n",
    "                        print(f'f1_score: {f1_score(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_accuracy_score):\n",
    "                    print(f'accuracy_score: {accuracy_score(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_precision_score):\n",
    "                    if multi_class:\n",
    "                        print(f'precision: {precision_score(y_true_num, y_pred_num, average=None)}', file=f)\n",
    "                    else:\n",
    "                        print(f'precision: {precision_score(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_recall_score):\n",
    "                    if multi_class:\n",
    "                        print(f'recall: {recall_score(y_true_num, y_pred_num, average=None)}', file=f)\n",
    "                    else:\n",
    "                        print(f'recall: {recall_score(y_true_num, y_pred_num)}', file=f)\n",
    "                # if(give_hinge_loss):\n",
    "                #     print(f'hinge_loss: {hinge_loss(y_true_num, y_pred)}', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 128\n",
    "hidden_dim = 64\n",
    "embedding_dim = 64\n",
    "label_size = 1\n",
    "seed = 911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_size = 32\n",
    "# hidden_dim = 16\n",
    "# embedding_dim = 16\n",
    "# label_size = 1\n",
    "# seed = 911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LabelSmoothingLoss(nn.Module):\n",
    "#     \"\"\"\n",
    "#     >> from https://github.com/OpenNMT/\n",
    "#     With label smoothing,\n",
    "#     KL-divergence between q_{smoothed ground truth prob.}(w)\n",
    "#     and p_{prob. computed by model}(w) is minimized.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, label_smoothing, tgt_vocab_size, ignore_index=-100):\n",
    "#         assert 0.0 < label_smoothing <= 1.0\n",
    "#         self.ignore_index = ignore_index\n",
    "#         super(LabelSmoothingLoss, self).__init__()\n",
    "\n",
    "#         smoothing_value = label_smoothing / (tgt_vocab_size - 2)\n",
    "#         one_hot = torch.full((tgt_vocab_size,), smoothing_value)\n",
    "#         one_hot[self.ignore_index] = 0\n",
    "#         self.register_buffer('one_hot', one_hot.unsqueeze(0))\n",
    "\n",
    "#         self.confidence = 1.0 - label_smoothing\n",
    "\n",
    "#     def forward(self, output, target):\n",
    "#         \"\"\"\n",
    "#         output (FloatTensor): batch_size x n_classes\n",
    "#         target (LongTensor): batch_size\n",
    "#         \"\"\"\n",
    "#         model_prob = self.one_hot.repeat(target.size(0), 1)\n",
    "#         model_prob.scatter_(1, target.unsqueeze(1), self.confidence)\n",
    "#         model_prob.masked_fill_((target == self.ignore_index).unsqueeze(1), 0)\n",
    "#         print(output.shape, model_prob.shape)\n",
    "#         return F.kl_div(output, model_prob, reduction='sum')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "def train_model(epochs=30, dropout=0.25, weight_decay=0.000012, lr=0.0002, amsgrad=False, fused=True, use_positional_encoder=[False, False, False]):\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    classifier_torch_model = CNN_for_Text_No_Positional_Encoding(num_embedding=num_embedding, hidden_dim=hidden_dim, embedding_dim=embedding_dim, pos_emb_size=4096, dropout=dropout, num_out_features=len(class_id), seed=seed, random_edges=6, lattice_edges=10, lattice_step=2, virtual_nodes=0, lattice_start_distance=2).to(device)\n",
    "    # optimizer = torch.optim.Adam(classifier_torch_model.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=amsgrad, fused=fused)\n",
    "    optimizer = torch.optim.AdamW(classifier_torch_model.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=amsgrad, fused=fused)\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 100, 150, 200, 250, 300, 350],gamma=0.5, verbose=False)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15, 20, 30, 40, 45,50,55],gamma=0.5, verbose=False)\n",
    "    loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "    classfier_lightning_model = CnnGnnClassifierLightningModel(classifier_torch_model, \n",
    "                                                        num_classes=len(class_id),\n",
    "                                                learning_rate=lr,\n",
    "                                                batch_size=batch_size,\n",
    "                                                optimizer=optimizer,\n",
    "                                                loss_func=loss_func,\n",
    "                                                lr_scheduler=lr_scheduler,\n",
    "                                                user_lr_scheduler=True\n",
    "                                                ).to(device)\n",
    "\n",
    "\n",
    "    model_manager = ClassifierModelManager(classifier_torch_model, classfier_lightning_model, log_name=f'CNN-GNN_{use_positional_encoder[0]}_{use_positional_encoder[1]}_{use_positional_encoder[2]}',device=device, num_train_epoch=epochs, accumulate_grad_batches=1)\n",
    "    # trainer = L.Trainer(\n",
    "    #             # callbacks=callbacks,\n",
    "    #             max_epochs=epochs,\n",
    "    #             accelerator= 'gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    #             logger=CSVLogger(save_dir='logs/', name='log2'), \n",
    "    #             num_sanity_val_steps=0,\n",
    "    #         #     default_root_dir='models\\model2_word_embedding-256-2'\n",
    "    #         )\n",
    "\n",
    "    train_dataset.reset_params()\n",
    "    train_dataset.position_j = 0\n",
    "    test_dataset.reset_params()\n",
    "    test_dataset.position_j = 0\n",
    "    \n",
    "    # train_dataset.section_i = 0\n",
    "    # train_dataset.each_section_i = np.zeros((train_dataset.num_sections, ), dtype=int)\n",
    "    # test_dataset.section_i = 0\n",
    "    # test_dataset.each_section_i = np.zeros((test_dataset.num_sections, ), dtype=int)\n",
    "    \n",
    "    model_manager.fit(train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)\n",
    "    model_manager.save_plot_csv_logger(loss_names=['train_loss_epoch', 'val_loss_epoch'], eval_names=['train_acc_epoch', 'val_acc_epoch'], name_prepend=f'tests_{dropout}_{weight_decay}_{lr}_{amsgrad}_{fused}')\n",
    "    model_manager.torch_model = model_manager.torch_model.to(device)\n",
    "    model_manager.save_evaluation(test_dataloader, f'{dropout}_{weight_decay}_{lr}]',True, True, True, True, True, True, True, multi_class=True)\n",
    "    # trainer.fit(classfier_lightning_model, train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)\n",
    "    classfier_lightning_model.model = classfier_lightning_model.model.eval()\n",
    "    classfier_lightning_model = classfier_lightning_model.eval()\n",
    "    calculate_metrics(classfier_lightning_model, test_dataloader)\n",
    "    model_manager.evaluate_best_models(CnnGnnClassifierLightningModel, test_dataloader,True, True, True, True, True, True, True, multi_class=True, model=classifier_torch_model, num_classes=len(class_id))\n",
    "    return model_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'loss_func' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_func'])`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3080') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                                | Params | Mode \n",
      "--------------------------------------------------------------------------\n",
      "0 | model     | CNN_for_Text_No_Positional_Encoding | 262 K  | train\n",
      "1 | loss_func | LabelSmoothingLoss                  | 0      | train\n",
      "2 | train_acc | MulticlassAccuracy                  | 0      | train\n",
      "3 | val_acc   | MulticlassAccuracy                  | 0      | train\n",
      "4 | test_acc  | MulticlassAccuracy                  | 0      | train\n",
      "--------------------------------------------------------------------------\n",
      "262 K     Trainable params\n",
      "0         Non-trainable params\n",
      "262 K     Total params\n",
      "1.049     Total estimated model params size (MB)\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d9e001f153459fbb329b2078f933ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "scatter(): Expected dtype int64 for index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\Articles\\CGNet\\FindBestModel\\7_FurtherImprovements\\with_positional_encoding_label_smoothing.ipynb Cell 68\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_manager \u001b[39m=\u001b[39m train_model(\u001b[39m70\u001b[39;49m, \u001b[39m0.2\u001b[39;49m, \u001b[39m0.000012\u001b[39;49m, \u001b[39m0.0032\u001b[39;49m, use_positional_encoder\u001b[39m=\u001b[39;49m[\u001b[39mFalse\u001b[39;49;00m, \u001b[39mFalse\u001b[39;49;00m, \u001b[39mFalse\u001b[39;49;00m])\n",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\Articles\\CGNet\\FindBestModel\\7_FurtherImprovements\\with_positional_encoding_label_smoothing.ipynb Cell 68\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m test_dataset\u001b[39m.\u001b[39mposition_j \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# train_dataset.section_i = 0\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# train_dataset.each_section_i = np.zeros((train_dataset.num_sections, ), dtype=int)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# test_dataset.section_i = 0\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# test_dataset.each_section_i = np.zeros((test_dataset.num_sections, ), dtype=int)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m model_manager\u001b[39m.\u001b[39;49mfit(train_dataloaders\u001b[39m=\u001b[39;49mtrain_dataloader, val_dataloaders\u001b[39m=\u001b[39;49mtest_dataloader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m model_manager\u001b[39m.\u001b[39msave_plot_csv_logger(loss_names\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mtrain_loss_epoch\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mval_loss_epoch\u001b[39m\u001b[39m'\u001b[39m], eval_names\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mtrain_acc_epoch\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mval_acc_epoch\u001b[39m\u001b[39m'\u001b[39m], name_prepend\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtests_\u001b[39m\u001b[39m{\u001b[39;00mdropout\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mweight_decay\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mlr\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mamsgrad\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mfused\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m model_manager\u001b[39m.\u001b[39mtorch_model \u001b[39m=\u001b[39m model_manager\u001b[39m.\u001b[39mtorch_model\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\Articles\\CGNet\\FindBestModel\\7_FurtherImprovements\\with_positional_encoding_label_smoothing.ipynb Cell 68\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mmax_epochs \u001b[39m=\u001b[39m max_epochs\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39m# self.max_epochs = max_epochs\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     \u001b[39m# self.trainer = self._create_trainer()\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mfit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_model,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m                  datamodule\u001b[39m=\u001b[39;49mdatamodule,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m                  train_dataloaders\u001b[39m=\u001b[39;49mtrain_dataloaders,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m                  val_dataloaders\u001b[39m=\u001b[39;49mval_dataloaders,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m                  ckpt_path \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m                  )\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[0;32m    542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 543\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    544\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    545\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    573\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    574\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[0;32m    575\u001b[0m     ckpt_path,\n\u001b[0;32m    576\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    577\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    578\u001b[0m )\n\u001b[1;32m--> 579\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[0;32m    581\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    582\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:986\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    983\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 986\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m    988\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    991\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1030\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1028\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1029\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1030\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m   1031\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 205\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance()\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    207\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    362\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[0;32m    139\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(data_fetcher)\n\u001b[0;32m    141\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[0;32m    142\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[0;32m    249\u001b[0m         \u001b[39m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[1;32m--> 250\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mautomatic_optimization\u001b[39m.\u001b[39;49mrun(trainer\u001b[39m.\u001b[39;49moptimizers[\u001b[39m0\u001b[39;49m], batch_idx, kwargs)\n\u001b[0;32m    251\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    252\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_optimization\u001b[39m.\u001b[39mrun(kwargs)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[1;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m         closure()\n\u001b[0;32m    185\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(batch_idx, closure)\n\u001b[0;32m    192\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[0;32m    193\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[1;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_ready()\n\u001b[0;32m    267\u001b[0m \u001b[39m# model hook\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m call\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[0;32m    269\u001b[0m     trainer,\n\u001b[0;32m    270\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    271\u001b[0m     trainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[0;32m    272\u001b[0m     batch_idx,\n\u001b[0;32m    273\u001b[0m     optimizer,\n\u001b[0;32m    274\u001b[0m     train_step_and_backward_closure,\n\u001b[0;32m    275\u001b[0m )\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n\u001b[0;32m    278\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:159\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    156\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[0;32m    158\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 159\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    161\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    162\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\core\\module.py:1308\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[0;32m   1277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[0;32m   1278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1279\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1282\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1283\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1284\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[39m    the optimizer.\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1306\u001b[0m \n\u001b[0;32m   1307\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1308\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\core\\optimizer.py:153\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    152\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strategy\u001b[39m.\u001b[39;49moptimizer_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer, closure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    155\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_after_step()\n\u001b[0;32m    157\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:238\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule)\n\u001b[1;32m--> 238\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision_plugin\u001b[39m.\u001b[39;49moptimizer_step(optimizer, model\u001b[39m=\u001b[39;49mmodel, closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision.py:122\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[1;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[0;32m    121\u001b[0m closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[1;32m--> 122\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     74\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 75\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    392\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adamw.py:165\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m--> 165\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    167\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[0;32m    168\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision.py:108\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[1;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_closure\u001b[39m(\n\u001b[0;32m     96\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     97\u001b[0m     model: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     98\u001b[0m     optimizer: Optimizer,\n\u001b[0;32m     99\u001b[0m     closure: Callable[[], Any],\n\u001b[0;32m    100\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    101\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[39m    hook is called.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m \n\u001b[0;32m    107\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     closure_result \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    109\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer)\n\u001b[0;32m    110\u001b[0m     \u001b[39mreturn\u001b[39;00m closure_result\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39m@override\u001b[39m\n\u001b[0;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclosure(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    145\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:129\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[39m@override\u001b[39m\n\u001b[0;32m    127\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39menable_grad()\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ClosureResult:\n\u001b[1;32m--> 129\u001b[0m     step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_fn()\n\u001b[0;32m    131\u001b[0m     \u001b[39mif\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarning_cache\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:317\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \n\u001b[0;32m    308\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m \n\u001b[0;32m    314\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    315\u001b[0m trainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\n\u001b[1;32m--> 317\u001b[0m training_step_output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mtraining_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[0;32m    318\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mpost_training_step()  \u001b[39m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[39mif\u001b[39;00m training_step_output \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m trainer\u001b[39m.\u001b[39mworld_size \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:311\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 311\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    313\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    314\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module:\n\u001b[0;32m    389\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_redirection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39m\"\u001b[39m\u001b[39mtraining_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 390\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module\u001b[39m.\u001b[39;49mtraining_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\Articles\\CGNet\\FindBestModel\\7_FurtherImprovements\\with_positional_encoding_label_smoothing.ipynb Cell 68\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m y_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(X)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_func(y_out\u001b[39m.\u001b[39;49mview(y\u001b[39m.\u001b[39;49mshape), y )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     loss,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     on_step\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\Articles\\CGNet\\FindBestModel\\7_FurtherImprovements\\with_positional_encoding_label_smoothing.ipynb Cell 68\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39moutput (FloatTensor): batch_size x n_classes\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mtarget (LongTensor): batch_size\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m model_prob \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mone_hot\u001b[39m.\u001b[39mrepeat(target\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m model_prob\u001b[39m.\u001b[39;49mscatter_(\u001b[39m1\u001b[39;49m, target\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfidence)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m model_prob\u001b[39m.\u001b[39mmasked_fill_((target \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignore_index)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), \u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mkl_div(output, model_prob, reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: scatter(): Expected dtype int64 for index"
     ]
    }
   ],
   "source": [
    "model_manager = train_model(70, 0.2, 0.000012, 0.0032, use_positional_encoder=[False, False, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculatge_metrics(chpt_path, target_data_loader, num_embedding):\n",
    "        classifier_torch_model = CNN_for_Text_No_Positional_Encoding(num_embedding=num_embedding, hidden_dim=hidden_dim, embedding_dim=embedding_dim, pos_emb_size=4096, dropout=0.2, num_out_features=len(class_id), seed=seed, random_edges=4, lattice_edges=8, lattice_step=2, virtual_nodes=0, lattice_start_distance=2)\n",
    "        classfier_lightning_model = CnnGnnClassifierLightningModel.load_from_checkpoint(chpt_path, model=classifier_torch_model, num_classes=len(class_id)).eval()\n",
    "        mean_infer_acc = []\n",
    "        mean_infer_f1 = []\n",
    "        mean_infer_prec = []\n",
    "        mean_infer_rec = []\n",
    "        for i in range(5):\n",
    "            all_ys = []\n",
    "            all_y_preds = []\n",
    "            for X, y in target_data_loader:\n",
    "                with torch.no_grad():\n",
    "                    y_pred = classfier_lightning_model(X.to(device))\n",
    "                all_ys.append(torch.argmax(y,dim=1))\n",
    "                all_y_preds.append(torch.argmax(y_pred.cpu(), dim=1))\n",
    "            all_ys = torch.concat(all_ys)\n",
    "            all_y_preds = torch.concat(all_y_preds)\n",
    "            \n",
    "            cm = confusion_matrix(all_ys, all_y_preds)\n",
    "            \n",
    "            accuracy = np.sum(np.diag(cm))/ np.sum(cm)\n",
    "            precision = np.mean(np.diag(cm) / np.sum(cm, axis=0))\n",
    "            recall = np.mean(np.diag(cm) / np.sum(cm, axis=1))\n",
    "            f1_score = (2*precision*recall)/(precision + recall)\n",
    "            \n",
    "            mean_infer_acc.append(accuracy)\n",
    "            mean_infer_f1.append(f1_score)\n",
    "            mean_infer_prec.append(precision)\n",
    "            mean_infer_rec.append(recall)\n",
    "        mean_infer_acc = torch.mean(torch.tensor(mean_infer_acc))\n",
    "        mean_infer_f1 = torch.mean(torch.tensor(mean_infer_f1))\n",
    "        mean_infer_prec = torch.mean(torch.tensor(mean_infer_prec))\n",
    "        mean_infer_rec = torch.mean(torch.tensor(mean_infer_rec))\n",
    "        return mean_infer_acc, mean_infer_f1, mean_infer_prec, mean_infer_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "def get_best_chpt(metrics_path, epoch_numbers):\n",
    "    epoch_data = pd.read_csv(metrics_path)\n",
    "    if 'val_acc_epoch' in epoch_data.columns and epoch_data['val_acc_epoch'].notna().any():\n",
    "        best_chpt = epoch_data.loc[epoch_data['val_acc_epoch'].idxmax()]\n",
    "    elif 'val_loss_epoch' in epoch_data.columns and epoch_data['val_loss_epoch'].notna().any():\n",
    "        best_chpt = epoch_data.loc[epoch_data['val_loss_epoch'].idxmin()]\n",
    "    else:\n",
    "        raise ValueError(f\"No valid validation metrics available for epoch {epoch_numbers}.\")\n",
    "    return np.argwhere(np.array(epoch_numbers)==best_chpt['epoch']).item(), best_chpt['val_loss_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_metrics_mean(num_embedding, base_path = 'logs\\CNN-GNN18_mr2k_seeds', start=0, interval=1):\n",
    "    total_accuracy = []\n",
    "    total_f1 = []\n",
    "    total_prec = []\n",
    "    total_rec = []\n",
    "    total_loss = []\n",
    "    \n",
    "    for i in range(start, start + interval):\n",
    "        version_path = join(base_path, f'version_{i}')\n",
    "        checkpoint_path = join(version_path, f'checkpoints')\n",
    "        onlyfiles  = [f for f in listdir(checkpoint_path) if (isfile(join(checkpoint_path, f)) and 'epoch' in f) ]\n",
    "        epoch_numbers = [int(re.search(r'\\d+', f).group()) for f in onlyfiles]\n",
    "        best_chpt_id, loss = get_best_chpt(join(version_path, 'metrics.csv'), epoch_numbers)\n",
    "        print(onlyfiles[best_chpt_id])\n",
    "        mean_infer_acc, mean_infer_f1, mean_infer_prec, mean_infer_rec = calculatge_metrics(join(checkpoint_path, f'{onlyfiles[best_chpt_id]}'), test_dataloader, num_embedding)\n",
    "            \n",
    "        total_accuracy.append(mean_infer_acc)\n",
    "        total_f1.append(mean_infer_f1)\n",
    "        total_prec.append(mean_infer_prec)\n",
    "        total_rec.append(mean_infer_rec)\n",
    "        total_loss.append(loss)\n",
    "\n",
    "    total_accuracy = torch.mean(torch.tensor(total_accuracy))\n",
    "    total_f1 = torch.mean(torch.tensor(total_f1))\n",
    "    total_prec = torch.mean(torch.tensor(total_prec))\n",
    "    total_rec = torch.mean(torch.tensor(total_rec))\n",
    "    total_loss = torch.mean(torch.tensor(total_loss))\n",
    "    print(f'total_accuracy: {total_accuracy}')\n",
    "    print(f'total_f1: {total_f1}')\n",
    "    print(f'total_prec: {total_prec}')\n",
    "    print(f'total_rec: {total_rec}')\n",
    "    print(f'total_loss: {total_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=16-step=1666.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'loss_func' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_func'])`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_accuracy: 0.49994419642857146\n",
      "total_f1: 0.39090660134256133\n",
      "total_prec: 0.33331539162468354\n",
      "total_rec: 0.49996012377109383\n",
      "total_loss: 0.2354898452758789\n"
     ]
    }
   ],
   "source": [
    "calculate_average_metrics_mean(212, r'logs\\CNN-GNN_False_False_False', start=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_average_metrics_mean(r'logs\\CNN-GNN_False_False_False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import captum\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from captum.attr import LayerIntegratedGradients, TokenReferenceBase, visualization, IntegratedGradients\n",
    "from torch_geometric.nn.models.captum import to_captum_model, CaptumModel\n",
    "from torch_geometric.explain.algorithm import CaptumExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(num_embedding, base_path = 'logs\\CNN-GNN18_mr2k_seeds', version=0, n_steps=50, step_of_test=0):\n",
    "        version_path = join(base_path, f'version_{version}')\n",
    "        checkpoint_path = join(version_path, f'checkpoints')\n",
    "        onlyfiles  = [f for f in listdir(checkpoint_path) if (isfile(join(checkpoint_path, f)) and 'epoch' in f) ]\n",
    "        epoch_numbers = [int(re.search(r'\\d+', f).group()) for f in onlyfiles]\n",
    "        best_chpt_id, loss = get_best_chpt(join(version_path, 'metrics.csv'), epoch_numbers)\n",
    "        chpt_path = join(checkpoint_path, f'{onlyfiles[best_chpt_id]}')\n",
    "        classifier_torch_model = CNN_for_Text_No_Positional_Encoding(num_embedding=num_embedding, hidden_dim=hidden_dim, embedding_dim=embedding_dim, pos_emb_size=4096, dropout=0.2, num_out_features=len(class_id), seed=seed, random_edges=4, lattice_edges=8, lattice_step=2, virtual_nodes=0, lattice_start_distance=2, isXaiTests=True, num_tests=n_steps, step_of_test=step_of_test)\n",
    "        classfier_lightning_model = CnnGnnClassifierLightningModel.load_from_checkpoint(chpt_path, model=classifier_torch_model, num_classes=len(class_id)).eval()\n",
    "        return classfier_lightning_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'loss_func' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_func'])`.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(212, r'logs\\CNN-GNN_False_False_False', 22, n_steps=50, step_of_test=i).eval()\n",
    "model = model.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_for_Text_No_Positional_Encoding(\n",
       "  (embedding): Embedding(212, 64)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (conv1): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (conv3): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (conv4): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (sentiment1): Sentiment_Injection(\n",
       "    (conv1): Conv1d(2, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (sentiment2): Sentiment_Injection(\n",
       "    (conv1): Conv1d(2, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (gcnn1): GCNN(\n",
       "    (gnn): GATv2Conv(64, 8, heads=4)\n",
       "    (conv): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (fc): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (gcnn2): GCNN(\n",
       "    (gnn): GATv2Conv(128, 16, heads=4)\n",
       "    (conv): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (fc): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (graph_generator): GenGraph(\n",
       "    (virtual_node_embeddings): Embedding(0, 64)\n",
       "  )\n",
       "  (fc0): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=32, bias=True)\n",
       "  (fc_out): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accumalate couple samples in this array for visualization purposes\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "def prepare_input_indices(graph, id_char_dict):\n",
    "    \n",
    "    characters = [id_char_dict[i.item()] for i in graph.x]\n",
    "    tokens = ['']*len(graph.token_indices)\n",
    "    for i, j in enumerate(graph.token_indices):\n",
    "        j = j.item()\n",
    "        tokens[j] += id_char_dict[graph.x[i].item()]\n",
    "    ref_graph = deepcopy(graph)\n",
    "    ref_graph.x = torch.ones_like(graph.x) * list(id_char_dict.keys())[-1]\n",
    "\n",
    "    return graph, ref_graph, tokens, characters\n",
    "\n",
    "token_reference = TokenReferenceBase(reference_token_idx=211)\n",
    "\n",
    "def interpret_sentence(lig, model, graph, labels, id_char_dict, min_len = 7, label = 0, visualize_on_tokens=False, n_steps=50, dev='cpu', return_delta=True):\n",
    "    #labels = labels.argmax().item()\n",
    "    graph.cumulative_token_indices = graph.token_indices\n",
    "    graph.edge_index = torch.zeros((2, 0))\n",
    "    # print(f'1: {graph.character_length.shape}')\n",
    "    graph, ref_graph, tokens, characters = prepare_input_indices(graph, id_char_dict)\n",
    "    #[tok.text for tok in tokenizer(sentence.lower())]\n",
    "    \n",
    "    graph = Batch.from_data_list([graph])\n",
    "    ref_graph = Batch.from_data_list([ref_graph])\n",
    "    \n",
    "    model.zero_grad()\n",
    "    # predict\n",
    "    ref_graph = ref_graph.to(dev)\n",
    "    graph = graph.to(dev)\n",
    "    model = model.to(dev)\n",
    "    \n",
    "    \n",
    "    model_output = model(graph.x, graph.edge_index, graph.token_subsampling_probabilities, graph.cumulative_token_indices, graph.token_sentiments, graph.token_lengths, graph.num_tokens, graph.character_length, graph.token_embeddings)\n",
    "    # print(f'graph: {graph}, model out dim: {model.num_out_features}, n_steps: {n_steps}')\n",
    "    # print(f'model_output.shape: {model_output.shape}')\n",
    "    pred = torch.softmax(model_output, dim=1).detach()\n",
    "    pred_ind = torch.argmax(pred).item()\n",
    "    # print(f'pred: {pred}, pred_ind: {pred_ind}')\n",
    "\n",
    "    seq_length = min_len\n",
    "    # generate reference indices for each sample\n",
    "    reference_indices = token_reference.generate_reference(seq_length, device=device).unsqueeze(0)\n",
    "\n",
    "    # compute attributions and approximation delta using layer integrated gradients\n",
    "    output = lig.attribute(graph.x, ref_graph.x, \\\n",
    "                                           additional_forward_args=(graph.edge_index, graph.token_subsampling_probabilities, graph.cumulative_token_indices, graph.token_sentiments, graph.token_lengths, graph.num_tokens, graph.character_length, graph.token_embeddings), \\\n",
    "                                           n_steps=n_steps, return_convergence_delta=return_delta, target=labels)\n",
    "    \n",
    "    if(return_delta):\n",
    "        attributions_ig, delta = output\n",
    "    else:\n",
    "        # print(f'outputoutput: {output}')\n",
    "        attributions_ig = output\n",
    "        # print(f'attributions_ig: {attributions_ig}')\n",
    "        delta = 0\n",
    "        \n",
    "    # captum_exp = CaptumExplainer(IntegratedGradients)\n",
    "\n",
    "    # print(f'true: {id_class[labels]}({labels}), pred: {id_class[pred_ind]}({pred_ind}), max delta: {delta}')\n",
    "    \n",
    "    if(type(attributions_ig) is tuple):\n",
    "        # print(f'attributions_ig: {attributions_ig[0].shape}, {attributions_ig[1].shape}')\n",
    "        attributions_ig = attributions_ig[0]\n",
    "    # else:\n",
    "    #     print(f'attributions_ig: {attributions_ig.shape}')\n",
    "    # print(f'text: {tokens}, attributions_ig: {attributions_ig.shape}, labels: {labels}, delta: {delta}')\n",
    "    # print(f'attributions_ig: {attributions_ig.shape}')\n",
    "    return attributions_ig, tokens, characters, pred, pred_ind, label, delta\n",
    "            \n",
    "def add_attributions_to_visualizer(attributions, text, pred, pred_ind, labels, delta, vis_data_records):\n",
    "    attributions = attributions.sum(dim=1)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    attributions = attributions.cpu().detach().numpy()\n",
    "    # print(attributions.shape)\n",
    "    # storing couple samples in an array for visualization purposes\n",
    "    vis_data_records.append(visualization.VisualizationDataRecord(\n",
    "                            attributions,\n",
    "                            pred,\n",
    "                            id_class[pred_ind],\n",
    "                            id_class[labels],\n",
    "                            id_class[1],\n",
    "                            attributions.sum(),\n",
    "                            text,\n",
    "                            delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10785\n",
      "[10785  5931 21341  2054  4867  6064 14618  6301 20927 10541]\n",
      "The following \"review\" is one from the wrong side of the tracks, meaning two things: You will hear nothing but praises coming from me and don't be fooled by my rating. I also could tell you that this review contains no spoilers, but technically... that's a lie.<br /><br />Well,... Screw the review. I'm just going to ramble a bit. It has been a while since I had so much fun with such a bad film. And if you cannot enjoy this piece of utter drivel, then you simply have no sense of humor. First off, this movie was meant to be taken seriously, and that's the main aspect were the fun is coming from. The story? A doctor's son is terminally ill. Daddy-Doctor decides that a hart-transplant must be the only way of saving his son. So, in true mad-doctor-style, he decides that the heart of a gorilla will do the trick. Of course, the gorilla-heart is \"much too potent for a human\", so sonny-boy transforms into this hideous Ape-Man that immediately breaks free and goes on a killing and raping spree (because that's simply what any horny beast would do, isn't it?). The make-up of our Ape-Man is hilarious. There's simply no other way of putting it: The guys' face looks like a turd! I'm talking human excrement here, the solid brown kind! Beautiful! The gore in this flick is wonderful too: Aside from real footage of an open-heart surgery, we also have incredibly fake (but quite nasty) looking blood & gore effects: a decapitation, an eye-gouging, a throat-ripping, the scalping of someone's skull,... It's hilarious, and indeed it's quite incomprehensible why NIGHT OF THE BLOODY APES ever made it to the notorious UK Video-Nasties list, because all of the nasty things portrayed in this film are simply too ridiculous to be taken seriously. Oh, and there's only one bloody ape running around in it too, by the way. So, needless to say that LA HORRIPILANTE BESTIA HUMANA is a much more accurate title for this terrifying and convincing tale of science gone horribly wrong.<br /><br />When not trying to shock with poorly staged acts of bloody violence or random portrayals of female nudity, this movie manages to be highly entertaining with ingredients like nonsensical dialogues, cheap cardboard sets, plot stupidity and incredibly bad acting. Just a few random examples, maybe? About the sets: One can clearly see that the set-designers just used the same walls, doors, windows (etc.) over and over again to be build various different interior sets (e.g.: One side of the hospital room - the window side - where the unconscious wrestling girl lies, looks suspiciously similar like the window side of the basement-laboratory from where Ape-Man escapes; The set-designers also took one of the side-windows from the laboratory, made it a bit larger and just placed it in the conference-room of the hospital; All the walls in any building are the same grey-ones;...). Then there's the setting of the park. Clearly shot in a studio, you can see (on several occasions) that the grass is loose. Whenever there's some struggling/fighting/raping going on, the grass just shifts and shuffles and you can see the grey concrete from the studio-floor beneath it.<br /><br />Just a few random lines that come out the actors' mouths: <br /><br />-- In the conference-room where all the doctors are debating the disappearance of Unconscious Wrestling Girl (a disappearance that would of course mean bad publicity for the hospital), Daddy-Doctor intelligently utters \"We find ourselves in a situation that is difficult\".<br /><br />-- During that same debate a colleague-doctor cleverly remarks \"A sleepwalker! Any sleepwalker gets up.\", hereby providing a solid excuse for the disappearance of Unconscious Wrestling Girl.<br /><br />-- After our investigating detective, through the amazing process of his own logical deduction, concludes and tells his superior that the murderer must be a half man/half beast, his superior answers that it's absurd, adding the line \"It's more probable that of late, more and more, you're watching on your television many of those pictures of terror\"... Truly one of the best lines of the movie.<br /><br />Other sources of laughter: <br /><br />-- The two scenes were Daddy-Doctor and his Igor-like assistant kidnap the gorilla from the zoo and Unconscious Wrestling Girl from the hospital - these well thought-out acts of abduction are like taking candy from a baby.<br /><br />-- Daddy-Doctor speaks to God a lot, doesn't he?<br /><br />-- Sonny-boy calling Daddy-Doctor \"Papa\" on more than one occasion.<br /><br />-- An old lady screaming \"Aaargh!!! A dead man! A dead Man! A Dead Man!! A DEAD MAN!!!\".<br /><br />-- The plot periodically stops to wallow in scenes of women wrestling, only to go on again and do nothing with that concept. Sure, Daddy-Doctor replaces Sonny-Boy's gorilla-heart a second time with that of Unconscious Wrestling Girl, but do you think something spectacular happens after that? Like Ape-Turd-Man growing breasts or something, trying to rape men this time? Our leading living wrestling beauty (Norma Lazareno) doesn't even go into a climactic wrestling contest with Ape-Turd-Man near the movie's finale... But Ape-Turd-Man does start to show some motherly love near the end... almost (and I say \"almost\") in true KING KONG-style (i.e. the top of a building and people on the ground pointing and screaming).<br /><br />Okay, I think that's enough now. I chipped in more than my two cents here. Vomitron's Rational Rating for this sleazy piece of hilarious dreck: 2/10. Vomitron's rating From the Wrong Side of the Tracks: 8/10. Go see this film, people. It is well worth it!\n",
      "Negative\n",
      "Remnants of an ambushed Army unit hook up with a group of cowboys to fight their way through Indians on the warpath. Sounds like it could be an exciting western, but this one is dull, dull, dull. It moves like molasses, the action scenes are uninspired, the acting is pedestrian, the writing is flat, even the photography isn't very good. Eastwood, in a very early role, plays an ex-Confederate who doesn't like the idea of fighting on the same side as Yankees. That's about the only remotely interesting situation in the whole movie, but Eastwood wasn't experienced enough an actor to pull it off, and his character comes across as petulant rather than angry or embittered. A very ordinary western. Actually, a very less-than-ordinary western. Worth a look if you're a die-hard Eastwood fan and want to see him at the very beginning of his career. Otherwise, don't bother.\n",
      "Negative\n",
      "This movie has too many things going on. Another reviewer comments on the disjointed, episodic nature of the film as reflecting the director's memories - that's fine, if that is how it was written and performed. Instead, what we get is straight-forward narrative - some of the time - that jumps around, under and over, leaves us dangling in some instances, interrupts the flow with unnecessary digressions in other instances, and otherwise simply doesn't work. <br /><br />There are also some plot details that just don't work. For example, why drag a body onto a beach in an urban area in broad daylight, as opposed to night time? Why leave your flat sheet on the body? Why would an artist who knew the Joe character for a brief time decide to leave him \"everything\" (even if it wasn't much)? This sub-plot was poorly developed to make that point work. For that matter, why even have the man be an invalid or an artist other than to provide the money and the gratuitous nude posing scenes? He could just as easily have been a photographer, or a opera composer? For that matter, how does someone rate an apartment in an Opera House - particularly without some clear connection to the Opera? The coincidences are also both too obvious and to unclear and unexplained. Why would the guys take everything in the warehouse and \"disappear.\" If Tim was a 10 year old school mate in a town as small as Bangor, how could Joe lose track of him for 8 years, especially if they knew each other well enough that one would recommend the other for a job. <br /><br />Some of the other subplots (like the mother and her boyfriend(s) and the sister wanting to escape felt like padding. There's some good ideas that might have made a feature with full development or could have been interesting shorts. As completed, this movie made little sense and offers even less.\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "test_lengths = np.array([len(v) for v in test_df.Content.values])\n",
    "test_lengths[test_lengths.argmin()]\n",
    "print(test_lengths.argmin())\n",
    "print(np.argsort(test_lengths, 0)[:10])\n",
    "print(test_df.iloc[1056].Content)\n",
    "print(id_class[test_df.iloc[1056].Topic])\n",
    "print(test_df.iloc[467].Content)\n",
    "print(id_class[test_df.iloc[467].Topic])\n",
    "print(test_df.iloc[284].Content)\n",
    "print(id_class[test_df.iloc[284].Topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_token = True\n",
    "need_transpose = True\n",
    "model_layers = [\n",
    "    (lambda m: m.embedding, not on_token, not need_transpose), \n",
    "    (lambda m: m.conv1, not on_token, need_transpose), \n",
    "    (lambda m: m.conv2, not on_token, need_transpose), \n",
    "    (lambda m: m.conv3, on_token, need_transpose), \n",
    "    (lambda m: m.sentiment1, on_token, not need_transpose),\n",
    "    (lambda m: m.gcnn1, on_token, not need_transpose),\n",
    "    (lambda m: m.gcnn2, on_token, not need_transpose),\n",
    "    # (lambda m: m.fc0, on_token, need_transpose)\n",
    "    ]\n",
    "\n",
    "def create_layers_attributions(content, label, file_name='result_5', n_steps=50):\n",
    "    vis_data_records_ig = []\n",
    "    true_label = torch.tensor(label)\n",
    "    graph = test_dataset.content_to_graph(content, subsampling_equation_sigmoid)\n",
    "    graph.cumulative_token_indices = test_dataset.caluculate_batch_token_positions(torch.tensor([graph.num_tokens]), torch.tensor([graph.character_length]), graph.token_indices)\n",
    "    for i, (ml, on_t, need_transpose) in enumerate(model_layers):\n",
    "        print(f'>>>>>>>>>>>>>>>>>>>> {i} <<<<<<<<<<<<<<<<<<<<<<')\n",
    "        # if i!=6: continue\n",
    "        model = load_model(212, r'logs\\CNN-GNN_False_False_False', 22, n_steps=n_steps, step_of_test=i)\n",
    "        model = model.model\n",
    "        lig = LayerIntegratedGradients(model, ml(model))\n",
    "        attributions_ig, tokens, characters, pred, pred_ind, label, delta = interpret_sentence(lig, model, graph, true_label, vocab_dict_rev, 7, 1, visualize_on_tokens=on_t, n_steps=n_steps, return_delta=False)\n",
    "        \n",
    "        attributions_ig = attributions_ig.T if need_transpose else attributions_ig \n",
    "        # print(on_t, len(tokens), len(characters), {attributions_ig.shape})\n",
    "        if on_t:\n",
    "            add_attributions_to_visualizer(attributions_ig, tokens[:attributions_ig.shape[0]], pred[0,pred_ind], pred_ind, true_label.item(), delta, vis_data_records_ig)\n",
    "        else:\n",
    "            add_attributions_to_visualizer(attributions_ig, characters, pred[0,pred_ind], pred_ind, true_label.item(), delta, vis_data_records_ig)\n",
    "        # time.sleep(1)\n",
    "            \n",
    "    # for i in range(len(vis_data_records_ig)):\n",
    "    #     print(i, vis_data_records_ig[i].word_attributions.shape)\n",
    "        \n",
    "    _ = visualization.visualize_text(vis_data_records_ig)\n",
    "    html = _.data\n",
    "    with open(rf'FindBestModel\\6_ReduceAttentionToStopWords\\VisualizationFiles\\{file_name}.html', 'w') as f:\n",
    "        f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_layers_attributions(\"The Claude Lelouch's movie is a pretty good moment of cinema. One of the most touching films about family and loneliness, and surely the best interpretation of French actor Jean-Paul Belmondo.\", 1, 'result_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_layers_attributions(\"Read the book, forget the movie!\", 0, 'result_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_layers_attributions(\"This is a great movie. Too bad it is not available on home video.\", 1, 'result_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_layers_attributions(\"\"\"Beyond Rangoon is one of the most emotional and intense films ever made. Superbly directed by John Boorman, and intensly acted by Patricia Arquette, this film can easily be called one of the best films of the 90's. The story and vivid characters just grab the audience from the very opening, and never lets go. After seeing the film, the viewer will never be able to forget \"Beyond Rangoon\". The film made little money at the box office, and is little known, but should be high profile. Watching it, you can tell that it was meant to be seen by a large audience. It is a very important and moving film, and should be seen by everyone.<br /><br />\"\"\", 1, 'result_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_layers_attributions(\"\"\"Beyond Rangoon is one of the most emotional and intense films ever made. Superbly directed by John Boorman, intensly acted by Patricia Arquette, this film can easily be called one of the best films of the 90's. The story and vivid characters just grab the audience from the very opening, and never lets go. After seeing the film, the viewer will never be able to forget \"Beyond Rangoon\". The film made little money at the box office, is little known, but should be high profile. Watching it, you can tell that it was meant to be seen by a large audience. It is a very important and moving film, and should be seen by everyone.\"\"\", 1, 'result_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_graph = []\n",
    "for row in test_df.values[:1000]:\n",
    "    rows_graph.append((Batch.from_data_list([test_dataset.content_to_graph(row[1], subsampling_equation_sigmoid)]), torch.tensor(row[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_wrong_samples(base_path, version_id, target_data_list, num_embedding):\n",
    "    \n",
    "        version_path = join(base_path, f'version_{version_id}')\n",
    "        checkpoint_path = join(version_path, f'checkpoints')\n",
    "        onlyfiles  = [f for f in listdir(checkpoint_path) if (isfile(join(checkpoint_path, f)) and 'epoch' in f) ]\n",
    "        epoch_numbers = [int(re.search(r'\\d+', f).group()) for f in onlyfiles]\n",
    "        best_chpt_id, loss = get_best_chpt(join(version_path, 'metrics.csv'), epoch_numbers)\n",
    "        chpt_path = join(checkpoint_path, f'{onlyfiles[best_chpt_id]}')\n",
    "        \n",
    "        classifier_torch_model = CNN_for_Text_No_Positional_Encoding(num_embedding=num_embedding, hidden_dim=hidden_dim, embedding_dim=embedding_dim, pos_emb_size=4096, dropout=0.2, num_out_features=len(class_id), seed=seed, random_edges=4, lattice_edges=8, lattice_step=2, virtual_nodes=0, lattice_start_distance=2).cpu()\n",
    "        classfier_lightning_model = CnnGnnClassifierLightningModel.load_from_checkpoint(chpt_path, model=classifier_torch_model, num_classes=len(class_id)).eval().cpu()\n",
    "\n",
    "        wrong_indices = []\n",
    "        for i, (X, y) in enumerate(target_data_list):\n",
    "            with torch.no_grad():\n",
    "                y_pred = classfier_lightning_model(X)\n",
    "            if torch.argmax(y_pred).item() != y.item():\n",
    "                wrong_indices.append(i)\n",
    "        return wrong_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_indices = find_wrong_samples(r'logs\\CNN-GNN_False_False_False', 22, rows_graph, 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, row in enumerate(test_df.iloc[wrong_indices].values):\n",
    "#     print(row)\n",
    "#     create_layers_attributions(row[1], row[0], f'wrong_result_{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 2), dtype=object)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.iloc[wrong_indices].values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
