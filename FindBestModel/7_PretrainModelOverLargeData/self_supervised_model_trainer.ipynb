{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import time\n",
    "from torch_scatter import scatter_max, scatter_mean, scatter_std\n",
    "from sklearn.model_selection import KFold\n",
    "import torchmetrics\n",
    "import lightning as L\n",
    "from torch_geometric.data import Batch, Data\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "from transformers import AutoModel, DebertaV2Tokenizer, AutoTokenizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_vocab = {v:k for k,v in tokenizer.vocab.items()}\n",
    "all_vocab_indices = list(id_vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# all_vocab_str = []\n",
    "# vector_keys = list(nlp.vocab.vectors.keys())\n",
    "# for i in range(len(vector_keys)):\n",
    "#     try:\n",
    "#         t = nlp.vocab.strings[vector_keys[i]]\n",
    "#         all_vocab_str.append(t)\n",
    "#     except:\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# Create a set of all English characters, numbers, and punctuation\n",
    "allowed_chars = string.ascii_letters + string.digits + string.punctuation\n",
    "all_chars = set(allowed_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'Data\\ReducedEmbeddings\\deberta_larg_reduced_embeddings_64.npy', 'rb') as f:\n",
    "    embeddings = np.load(f)\n",
    "embeddings = torch.from_numpy(embeddings)\n",
    "all_vocab_str = []\n",
    "for i in range(len(id_vocab)):\n",
    "    all_vocab_str.append(id_vocab[i])\n",
    "    \n",
    "# embeddings = (embeddings - torch.min(embeddings)) / (torch.max(embeddings)-torch.min(embeddings))\n",
    "token_vocab_dict = dict(zip(all_vocab_str, embeddings))\n",
    "# del all_vocab_str\n",
    "# del all_vocab_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(r'Data\\ReducedEmbeddings\\polarity_debertav3_tokens_gpt_mini_emb.npy', 'rb') as f:\n",
    "    polarities_subjectivities= np.load(f)\n",
    "polarities_subjectivities = torch.from_numpy(polarities_subjectivities)\n",
    "polarity_vocab_dict = dict(zip(all_vocab_str, polarities_subjectivities))\n",
    "polarity_vocab_dict['<n>'] = torch.tensor([0.0, 0.0])\n",
    "len(token_vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128001, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarities_subjectivities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1085 tensor([0.7000, 0.6000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_vocab_str)):\n",
    "    if 'nice' in all_vocab_str[i]:\n",
    "        print(i, polarities_subjectivities[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# Create a set of all English characters, numbers, and punctuation\n",
    "allowed_chars = string.ascii_letters + string.digits + string.punctuation + ' \\t'\n",
    "all_chars = set(allowed_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 256\n",
    "folder_path = r'C:\\Users\\fardin\\Projects\\CGNet\\Data\\TextClassification\\IMDB'\n",
    "# t_tokenizer = TweetTokenizer()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_ratio = 1.0\n",
    "test_df = pd.read_csv(r'data\\TextClassification\\IMDB\\test.csv')\n",
    "test_df['Topic'] = test_df['label']\n",
    "# test_df['Content'] = [test_df.text[i].values[0] + \" \\n \" + test_df.text[i].values[1] for i in range(len(test_df))]\n",
    "test_df['Content'] = test_df['text']\n",
    "# test_df['Content'] = test_df['Content']\n",
    "test_df.drop(['label', 'text'], axis=1, inplace=True)\n",
    "test_df.dropna(inplace=True)\n",
    "test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "test_df = test_df.iloc[:int(keep_ratio*test_df.shape[0])]\n",
    "train_df = pd.read_csv(r'data\\TextClassification\\IMDB\\train.csv')\n",
    "train_df['Topic'] = train_df['label']\n",
    "# train_df['Content'] = [train_df.text[i].values[0] + \" \\n \" + train_df.text[i].values[1] for i in range(len(train_df))]\n",
    "train_df['Content'] = train_df['text']\n",
    "train_df.drop(['label', 'text'], axis=1, inplace=True)\n",
    "train_df.dropna(inplace=True)\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "train_df = train_df.iloc[:int(keep_ratio*train_df.shape[0])]\n",
    "sst_classes = [\"Negative\", \"Positive\"]\n",
    "df = pd.DataFrame(np.concatenate([train_df.values, test_df.values]), columns=train_df.columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13704\n",
      "CPU times: total: 1.09 s\n",
      "Wall time: 1.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_lengths = np.array([len(df.Content[i]) for i in df.index])\n",
    "print(np.max(doc_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_list = df.Topic.unique()\n",
    "class_id = {c:i for i, c in enumerate(sst_classes)}\n",
    "id_class = {i:c for i, c in enumerate(sst_classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_dict = {c:i for i, c in enumerate(allowed_chars)}\n",
    "# if '\\x01' not in vocab_dict:\n",
    "#     vocab_dict['\\x01'] = len(vocab_dict)\n",
    "# char_Set = set(vocab_dict.keys())\n",
    "# len(char_Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13704\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a histogram\n",
    "plt.hist(doc_lengths, bins='auto', edgecolor='black')\n",
    "plt.xlim([0,14000])\n",
    "# Add labels and title\n",
    "plt.xlabel('String Lengths in Character')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of String Lengths')\n",
    "print(doc_lengths.max())\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 31s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_token_lengths = np.array([len(tokenizer.tokenize(df.Content[i])) for i in df.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3088\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a histogram\n",
    "plt.hist(doc_token_lengths, bins='auto', edgecolor='black')\n",
    "plt.xlim([0,2000])\n",
    "# Add labels and title\n",
    "plt.xlabel('String Lengths in Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of String Lengths')\n",
    "print(doc_token_lengths.max())\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'too'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Too'.strip('').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "vocabs_lists = list(token_vocab_dict.keys())\n",
    "term_frequencies = {t:1 for t in vocabs_lists}\n",
    "temp_term_frequencies = {}\n",
    "\n",
    "char_set = set()\n",
    "for doc in train_df.Content.values:\n",
    "    tokens_list = tokenizer.tokenize(doc)\n",
    "    char_set.update(set(' '.join(tokens_list)))\n",
    "    char_set.update(set(doc))\n",
    "    new_tokens = {t.strip('').lower() for t in tokens_list}\n",
    "    for t in new_tokens:\n",
    "        if t not in temp_term_frequencies:\n",
    "            temp_term_frequencies[t] = 0\n",
    "        temp_term_frequencies[t] += 1\n",
    "        \n",
    "# stripped_key_pairs = {}\n",
    "# stripped_key_frequencies = {}\n",
    "# for k in vocabs_lists:\n",
    "#     stripped_k = k.strip('').lower()\n",
    "#     stripped_key_pairs[k] = stripped_k\n",
    "    # if stripped_k not in stripped_key_frequencies:\n",
    "    #     stripped_key_frequencies[stripped_k] = 0\n",
    "    # stripped_key_frequencies[stripped_k] += v\n",
    "    \n",
    "for k, v in term_frequencies.items():\n",
    "    stripped_token = k.strip('').lower()\n",
    "    term_frequencies[k] = temp_term_frequencies[stripped_token] if stripped_token in temp_term_frequencies else 1\n",
    "    # token_frequencies[k] = temp_token_frequencies[stripped_key_pairs[k]]\n",
    "    \n",
    "# del stripped_key_pairs\n",
    "# del stripped_key_frequencies\n",
    "del temp_term_frequencies\n",
    "    \n",
    "for doc in test_df.Content.values:\n",
    "    char_set.update(set(' '.join(tokenizer.tokenize(doc))))\n",
    "    char_set.update(set(doc))\n",
    "len(char_set)\n",
    "\n",
    "allowed_chars = string.ascii_letters + string.digits + string.punctuation + ' '\n",
    "all_chars = set(''.join(char_set).join(allowed_chars))\n",
    "vocab_dict = {c:i for i, c in enumerate(all_chars)}\n",
    "if '\\x01' not in vocab_dict:\n",
    "    vocab_dict['\\x01'] = len(vocab_dict)\n",
    "char_Set = set(vocab_dict.keys())\n",
    "num_embedding = len(vocab_dict)\n",
    "vocab_dict_rev = dict(zip(list(vocab_dict.values()), list(vocab_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def subsampling_equation_sigmoid(x: torch.Tensor):\n",
    "#     x = 0.9*F.sigmoid(0.06*(70-x))\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tensor(1), tensor(3), tensor(4), tensor(4), tensor(5)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(torch.tensor([1,4,3,5,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.00001\n",
    "total_token_count = np.array(list(term_frequencies.values())).sum()\n",
    "one_tensor = torch.tensor(1)\n",
    "def subsampling_equation_linear(x: torch.Tensor):\n",
    "    f_x = x/total_token_count\n",
    "    x = torch.min(one_tensor, torch.sqrt_(threshold/f_x))\n",
    "    return x\n",
    "\n",
    "def subsampling_equation_sigmoid(x: torch.Tensor):\n",
    "    f_x = x/total_token_count\n",
    "    x = 1-0.95*F.sigmoid(0.05*((f_x/threshold)-90))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0365597667011765e-05"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_token_count\n",
    "676/total_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsample: tensor([0.9262])\n"
     ]
    }
   ],
   "source": [
    "print(f'subsample: {subsampling_equation_sigmoid(torch.from_numpy(np.array([term_frequencies[\"were\"]])))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "676"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_frequencies[\"is\"]\n",
    "term_frequencies[\"king\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsample: tensor([0.9873])\n"
     ]
    }
   ],
   "source": [
    "print(f'subsample: {subsampling_equation_sigmoid(torch.from_numpy(np.array([term_frequencies[\"king\"]])))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterandTokenLevelCustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, num_classes, char_dict, token_dict, sentiment_dict, tokenizer, token_frequencies, sampling_equation, shuffle=True, batch_size=128) -> None:\n",
    "        super().__init__()\n",
    "        # y = y[indices].values\n",
    "        # y = torch.from_numpy(np.array([class_id[c] for c in y], dtype=np.longlong))\n",
    "        \n",
    "        # print(f'self.num_sections1: {len(y) // batch_size}')\n",
    "        # self.sampling_equation = sampling_equation\n",
    "        if len(y) % batch_size != 0:\n",
    "            self.shortage = ((len(y) // batch_size)+1)*batch_size - len(y)\n",
    "            empty_labels = [i%2 for i in range(self.shortage)]\n",
    "            empty_strings = [id_class[l] for l in empty_labels]\n",
    "            \n",
    "            # print(f'y1 - {y.shape}: {y}')\n",
    "            y = np.concatenate([y, empty_labels])\n",
    "            # print(f'y2 - {y.shape}: {y}')\n",
    "            # print(f'X1 - {X.shape}: {X}')\n",
    "            X = np.concatenate([X, empty_strings])\n",
    "        #     print(f'X2 - {X.shape}: {X}')\n",
    "        \n",
    "        # print(f'self.num_sections2: {len(y) // batch_size}')\n",
    "        \n",
    "        y = torch.from_numpy(y)\n",
    "        self.shuffle = shuffle\n",
    "        self.y = torch.nn.functional.one_hot(y, num_classes=num_classes).float()\n",
    "        self.X = X\n",
    "        self.char_dict = char_dict\n",
    "        self.char_Set = set(char_dict.keys())\n",
    "        self.vocab_size = len(self.char_dict)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_dict = token_dict\n",
    "        self.sentiment_dict = sentiment_dict\n",
    "        self.token_frequencies = token_frequencies\n",
    "        self.max_token_count = 0\n",
    "        \n",
    "        \n",
    "        self.all_data = []\n",
    "        self.token_lengths = []\n",
    "        self.token_embeddign_ids = []\n",
    "        \n",
    "        self.sum_a = 0\n",
    "        \n",
    "        for doc in tqdm(self.X):\n",
    "            g_data = self.content_to_graph(doc, sampling_equation)\n",
    "            self.all_data.append(g_data)\n",
    "        \n",
    "        \n",
    "        self.num_sections = len(y) // batch_size\n",
    "        self.x_lengths = np.array([self.all_data[i].character_length for i in range(len(self.all_data))])\n",
    "        self.x_len_args = np.argsort(self.x_lengths)[::-1]\n",
    "        \n",
    "        self.section_ranges = np.linspace(0, len(self.x_len_args), self.num_sections+1)\n",
    "        self.section_ranges = [(int(self.section_ranges[i-1]), int(self.section_ranges[i])) for i in range(1, len(self.section_ranges))]\n",
    "\n",
    "        self.position_j = 0\n",
    "        self.section_i = 0\n",
    "        self.epoch = 0\n",
    "        self.each_section_i = np.zeros((self.num_sections, ), dtype=int)\n",
    "        \n",
    "        self.sections, self.section_size = self.split_into_k_groups(self.x_len_args, self.x_lengths, self.num_sections)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        index = self.get_section_index()\n",
    "        return self.all_data[index], self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def get_section_index(self):\n",
    "        # if self.shuffle:\n",
    "            \n",
    "        #     t_range = self.section_ranges[self.section_i]\n",
    "        #     target_index = np.random.randint(t_range[0], t_range[1])\n",
    "        # else:\n",
    "        #     t_range = self.section_ranges[self.section_i]\n",
    "        #     target_index = t_range[0] + self.each_section_i[self.section_i]\n",
    "        #     self.each_section_i[self.section_i] = (self.each_section_i[self.section_i] + 1) % (t_range[1] - t_range[0])\n",
    "        # print()\n",
    "        # print(f'self.section_i: {self.section_i},   self.position_j: {self.position_j}')\n",
    "        target_index = self.sections[self.section_i, self.position_j]\n",
    "        \n",
    "        self.position_j = (self.position_j + 1) % self.section_size\n",
    "        if self.position_j == 0:\n",
    "            self.section_i = (self.section_i + 1) % self.num_sections\n",
    "            if self.shuffle and self.section_i == 0:\n",
    "                self.sections, self.section_size = self.split_into_k_groups(self.x_len_args, self.x_lengths, self.num_sections)\n",
    "                # random_positions = np.random.choice(np.arange(0, self.section_size), size=self.section_size, replace=False)\n",
    "        # return self.x_len_args[target_index]\n",
    "        return target_index\n",
    "\n",
    "    def reset_params(self):\n",
    "        self.section_i = 0\n",
    "        self.position_j = 0\n",
    "        self.each_section_i = np.zeros((self.num_sections, ), dtype=int)\n",
    "        \n",
    "    def split_into_k_groups(self, len_sorted_args, lengths:np.array, k):\n",
    "        if self.shuffle and self.epoch > 0:\n",
    "            randomize_sections = np.concatenate([np.random.choice(np.arange(r[0], r[1]), size=r[1]-r[0], replace=False) for r in self.section_ranges])\n",
    "            len_sorted_args = len_sorted_args[randomize_sections]\n",
    "        \n",
    "        nums = lengths[len_sorted_args]\n",
    "        groups_size = len(len_sorted_args) // k\n",
    "        \n",
    "        \n",
    "        groups = [[] for _ in range(k)]\n",
    "        group_sums = np.zeros(k, dtype=int)\n",
    "        group_sizes = np.zeros(k, dtype=int)\n",
    "        \n",
    "        # print(f'groups_size: {groups_size}')\n",
    "        # print(f'len(len_sorted_args): {len(len_sorted_args)}')\n",
    "        # print(f'k: {k}')\n",
    "        for i, num in enumerate(nums):\n",
    "            candidate_indices = np.where(group_sizes<groups_size)[0]\n",
    "            # print(f'candidate_indices: {candidate_indices}')\n",
    "            min_group_idx = candidate_indices[np.argmin(group_sums[candidate_indices])]\n",
    "            groups[min_group_idx].append(len_sorted_args[i])\n",
    "            group_sums[min_group_idx] += num\n",
    "            group_sizes[min_group_idx] += 1\n",
    "        self.epoch += 1\n",
    "        \n",
    "        groups = np.array(groups)\n",
    "        group_sums_argsort = np.argsort(group_sums)[::-1]\n",
    "        groups = groups[group_sums_argsort]\n",
    "        \n",
    "        # check_x = self.X[groups]\n",
    "        # check_x_lens = [np.sum(np.array([len(sx) for sx in rx])) for rx in check_x]\n",
    "        # print(f'check_x: {check_x}')\n",
    "        \n",
    "        \n",
    "        return np.array(groups), groups_size\n",
    "        \n",
    "    def content_to_graph(self, doc, sampling_equation):\n",
    "        # tokens = self.tokenizer(''.join(c for c in doc if c in self.char_Set))\n",
    "        # tokens = [t.text for t in tokens]\n",
    "        tokens = self.tokenizer(doc)\n",
    "        if len(tokens) == 0:\n",
    "            tokens = ['empty']\n",
    "                        \n",
    "        token_lengths = [len(t) for t in tokens]\n",
    "        tokens.append('\\x01')\n",
    "        \n",
    "        token_lengths.append(len(tokens[-1])-1)\n",
    "        token_lengths = torch.from_numpy(np.array(token_lengths, dtype=np.longlong))+1\n",
    "        token_embs = [self.token_dict[t] if t in self.token_dict else torch.zeros((64, ), dtype=torch.float32) for t in tokens]\n",
    "        token_sentiments = [self.sentiment_dict[t] if t in self.sentiment_dict else (0.0, 0.0) for t in tokens]\n",
    "        token_embs = torch.from_numpy(np.array(token_embs, dtype=np.float32))\n",
    "        token_sentiments = torch.from_numpy(np.array(token_sentiments, dtype=np.float32))\n",
    "        doc = ' '.join(tokens)\n",
    "        characters = torch.from_numpy(np.array([self.char_dict[t] for t in doc], dtype=np.longlong))\n",
    "        token_positions = torch.arange(len(token_lengths), dtype=torch.long)\n",
    "        token_indices = torch.repeat_interleave(token_positions, token_lengths)\n",
    "        token_subsampling_probabilities = sampling_equation(torch.from_numpy(np.array([self.token_frequencies[t] if t in self.token_frequencies else 1 for t in tokens])))\n",
    "        num_tokens = len(token_lengths)\n",
    "        if num_tokens > self.max_token_count:\n",
    "            self.max_token_count = num_tokens\n",
    "        g_data = Data(x=characters,\n",
    "                        token_positions=token_positions,\n",
    "                        character_length = len(characters),\n",
    "                        num_tokens = num_tokens,\n",
    "                        token_indices=token_indices,\n",
    "                        token_lengths=token_lengths,\n",
    "                        token_embeddings=token_embs,\n",
    "                        token_sentiments=token_sentiments,\n",
    "                        token_subsampling_probabilities=token_subsampling_probabilities)\n",
    "        return g_data\n",
    " \n",
    "    def caluculate_batch_token_positions(self, num_tokens, character_length, token_indices):\n",
    "        cumsum_vals = torch.cumsum(num_tokens, dim=0).roll(1)\n",
    "        cumsum_vals[0] = 0\n",
    "        additions = torch.repeat_interleave(cumsum_vals, character_length)\n",
    "        cumulative_token_indices = token_indices + additions\n",
    "        return cumulative_token_indices       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "class CharacterandTokenLevelDataLoader(DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size: int = 1,\n",
    "        shuffle: bool = False,\n",
    "        follow_batch: List[str] | None = None,\n",
    "        exclude_keys: List[str] | None = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(CharacterandTokenLevelDataLoader, self).__init__(\n",
    "            dataset, batch_size, shuffle, follow_batch, exclude_keys, **kwargs\n",
    "        )\n",
    "\n",
    "    def __iter__(self):\n",
    "        base_iterator = super(CharacterandTokenLevelDataLoader, self).__iter__()\n",
    "        for batch in base_iterator:\n",
    "            cumsum_vals = torch.cumsum(batch[0].num_tokens, dim=0).roll(1)\n",
    "            cumsum_vals[0] = 0\n",
    "            additions = torch.repeat_interleave(cumsum_vals, batch[0].character_length)\n",
    "            batch[0].cumulative_token_indices = batch[0].token_indices + additions\n",
    "            yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(df.Content.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3760, 1.2731, 3.5556, 8.2565, 3.0525, 4.3947, 8.9001, 0.7448, 9.5223,\n",
      "        6.1687, 9.8460, 1.3974, 8.8606, 4.4414, 6.8289, 1.4518, 0.5102, 0.1452,\n",
      "        9.4172, 3.9890, 1.9018, 4.3660, 0.6702, 0.1788, 9.1955, 2.1227, 9.5538,\n",
      "        9.0033, 5.3526, 9.5499, 2.9036, 7.0387, 6.2835, 2.4924, 9.0960, 3.7956,\n",
      "        5.2009, 8.1702, 5.5275, 7.7821, 7.1771, 5.1397, 2.7790, 3.0315, 2.3583,\n",
      "        3.9104, 2.5695, 4.6973, 4.8258, 7.7537, 6.9126, 2.6768, 6.3562, 2.2382,\n",
      "        5.5057, 5.4118, 4.6455, 4.7651, 5.3609, 6.1031, 4.4532, 2.6402, 3.9951,\n",
      "        2.2412])\n"
     ]
    }
   ],
   "source": [
    "print(token_vocab_dict['charm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6000, 0.7000], dtype=torch.float64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_vocab_dict['charm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 54/25088 [00:01<07:42, 54.14it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 25088/25088 [06:37<00:00, 63.11it/s]\n",
      "100%|| 25088/25088 [06:37<00:00, 63.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 50min 32s\n",
      "Wall time: 13min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_dataset = CharacterandTokenLevelCustomDataset(train_df.Content.values, train_df.Topic.values, len(class_id), vocab_dict, token_vocab_dict, polarity_vocab_dict, tokenizer.tokenize, token_frequencies=term_frequencies, sampling_equation=subsampling_equation_sigmoid, batch_size=batch_size)\n",
    "test_dataset = CharacterandTokenLevelCustomDataset(test_df.Content.values, test_df.Topic.values, len(class_id), vocab_dict, token_vocab_dict, polarity_vocab_dict, tokenizer.tokenize, token_frequencies=term_frequencies, sampling_equation=subsampling_equation_sigmoid, batch_size=batch_size)\n",
    "max_token_count = max(train_dataset.max_token_count, test_dataset.max_token_count)\n",
    "train_dataloader = CharacterandTokenLevelDataLoader(train_dataset, batch_size=batch_size, drop_last=False, shuffle=True)\n",
    "test_dataloader = CharacterandTokenLevelDataLoader(test_dataset, batch_size=batch_size, drop_last=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_lengths = np.array([train_dataset[i][0].num_tokens for i in range(len(train_dataset))])\n",
    "# test_lengths = np.array([test_dataset[i][0].num_tokens for i in range(len(test_dataset))])\n",
    "# print(np.max(train_lengths))\n",
    "# print(np.max(test_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[7584], token_positions=[1367], character_length=[1], num_tokens=[1], token_indices=[7584], token_lengths=[1367], token_embeddings=[1367, 64], token_sentiments=[1367, 2], token_subsampling_probabilities=[1367])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" The belief in '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([vocab_dict_rev[t.item()] for t in X[0].x[:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7500, 0.0994, 0.9890,  ..., 0.9796, 0.0977, 0.9896])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].token_subsampling_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Topic                                                      0\n",
       "Content    So I finally saw the film \"My Left Foot\" last ...\n",
       "Name: 15926, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[np.argwhere(['hardy' in c and 'obscure' in c for c in  df.Content.values]).squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 * * * Tip : Have It Read To You , Here s How * * * < br / > < br / > 1 ) Copy And Paste This To Notepad ( NOT WORD ) 2 ) Go To . START > ALL PROGRAMS > ACC ESS O RIES > ACC ESSA BIL TY > NAR RATOR < br / > < br / > having your testicles ironed . < br / > < br / > When Jonathan Ross started his career he was on a show call \" The Last Resort \" now a days he is the first resort to host anything and anything . TV Award Shows that half the time he is up for nominations in , Comic relief , chat shows , quiz shows , game shows , charity shows , Brighton . Just when you at you wits end and think you can find salvation in the wireless the lisp ing twang of good old J . R . Hits you like a freight train going none stop from Texas to downtown N . Y . That has lost a hour and is trying to make it up . < br / > < br / > About this show ( F NW JR ) . < br / > < br / > It s a normal chat show format with J . R . As host and a house band that concise st of four gay men ( ha ha ha , ow my aching sides . ) and season one had Andy Davis , but he left or was fired to give way to Ross ' s Ego . < br / > < br / > Ross will more less use his guests as props and you really don ' t hear them speak because of his \" It s my ball and I ' ll take it home \" attitude , you also see that the bigger the guest the more he is willing to lie and suck up to them , to get in with the big boys ( Like the weak kid at school who hangs round with the bully ) . < br / > < br / > However when a small reality T . V . Star comes on he ' ll happily humiliate them , asking personal questions about the past and telling them about their lack of talent to get the laughs . Sometimes he will under estimate the popularity of a guest , say something to belittle them and then when the audience act shocked , he will quickly turn and start making himself the fall guy , the best example of this was when \" Life On Mars \" star John Sim m came on and he said how does someone like you get work , your OK looking but not Hollywood good - looking ( Ba re in mind the Hugh Jackman and Halle Berry was in the green room , he was really only trying to suck up to them before they were even on the couch ) . When the audience acted shock Ross quickly said \" What , I ' m bit light headed from wearing that corset , I don ' t know what I ' m saying \" . If he don ' t have any low forms of TV life on he ' ll just dig at the four gay men on the piano with jokes more out of date than his fashion . < br / > < br / > It s very much a different story when a Hollywood A - lister or big TV star comes on the show in that he ' ll tell them stories to humor them . When some actor explains that he was in a support band then Jonathon Ross will say something like \" Wow , well he ever I go to see a band i was try to look interested for the support band , to make them feel as though they are wanted \" with an underline message being \" please like me , I was probably one of the people that cheered you when you was in your band \" . Top this off with an audience of Ross fans so hooked on every bad old joke and bull ing , it really makes for a poor show . < br / > < br / > Your better off watching US chat shows instead , they are more scripted but not anywhere near as hard to watch . \u0001\n",
      "0 Once again , I ' ve been duped by seemingly intelligent reviews making seemingly intelligent comments about an obviously crappy movie . I actually put my shoes on , got in my car , burned expensive gasoline and drove to the nearest rental place AFTER reading said reviews and paid the requisite 4 dollars and change to rent this thing . I ' m telling you , this one ' s not worth the minuscule kilo - calorie s spent on lifting one ' s index finger to switch channels on a TV remote . < br / > < br / > I even gave it a few more minutes after seeing all the tell - tale signs of a pedigree dog - pile . These presented as clinical symptoms of a director who is a . going senile or , b . is only marginally interested in the film he / she is obligated to create . I saw similar deterioration with John Carpenter ' s string of ridiculous caricature ' s over the past number of years . < br / > < br / > Here are a couple of scenes as incriminating evidence . The priest is having a disturbing dream . . . supposedly a harbinger of nastiness to come since he seems hell bent on opening the archaeological feature which houses the demon . The dream is a goofy collage of disjointed images right out of the Twilight Zone ' s stock footage . A ticking clock care ens through the dream scape ' s blackness implying , what ? , the unfathomable mystery of Time ? . . . . big deal ! A disembodied head , painted in demon features with convenience store quality Halloween make - up , flicker s back and forth in a convulsive frenzy . Every time I see this effect , a big fat rip - off from Jacob ' s Ladder , it pisses me off . This , in itself , almost instantly discredit s a film . < br / > < br / > The whole build - up of the archaeological dig itself is laughable . Everything is so obvious . . . so tired and over - wrought . . . the only possible response is boredom . At one point in the dig , the priest comments on finding the statues of Angels surrounding a sarcophagus . . . they ' re all pointing down toward the crypt with their weapons . He queries \" Look at these surrounding statues . . . . It ' s as if they are holding . . something . . down ! \" This is supposed to build tension . . . critical mass . . but it doesn ' t even come close ! How can there be suspense if you treat the audience like a bunch of morons having to EXPLAIN the suspense as you go along . The imagery is over - done in the first place but the added comments only add insult to injury in my opinion . Soon thereafter , the tomb is \" decorated \" with the remains of the soldiers placed there to guard the main atrium ( another shameless rip - off of The Keep , btw ) . Who , for crying out loud , did the make - up effects for this film ? ? ! The blood actually had that pinkish quality one might see in 70 ' s T roma ville flicks . At this point I became almost convinced that they simply forgot the make - up and had to go to Wal - Mart in the interest of time and money . < br / > < br / > DON ' T listen to glowing comments on this one ! I ' ll be keeping a suspicious eye on Schrader too . Looks like it might be time to hang up his gloves . Perhaps a close friend will offer a gentle admonition to quit while there ' s still dignity in memory of films gone by . \u0001\n",
      "0 Like many people in my general age range , I remember going to see this movie as a kid in ' 98 and coming out of the theatre practically in tears . It seemed , at the time , to be one of the most important , awe - inspiring cinematic experiences of our generation . At once riveting , action - packed , funny , heartbreaking , and truly inspirational , Arm eg ed de on really did have everything going on , right down to the catchy Aerosmith theme song and sappy tear - jerk er of an ending . < br / > < br / > Sweet Jeb us . What were we smoking ? I watched it for the first time in years last night on one of the movie channels , and . . . I cannot even describe it . This is , truly , one of the worst movies ever made . Where to even begin ? Leaving aside the plethora of LAUGH ABLE scientific errors ( ' personnel trackers ' on astronauts ? yeah , sure , thanks for that , Billy Bob ) , I ' d have to say the worst thing about it was the remarkable - dare I even say unmatched - way in which it combined crappy writing with crappy acting . There are too many examples of this to even begin listing here , but one in particular springs to mind - the scene where Bruce Willis is telling the Fed s exactly where to go to track down each of the oh - so - charm ingly - ro guish members of his oil drilling team ( ' check every bar in New Orleans ' , ' the craps tables in Vegas ' , ' the only black guy on a motorcycle in Sturgis ' . . . all to the tune of ' Come Together ' . . . it reminded me a bit of the \" NEWS TEAM ! AS SEM BLE ! \" scene from Anchor man , except serious ) . Ben Affleck proves , once again , that he is by far the most overpaid actor in Hollywood , having less depth , range , and overall talent than anyone else in the business . Not that Bruce Willis , Liv Tyler , OR ANYONE ELSE IN THIS GOD FOR SA KEN PIECE OF GAR B AGE was much better . < br / > < br / > ( I have to say , though , I got a kick out of seeing a pre - star dom Owen Wilson get killed off half - way through . . . is this the only movie where his character dies ? ) < br / > < br / > Peter Storm are is perfect as THE MOST STEREO TY P ICAL UN SHA VEN Russian COSMO N AUT YOU HAVE EVER SCENE . ( Then again , Peter Storm are does seem to have a talent for playing over - the - top Euro types . ) It really was quite amusing how , almost IMMEDIATELY after the Americans dock with the Russian Space Station ( which is actually called that in the movie ) , Ben Affleck succeeds in singlehandedly causing the whole joint to explode in spectacular Hollywood fashion . I also love the fact that , in the end , Paris is the only place on Earth to get destroyed , and that absolutely no one seems to care . And on top of all that , it at points literally turns into simultaneous ads for Lockheed Martin AND Kerr McGee . Oh how proud I am to be an American . < br / > < br / > There ' s plenty of other stuff to rant about , but I won ' t . . . suffice it to say that this is a really , really , REALLY terrible movie , that I feel ashamed to have ever genuinely liked . < br / > < br / > I give it two stars just for the mock ability factor . \u0001\n",
      "1 I ' m not sure why I picked for a borrow from mom for \" Nurse Betty \" . I think just because I had heard a little bit of this movie . But I ' m glad I did . \" Nurse Betty \" is an original and clever movie that has humor and a darker side . < br / > < br / > This was one of Renee ' s first big one ' s before hitting it major in Hollywood . I can see why , she is an incredible actress . The scene where she finally realizes what had happened and she ' s on the set of her favorite soap opera , you can see pain , confusion , fear , and embarrassment on her face . Just to let you in on the movie , she plays Betty . A shy and insecure woman who stands by her abusive husband , she ' s a waitress , and is in love with soap operas , especially one where a certain cute doctor , Dr . Dave Revell . When she happens to see her husband ' s murder accidentally in separate room , the murders she notices are two customers she just had , Morgan Freeman and Chris Rock . She then just loose s her mind and leaves town after talking to he police and says she needs to find her former fiance , Dr . Dave Revell . So , she travels along the country to California to find Dr . Revell , and wants a job as a nurse to work with Dave , she ' s seen the show so many times , somehow she ' s just awesome at being a nurse when she saves a woman ' s brother . Despite everyone telling her that she is delusional , she just looks at them like as if they were the crazy one ' s . When she meets the actor who plays Dave Revell , George ( his real name ) thinks she ' s just a crazy fan trying to get on the show . She just looks at him with confusion and believes that he and her belong together . < br / > < br / > Re nee was terrific , she was so believable on loosing her mind in the movie . She has come such a long way , and with er you want to admit it or not , she ' s adorable and a great actress . < br / > < br / > Morgan Freeman plays one of the assassin ' s , Charlie , who is the father of the two . He is so charmed and smitten ed by Betty and while chasing her around the country , he becomes almost just infatuated with Betty to the point where he almost falls in love with her . He and his son Wesley must find Betty when they find out she was there at the murder scene and could give away their identities . When Charlie sees Betty and catches her finally , she ' s scarred at first , but calms down and they know they have a real connection . It was a beautifully played scene , my opinion is that Morgan gave a stronger performance . He ' s just great . < br / > < br / > A surprisingly decent performance by Chris Rock , the son , Wesley . He is so \" gun \" - ho about just getting the job done in a rush and taking care of business . I loved his comedic performance at the end where he and the gang he ' s holding hostage by gun point are just watching the soap opera ' s together . Classic . \" Nurse Betty \" is a great movie that I ' d recommend for a good laugh and just in all a nice honest little movie I think anyone could enjoy . < br / > < br / > 9 / 10 \u0001\n",
      "0 There ' s something intriguing about disaster movies . The simple , primal premise can lead to several great stories . Granted , most disaster movies tend to explore familiar territory instead but I can usually live with that . < br / > < br / > Unfortunately , Flood probably marks the low point in the history of this sub - genre . Robert Carlyle is undoubtedly the star of the movie , even though screen time is split between different locations and characters . He gives a barely decent performance . As well , Joanne Whalley is very uneven . Veteran actor Tom Courtenay ( he played in Doctor Zhi va go for heaven ' s sake ) is particularly bad . I mean , his timing is completely off most of the time and his characterization is extremely poor . What an embarrassing performance for that man . The rest of the cast ranges from decent to really bad with one exception : Jessa lyn Gil sig , whom I thought might be there as a plot device / eye candy gives by far the most convincing performance . Doesn ' t mean much considering how bad everybody else is but still nice to see that she cared . < br / > < br / > The script is really bad , confusing and clich . Some of the worse lines I have heard in quite some time are delivered by the actors one after the other . You ' ve seen this story a thousand times . It employs every dramatic hook and tear - jerk ers you ' ve seen in \" Out break \" , \" Ar mageddon \" , the Poseidon movies ( original and remake ) and many others . < br / > < br / > The direction is awful . No sense of timing , nothing inspired . The shots are bland , dialog and action both fail to flow . Editing is bad but how do you edit such a mess ? Without a doubt , this movie tried to rely way too much on ( rather poor ) CGI . The human factor , the drama and struggles of the characters are glossed over . Scenes where the characters must actually face the flood are rare and poorly done . The made - for - TV feel gives nausea . Some guy is supposed to go down a rope from an helicopter ? No problem , let ' s show him inside a helicopter and make a really poor cut / editing job and have the next frame with him safely on the ground , in the most obvious way possible . < br / > < br / > The movie score is rather poor . All over the place , no timing . < br / > < br / > The ending is probably the worse I have seen in quite some time . Very much like they ran out of ideas . Scrap that , you can ' t run out of something if you never had it in the first place . Must have ran out of budget . < br / > < br / > This is a really amateur job . I give it a 2 for using London as a location , which is a nice change , for Gil sig being actually decent in a key support role and for the few CGI shots that were decent ( those of the water closing in on London and the gates ) . < br / > < br / > Do yourself a favor and check out Day After Tomorrow or just about any disaster movie before this one . This includes older classics like The Tower ing Inferno . \u0001\n"
     ]
    }
   ],
   "source": [
    "for i in range(15,20):\n",
    "    char_ids = X[i].x\n",
    "    text_for_ids = ''.join([vocab_dict_rev[ci.item()] for ci in char_ids])\n",
    "    print(torch.argmax(y[i]).item(), text_for_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rand = torch.randn((64, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9990)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rand.var(unbiased=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[406094], token_positions=[75621], character_length=[256], num_tokens=[256], token_indices=[406094], token_lengths=[75621], token_embeddings=[75621, 64], token_sentiments=[75621, 2], token_subsampling_probabilities=[75621], batch=[406094], ptr=[257], cumulative_token_indices=[406094])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = torch.randn((len(X.token_positions), 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([75621])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.repeat_interleave(torch.arange(len(X.num_tokens)), X.num_tokens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  ..., 20, 21, 22])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.token_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7994, -0.5107,  0.3645,  ...,  0.6588,  1.5570, -0.5494],\n",
       "        [-0.5500,  1.2228, -0.3744,  ...,  0.4838, -1.4837,  0.7082],\n",
       "        [ 0.0379,  0.0511,  0.0942,  ..., -0.2066,  0.7047, -0.4536],\n",
       "        ...,\n",
       "        [-0.4543, -0.9508,  1.2477,  ..., -1.0361,  0.8070,  1.4626],\n",
       "        [-0.0239,  1.1046, -0.7905,  ...,  1.3271, -0.2336, -0.4396],\n",
       "        [ 0.1513, -1.0826, -0.6217,  ..., -1.3772,  0.4925,  0.8410]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATv2Conv, SimpleConv\n",
    "\n",
    "# Normalization on each feature of all tokens, for this we used batch norm class but with tokens at batch dimention\n",
    "class GCNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, *args, **kwargs):\n",
    "        super(GCNN, self).__init__(*args, **kwargs)\n",
    "        self.gnn = GATv2Conv(hidden_dim, hidden_dim//8, heads=4, add_self_loops=False)\n",
    "        self.conv = nn.Conv1d(hidden_dim, hidden_dim//2, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(hidden_dim//2, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim//2)\n",
    "        \n",
    "    def forward(self, x, edge_data, return_attention_weights = False):\n",
    "        x1, edge_weights = self.gnn(x, edge_data, return_attention_weights=return_attention_weights) \n",
    "        x2 = F.relu(self.conv(x.T).T)\n",
    "        x1 = F.leaky_relu_(self.bn1(x1))\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        return x, edge_weights, edge_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx, to_undirected\n",
    "\n",
    "class GenGraph(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, virtual_nodes, lattice_step, lattice_pattern=None, *args, **kwargs):\n",
    "        super(GenGraph, self).__init__(*args, **kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.virtual_nodes = virtual_nodes\n",
    "        self.lattice_step = lattice_step\n",
    "        self.lp = lattice_pattern if lattice_pattern is None else torch.tensor(lattice_pattern)\n",
    "        self.virtual_node_embeddings = nn.Embedding(self.virtual_nodes, hidden_dim)\n",
    "        \n",
    "    def gen_graph(self, x, token_subsampling_probabilities, total_token_counts, token_counts, random_edges, lattice_edges, lattice_start_distance=2):\n",
    "        random_links, lattice_links, tc_range = self.calculate_graph(x, total_token_counts, token_counts, random_edges, lattice_edges, lattice_start_distance)\n",
    "        v_n_e_counts = total_token_counts*self.virtual_nodes\n",
    "        base_numel = random_links.numel() + lattice_links.numel()*2\n",
    "        edge_indices = torch.empty((2, base_numel + v_n_e_counts*2), dtype=torch.int64, device=x.device)\n",
    "        self.fill_lattice_and_random_edges(edge_indices, random_links, lattice_links, tc_range)\n",
    "            \n",
    "        if self.virtual_nodes > 0:\n",
    "            virtual_nodes_range = torch.arange(self.virtual_nodes, device=x.device).view(1, -1)\n",
    "            virtual_nodes_ids = torch.repeat_interleave(virtual_nodes_range, len(token_counts), dim=0)\n",
    "            v_n_idx = (virtual_nodes_ids + torch.arange(0, len(token_counts)*self.virtual_nodes, self.virtual_nodes, device=x.device).view(-1, 1) + total_token_counts )\n",
    "            virtual_edge_ids = torch.repeat_interleave(v_n_idx.view(-1), token_counts.view(-1, 1).expand(len(token_counts), self.virtual_nodes).reshape(-1), dim=0).view(1, -1)\n",
    "            \n",
    "            embs = self.virtual_node_embeddings(virtual_nodes_ids.T).view(-1, self.hidden_dim)\n",
    "            x_extended = torch.cat([x, embs], dim=0)\n",
    "            x_index = torch.arange(total_token_counts, device=x.device).repeat(self.virtual_nodes).view(1, -1)\n",
    "            edge_indices[:, base_numel:base_numel+v_n_e_counts] = torch.cat([x_index, virtual_edge_ids], dim=0)\n",
    "            edge_indices[:, base_numel+v_n_e_counts:] = torch.cat([virtual_edge_ids, x_index], dim=0)\n",
    "            x = x_extended\n",
    "        \n",
    "        edge_indices = self.subsample_edges(edge_indices, token_subsampling_probabilities)\n",
    "        return Batch.from_data_list([Data(x=x, edge_index=edge_indices)])\n",
    "        \n",
    "    def re_gen_graph(self, x, edge_indices, token_subsampling_probabilities, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance=2):\n",
    "        random_links, lattice_links, tc_range = self.calculate_graph(x, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance)\n",
    "        base_numel = random_links.numel() + lattice_links.numel()*2\n",
    "        \n",
    "        self.fill_lattice_and_random_edges(edge_indices, random_links, lattice_links, tc_range)\n",
    "        # for i in range(base.shape[1]):\n",
    "        #     edge_indices[:, i*base.shape[0]:(i+1)*base.shape[0]] = torch.cat([tc_range, base[:,i].view(1,-1)], dim=0)\n",
    "        edge_indices = self.subsample_edges(edge_indices, token_subsampling_probabilities)\n",
    "        return Batch.from_data_list([Data(x=x, edge_index=edge_indices)])\n",
    "    \n",
    "    def replace_unimportant_edges(self, edge_weights, x, edge_indices, token_subsampling_probabilities, total_token_counts, token_counts, random_edges, lattice_edges, p_keep=1, lattice_start_distance=2):\n",
    "        v_n_e_counts = total_token_counts*self.virtual_nodes\n",
    "        # if v_n_e_counts>0:\n",
    "        #     important_indices = torch.topk(edge_weights[:-2*v_n_e_counts].view(-1, total_token_coutns), p_keep, dim=0).indices\n",
    "        # else:\n",
    "        #     print(f'edge_weights.shape: {edge_weights.shape}')\n",
    "        #     print(f'total_token_coutns: {total_token_coutns}')\n",
    "        #     print(f'p_keep: {p_keep}')\n",
    "        #     important_indices = torch.topk(edge_weights.view(-1, total_token_coutns), p_keep, dim=0).indices\n",
    "        # important_indices = torch.topk(edge_weights[:-1*total_token_coutns].view(-1, total_token_coutns), 1, dim=0).indices.squeeze()\n",
    "        # print(f'edge_weights.shape: {edge_weights.shape}')\n",
    "        # print(f'edge_indices.shape: {edge_indices.shape}')\n",
    "        # print(f'1: edge_weights: {edge_weights.shape}')\n",
    "        important_indices = torch.topk(edge_weights.squeeze(), p_keep*total_token_counts, dim=0).indices\n",
    "        # print(f'2: important_indices: {important_indices.shape}')\n",
    "        # print(f'2.5: \\n {edge_weights} \\n\\n {important_indices}')\n",
    "\n",
    "        # important_indices = torch.arange(total_token_counts, dtype=torch.int64, device=x.device)\n",
    "        # important_indices = important_indices.view(-1)\n",
    "        random_links, lattice_links, tc_range = self.calculate_graph(x, total_token_counts, token_counts, random_edges, lattice_edges, lattice_start_distance)\n",
    "        # print(f'3: random_links: {random_links.shape}, lattice_links: {lattice_links.shape}, tc_range: {tc_range.shape},')\n",
    "        base_numel = random_links.numel() + lattice_links.numel()*2\n",
    "        # print(f'4: base_numel: {base_numel}')\n",
    "        \n",
    "        new_edge_index = torch.empty((2, base_numel + important_indices.shape[0] + 2*v_n_e_counts), dtype=torch.int64, device=x.device)\n",
    "        # print(f'5: new_edge_index: {new_edge_index.shape}')\n",
    "        # print(f'new_edge_index.shape 1: {new_edge_index.shape}, base_numel + important_indices.shape[0] + 2*v_n_e_counts: {base_numel + important_indices.shape[0] + 2*v_n_e_counts}')\n",
    "        self.fill_lattice_and_random_edges(new_edge_index, random_links, lattice_links, tc_range)\n",
    "        # print(f'6: new_edge_index: {new_edge_index.shape}, random_links: {random_links.shape}, lattice_links: {lattice_links.shape}, tc_range: {tc_range.shape}')\n",
    "        # print(f'new_edge_index.shape 2: {new_edge_index.shape}, edge_indices: {edge_indices.shape}, important_indices shape: {important_indices.shape}, important_indices max: {important_indices.max()}')\n",
    "        new_edge_index[:, base_numel:base_numel+important_indices.shape[0]] = edge_indices[:, important_indices]\n",
    "        # print(f'7: new_edge_index: {new_edge_index.shape}')\n",
    "\n",
    "        if(self.virtual_nodes>0):\n",
    "            new_edge_index[:, -2*v_n_e_counts:] = edge_indices[:, -2*v_n_e_counts:]\n",
    "            \n",
    "        # for i in range(base.shape[1]):\n",
    "        #     new_edge_index[:, i*base.shape[0]:(i+1)*base.shape[0]] = torch.cat([tc_range, base[:,i].view(1,-1)], dim=0)\n",
    "        # print(f'7.5: \\n {new_edge_index} \\n\\n {token_subsampling_probabilities}')\n",
    "        new_edge_index = self.subsample_edges(new_edge_index, token_subsampling_probabilities)\n",
    "        return Batch.from_data_list([Data(x=x, edge_index=new_edge_index)])\n",
    "    \n",
    "    # def replace_unimportant_edges(self, edge_weights, x, edge_index, token_subsampling_probabilities, total_token_coutns, token_counts, random_edges, lattice_edges, p_keep=1, lattice_start_distance=2, seed=-1):\n",
    "    #     v_n_e_counts = total_token_coutns*self.virtual_nodes\n",
    "    #     if v_n_e_counts>0:\n",
    "    #         important_indices = torch.topk(edge_weights[:-2*v_n_e_counts].view(-1, total_token_coutns), p_keep, dim=0).indices\n",
    "    #     else:\n",
    "    #         print(f'edge_weights.shape: {edge_weights.shape}')\n",
    "    #         print(f'total_token_coutns: {total_token_coutns}')\n",
    "    #         print(f'p_keep: {p_keep}')\n",
    "    #         important_indices = torch.topk(edge_weights.view(-1, total_token_coutns), p_keep, dim=0).indices\n",
    "    #     # important_indices = torch.topk(edge_weights[:-1*total_token_coutns].view(-1, total_token_coutns), 1, dim=0).indices.squeeze()\n",
    "\n",
    "    #     important_indices = torch.arange(total_token_coutns, dtype=torch.int64, device=x.device) + important_indices*total_token_coutns\n",
    "    #     important_indices = important_indices.view(-1)\n",
    "    #     random_links, lattice_links, tc_range = self.calculate_graph(x, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance, seed)\n",
    "    #     base_numel = random_links.numel() + lattice_links.numel()*2\n",
    "        \n",
    "    #     new_edge_index = torch.empty((2, base_numel + important_indices.shape[0] + 2*v_n_e_counts), dtype=torch.int64, device=x.device)\n",
    "    #     self.fill_lattice_and_random_edges(new_edge_index, random_links, lattice_links, tc_range)\n",
    "    #     new_edge_index[:, base_numel:base_numel+important_indices.shape[0]] = edge_index[:, important_indices]\n",
    "    #     if(self.virtual_nodes>0):\n",
    "    #         new_edge_index[:, -2*v_n_e_counts:] = edge_index[:, -2*v_n_e_counts:]\n",
    "            \n",
    "    #     # for i in range(base.shape[1]):\n",
    "    #     #     new_edge_index[:, i*base.shape[0]:(i+1)*base.shape[0]] = torch.cat([tc_range, base[:,i].view(1,-1)], dim=0)\n",
    "    #     new_edge_index = self.subsample_edges(new_edge_index, token_subsampling_probabilities)\n",
    "    #     return Batch.from_data_list([Data(x=x, edge_index=new_edge_index)])\n",
    "    \n",
    "    \n",
    "         \n",
    "    def calculate_graph(self, x, total_token_counts, token_counts, random_edges, lattice_edges, lattice_start_distance):\n",
    "\n",
    "        tc_extended = torch.repeat_interleave(token_counts, token_counts, dim=0).view(-1,1)\n",
    "        tc_lower_bound = torch.empty((len(token_counts)+1), dtype=torch.long, device=x.device) #torch.cuda.IntTensor(len(token_counts)+1) #\n",
    "        tc_lower_bound[0] = 0\n",
    "        tc_lower_bound[1:] = torch.cumsum(token_counts, dim=0)\n",
    "        tc_lower_bound_extended = torch.repeat_interleave(tc_lower_bound[:-1], token_counts, dim=0).view(-1,1)\n",
    "        tc_range = torch.arange(tc_lower_bound[-1], device=x.device).view(-1,1)\n",
    "        # torch.arange(tc_lower_bound[-1], dtype=torch.int32, device=x.device).view(-1,1)\n",
    "        \n",
    "        random_ints = torch.randint(0, 2*total_token_counts, (total_token_counts, random_edges), device=x.device) # torch.cuda.IntTensor(len(token_lengths), random_edges).random_()\n",
    "        lattice = self.lp.to(x.device) if self.lp is not None else torch.arange(lattice_start_distance, max(lattice_start_distance, self.lattice_step*lattice_edges+1), self.lattice_step, device=x.device).view(1, -1)\n",
    "        \n",
    "\n",
    "        # exponentials = torch.pow(2, torch.arange(1, self.exp_edges+1, device=x.device)).view(1, -1)\n",
    "        tc_local_range = tc_range - tc_lower_bound_extended\n",
    "        random_links = (((random_ints % (tc_extended - 1))+1 + tc_local_range) % tc_extended)+tc_lower_bound_extended\n",
    "        lattice_links = ((lattice + tc_local_range) % tc_extended)+tc_lower_bound_extended\n",
    "        # base = torch.cat([base1, base2], dim=1)\n",
    "        tc_range = tc_range.view(1,-1)\n",
    "        return random_links, lattice_links, tc_range\n",
    "    \n",
    "    def fill_lattice_and_random_edges(self, edge_indices, random_links, lattice_links, tc_range):\n",
    "        for i in range(0, lattice_links.shape[1]*2, 2):\n",
    "            edge_indices[:, i*lattice_links.shape[0]:(i+1)*lattice_links.shape[0]] = torch.cat([lattice_links[:,i//2].view(1,-1), tc_range], dim=0)\n",
    "            edge_indices[:, (i+1)*lattice_links.shape[0]:(i+2)*lattice_links.shape[0]] = edge_indices[:, i*lattice_links.shape[0]:(i+1)*lattice_links.shape[0]][[1, 0]]\n",
    "            \n",
    "        for i in range(random_links.shape[1]):\n",
    "            j = i + lattice_links.shape[1]*2\n",
    "            edge_indices[:, j*random_links.shape[0]:(j+1)*random_links.shape[0]] = torch.cat([random_links[:,i].view(1,-1), tc_range], dim=0)\n",
    "            \n",
    "    def subsample_edges(self, edge_indices, token_subsampling_probabilities, keep_ratio=0.65):\n",
    "        \n",
    "        p = torch.rand(edge_indices.shape, dtype=torch.float, device=edge_indices.device)\n",
    "        to_keep = (p<token_subsampling_probabilities[edge_indices]).float()\n",
    "        to_keep = torch.topk(to_keep[0] + to_keep[1], (int)(edge_indices.shape[1]*keep_ratio), dim=0).indices\n",
    "        edge_indices = edge_indices[:, to_keep]\n",
    "        return edge_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6016, 0.7661, 0.2928, 0.7894, 0.5939],\n",
      "        [0.6280, 0.3027, 0.0625, 0.2144, 0.7388]])\n",
      "tensor([[0.3811, 0.9040, 0.6691, 0.8094, 0.1984],\n",
      "        [0.7413, 0.8390, 0.3304, 0.6647, 0.7376]])\n"
     ]
    }
   ],
   "source": [
    "base = torch.rand((2, 5), dtype=torch.float)\n",
    "p = torch.rand((2, 5), dtype=torch.float)\n",
    "print(base)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 1.],\n",
       "        [1., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_keep = (p<torch.tensor([[1, 0.5, 0.5, 0.0, 0.7], [1.0, 0.5, 1.0, 0.5, 0.0]])).float()\n",
    "to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 4])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_keep = torch.topk(to_keep[0] + to_keep[1], 3, dim=0).indices\n",
    "to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# to_keep = torch.topk(to_keep[0] + to_keep[1], 3, dim=0).indices\n",
    "# edge_indices = edge_indices[:, to_keep]\n",
    "# torch.topk(to_keep[0] + to_keep[1], (int)(edge_indices.shape[1]*keep_ratio), dim=0).indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment_Injection(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)        \n",
    "        self.conv1 = nn.Conv1d(2, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim*2, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "    def forward(self, x, token_sentiments):\n",
    "        x1 = F.relu_(self.bn1(self.conv1(token_sentiments.T).T))\n",
    "        x = F.relu_(self.conv2(torch.cat([x, x1], dim=1).T))\n",
    "        return x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATv2Conv, SimpleConv, summary\n",
    "\n",
    "class CNN_for_Text_No_Positional_Encoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embedding, pos_emb_size=8192, embedding_dim=64, hidden_dim=64, dropout=0.3, num_out_features=4, seed=-1, random_edges=4, lattice_edges=10, virtual_nodes=1, lattice_step=2, lattice_start_distance=2, inject_embedding_dim=64, isXaiTests=False, step_of_test = 0, num_tests=50, *args, **kwargs) -> None:\n",
    "        super(CNN_for_Text_No_Positional_Encoding, self).__init__(*args, **kwargs)\n",
    "        self.pos_emb_size = pos_emb_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.virtual_nodes = virtual_nodes\n",
    "        self.base_random_edges = random_edges\n",
    "        self.base_lattice_edges = lattice_edges\n",
    "        self.lattice_start_distance = lattice_start_distance\n",
    "        self.num_out_features = num_out_features\n",
    "        self.isXaiTests = int(isXaiTests)\n",
    "        self.num_tests = num_tests\n",
    "        self.step_of_test = step_of_test\n",
    "        # self.use_token_polarity = use_token_polarity\n",
    "        if seed>-1:\n",
    "            torch.manual_seed(seed)\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embedding, embedding_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(embedding_dim, hidden_dim, kernel_size=5, padding=2)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, padding=2)\n",
    "        \n",
    "        # if self.use_token_polarity[0]:\n",
    "        #     self.conv3 = nn.Conv1d(2*hidden_dim + 2, hidden_dim, kernel_size=3, padding=1)\n",
    "        # else:\n",
    "        self.conv3 = nn.Conv1d(2*hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv4 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        \n",
    "        # if self.use_token_polarity[1]:\n",
    "        self.sentiment1  = Sentiment_Injection(hidden_dim)\n",
    "        # if self.use_token_polarity[2]:\n",
    "        self.sentiment2  = Sentiment_Injection(hidden_dim)\n",
    "            \n",
    "        self.gcnn1 = GCNN(hidden_dim)\n",
    "        self.gcnn2 = GCNN(hidden_dim+inject_embedding_dim)\n",
    "        self.graph_generator = GenGraph(hidden_dim, virtual_nodes, lattice_step)\n",
    "        \n",
    "        k = 4\n",
    "        self.fc0 = nn.Linear(hidden_dim , hidden_dim+inject_embedding_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim+inject_embedding_dim , hidden_dim * k)\n",
    "        self.fc2 = nn.Linear(hidden_dim * (2+virtual_nodes) * k , 32)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(32, self.num_out_features)\n",
    "        self.max_length = 0\n",
    "    \n",
    "    def forward(self, x, edge_index, token_subsampling_probabilities, token_indices, token_sentiments, token_lengths, num_tokens, character_length, token_embeddings):\n",
    "        # cumulative_token_indices = token_indices if not self.isXaiTests else self.caluculate_batch_token_positions(num_tokens, character_length, token_indices)\n",
    "        cumulative_token_indices = self.caluculate_batch_token_positions(num_tokens, character_length, token_indices)\n",
    "        \n",
    "        # print(f'2: {x.shape}')\n",
    "        x = self.embedding(x)\n",
    "        # print(f'2.5: {x.shape}')\n",
    "        x = self.dropout(x)\n",
    "        # print(f'2.6: {x.shape}')\n",
    "        x = x.T\n",
    "        # print(f'2.7: {x.shape}')\n",
    "        # x = self.refine_shape(1, x, 0)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # print(f'2.8: {x.shape}')\n",
    "        x = self.refine_shape(1, x, self.hidden_dim, 0)\n",
    "        # print(f'2.8 refined: {x.shape}')\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.refine_shape(2, x, self.hidden_dim, 0)\n",
    "        # print(f'2.9: {x.shape}')\n",
    "        x = self.dropout(x)\n",
    "        # x = self.refine_shape(4, x, 0)\n",
    "        # print(f'3: {x.shape}')\n",
    "        x1 = scatter_max(x, cumulative_token_indices, dim=1)[0]\n",
    "        x2 = scatter_mean(x, cumulative_token_indices, dim=1)\n",
    "\n",
    "        # if self.use_token_polarity[0]:\n",
    "        #     x = torch.cat([x1, x2, g_data.token_sentiments.T], dim=0)\n",
    "        # else:\n",
    "        x = torch.cat([x1, x2], dim=0)\n",
    "            \n",
    "        # print(f'4: {x.shape}')\n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        # print(f'4.5: {x.shape}, self.hidden_dim: {self.hidden_dim}, self.is_tests_token_level: {self.step_of_test}')\n",
    "        # if self.isXaiTests and x.shape[0] != self.hidden_dim:\n",
    "        # x = torch.chunk(x, self.num_tests ** (1 - self.isXaiTests), dim=0)\n",
    "        x = self.refine_shape(3, x, self.hidden_dim, 0)\n",
    "        \n",
    "        # x = torch.chunk(x, (x.shape[0] // self.hidden_dim)**self.is_tests_token_level, dim=0)\n",
    "        # x = torch.cat(x, dim=1)\n",
    "        \n",
    "        # if self.isXaiTests and x.shape[0] != self.hidden_dim:\n",
    "        #     x = torch.chunk(x, self.num_tests, dim=0)\n",
    "        #     x = torch.cat(x, dim=1)\n",
    "        # x = x.reshape(self.hidden_dim, -1)\n",
    "        # print(\"abababdadasd\")\n",
    "        # print(f'5: {x.shape}, {edge_index.shape}, {cumulative_token_indices.shape}, {token_sentiments.shape}, {token_lengths.shape}, {num_tokens.shape}, {character_length.shape}, {token_embeddings.shape}')\n",
    "        # if self.use_token_polarity[1]:\n",
    "        x = self.sentiment1(x.T, token_sentiments)\n",
    "\n",
    "        # print(f'6: {x.shape}')\n",
    "        x = self.refine_shape(4, x, self.hidden_dim, 1)\n",
    "        # print(f'6 refined: {x.shape}')\n",
    "        rand_edges, lattice_edges = self.base_random_edges, self.base_lattice_edges\n",
    "            \n",
    "        graph = self.graph_generator.gen_graph(x, token_subsampling_probabilities, len(token_lengths), num_tokens, rand_edges, lattice_edges, lattice_start_distance=self.lattice_start_distance)\n",
    "        rand_edges = rand_edges-1\n",
    "        lattice_edges = lattice_edges-1\n",
    "        \n",
    "        # print(f'7: {graph.x.shape}')\n",
    "        \n",
    "        doc_token_index = torch.repeat_interleave(torch.arange(len(num_tokens), device=x.device), num_tokens)\n",
    "        x, edge_weights, edge_index = self.gcnn1(graph.x, graph.edge_index, return_attention_weights = True)\n",
    "        # print(f'7.1: {x.shape}')\n",
    "        x = self.refine_shape(5, x, self.hidden_dim, 1)\n",
    "        # print(f'7.1 2 : {len(torch.cat(edge_weights[1::2], dim=0))}')\n",
    "        edge_weights = self.refine_edge_weights(edge_weights)\n",
    "        edge_index = self.refine_edge_index(edge_index)\n",
    "        \n",
    "        # edge_weights = edge_weights[1].unsqueeze(-1)\n",
    "        # print(f'7.5 edge_weights: {edge_weights.shape}, - {graph.edge_index.shape[1]}, {edge_weights.shape[0]}')\n",
    "        # edge_weights = edge_weights[:min(graph.edge_index.shape[1], edge_weights.shape[0]), 0]\n",
    "        # edge_weights = edge_weights.squeeze()\n",
    "        \n",
    "        edge_weights = edge_weights.unsqueeze(-1)\n",
    "        # print(f'7.5 edge_weights: {edge_weights.shape}, - {edge_weights.shape[0]}')\n",
    "        edge_weights = edge_weights[:edge_weights.shape[0], 0]\n",
    "        \n",
    "        # print(f'7.6 edge_weights: {edge_weights.shape}')\n",
    "        # print(f'7.7: {x.shape}')\n",
    "        x = self.refine_shape(5, x, self.hidden_dim, 1)\n",
    "        # print(f'7.8 refined: {x.shape}')\n",
    "        \n",
    "        graph = self.graph_generator.replace_unimportant_edges(edge_weights, x, edge_index, token_subsampling_probabilities, len(token_lengths), num_tokens, rand_edges, lattice_edges, p_keep=2, lattice_start_distance=self.lattice_start_distance+1)\n",
    "        \n",
    "        # print(f'8: {graph.x.shape}')\n",
    "        \n",
    "        # if self.use_token_polarity[2]:\n",
    "        x = self.sentiment2(x, token_sentiments)\n",
    "          \n",
    "        # print(f'8.1: {x.shape}')\n",
    "        x = self.refine_shape(6, x, self.hidden_dim, 1)\n",
    "        # print(f'8.2 refined: {x.shape}')\n",
    "        \n",
    "        # print(f'9: {x.shape}')  \n",
    "        xa = graph.x[:token_embeddings.shape[0]]\n",
    "        xb = token_embeddings\n",
    "        x = torch.cat([xa, xb], dim=1)\n",
    "        # x = torch.cat([graph.x[:g_data.token_embeddings.shape[0]], g_data.token_embeddings], dim=1)\n",
    "        \n",
    "        # print(f'10: {x.shape}')  \n",
    "        x1 = F.relu(self.fc0(graph.x[token_embeddings.shape[0]:]))\n",
    "        x = torch.cat([x, x1], dim=0)\n",
    "        \n",
    "        # print(f'11: {x.shape}')  \n",
    "        sum1 = torch.sum(edge_weights) + torch.sum(edge_index)\n",
    "        x, edge_weights, edge_index = self.gcnn2(x, graph.edge_index)\n",
    "        edge_weights = self.refine_edge_weights(edge_weights)\n",
    "        sum1 = sum1 + torch.sum(edge_weights) + torch.sum(edge_index) \n",
    "        \n",
    "        # print(f'12: {x.shape}')  \n",
    "        x = F.elu_(self.fc1(x))\n",
    "        x1 = scatter_max(x[:len(token_lengths)], doc_token_index, dim=0)[0]\n",
    "        x2 = scatter_mean(x[:len(token_lengths)], doc_token_index, dim=0)\n",
    "        vn_embs = x[len(token_lengths):]\n",
    "        x_for_cat = [x1, x2]\n",
    "        x_for_cat.extend([vn_embs[i*x1.shape[0]:(i+1)*x1.shape[0]] for i in range(self.virtual_nodes)])\n",
    "        x = torch.cat(x_for_cat, dim=1)\n",
    "        \n",
    "        # print(f'13: {x.shape}')  \n",
    "        x = F.elu_(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc_out(x)\n",
    "        # print(f'14: {x.shape}')  \n",
    "        return x + sum1 * 0.0\n",
    "    \n",
    "    def caluculate_batch_token_positions(self, num_tokens, character_length, token_indices):\n",
    "        cumsum_vals = torch.cumsum(num_tokens, dim=0).roll(1)\n",
    "        cumsum_vals[0] = 0\n",
    "        additions = torch.repeat_interleave(cumsum_vals, character_length)\n",
    "        cumulative_token_indices = token_indices + additions\n",
    "        return cumulative_token_indices\n",
    "    \n",
    "    def refine_shape(self, test_step, x, num_chunks, section=0):\n",
    "        x = torch.chunk(x, (x.shape[section] // num_chunks)**(self.step_of_test==test_step), dim=0)\n",
    "        x = torch.cat(x, dim=1-section)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def refine_edge_weights(self, edge_weights):\n",
    "        edge_weights = edge_weights[1::2] + edge_weights[0::2] * 0\n",
    "        edge_weights = [edge_weights[i] for i in range(len(edge_weights))]\n",
    "        edge_weights = torch.cat(edge_weights, dim=0)\n",
    "        return edge_weights\n",
    "    \n",
    "    \n",
    "    def refine_edge_index(self, edge_index):\n",
    "        edge_index = torch.cat([edge_index[::2].reshape(1, -1), edge_index[1::2].reshape(1, -1)], dim=0)\n",
    "        return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 20],\n",
       "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 30]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([torch.arange(1000).reshape(100,10)[::2].reshape(1, -1),torch.arange(1000).reshape(100,10)[1::2].reshape(1, -1)], dim=0)[:, :11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for p1 in [False, True]:\n",
    "# #     for p2 in [False, True]:\n",
    "# #         for p3 in [False, True]:\n",
    "# # print(f'\\n{p1}, {p2}, {p3}: \\n')\n",
    "# classifier_torch_model = CNN_for_Text_No_Positional_Encoding(num_embedding=num_embedding, hidden_dim=64, embedding_dim=64, pos_emb_size=3080, dropout=0.2, num_out_features=len(class_id), seed=911, random_edges=4, lattice_edges=4, lattice_step=2, virtual_nodes=0, lattice_start_distance=2, isXaiTests=True, num_tests=len(X.num_tokens)).eval()\n",
    "# flopt_counter = FlopCounterMode(classifier_torch_model)\n",
    "# with flopt_counter:\n",
    "#     classifier_torch_model(X.x, torch.zeros((2, 0)), X.token_subsampling_probabilities, X.token_indices, X.token_sentiments, X.token_lengths, X.num_tokens, X.character_length, X.token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CnnGnnClassifierLightningModel(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        num_classes,\n",
    "        optimizer=None,\n",
    "        loss_func=None,\n",
    "        learning_rate=0.01,\n",
    "        batch_size=64,\n",
    "        lr_scheduler=None,\n",
    "        user_lr_scheduler=False,\n",
    "        min_lr=0.0,\n",
    "    ):\n",
    "        super(CnnGnnClassifierLightningModel, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "        self.min_lr = min_lr\n",
    "        # self.save_hyperparameters(ignore=[\"model\"])\n",
    "        self.save_hyperparameters(logger=False)\n",
    "        self.optimizer = self._get_optimizer(optimizer)\n",
    "        self.lr_scheduler = (\n",
    "            self._get_lr_scheduler(lr_scheduler) if user_lr_scheduler else None\n",
    "        )\n",
    "        self.loss_func = loss_func\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.model(x.x, torch.zeros((2, 0)), x.token_subsampling_probabilities, x.token_indices, x.token_sentiments, x.token_lengths, x.num_tokens, x.character_length, x.token_embeddings)\n",
    "\n",
    "    def on_train_epoch_start(self) -> None:\n",
    "        param_groups = next(iter(self.optimizer.param_groups))\n",
    "        if \"lr\" in param_groups and param_groups[\"lr\"] is not None:\n",
    "            current_learning_rate = float(param_groups[\"lr\"])\n",
    "            self.log(\n",
    "                \"lr\",\n",
    "                current_learning_rate,\n",
    "                batch_size=self.batch_size,\n",
    "                on_epoch=True,\n",
    "                on_step=False,\n",
    "            )\n",
    "\n",
    "    def training_step(self, batch, *args, **kwargs):\n",
    "        X, y = batch\n",
    "        X.to(self.device)\n",
    "        y.to(self.device)\n",
    "        \n",
    "        self.model.train()\n",
    "        y_out = self(X)\n",
    "\n",
    "        loss = self.loss_func(y_out.view(y.shape), y )\n",
    "        self.train_losses.append(loss.detach().item())\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            batch_size=self.batch_size,\n",
    "            on_epoch=True,\n",
    "            on_step=True,\n",
    "        )\n",
    "        \n",
    "        self.train_acc(torch.argmax(y_out, dim=1), torch.argmax(y, dim=1))\n",
    "        self.log('train_acc', self.train_acc, prog_bar=True, on_epoch=True, on_step=True, batch_size=self.batch_size)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, *args, **kwargs):\n",
    "        X, y = batch\n",
    "        X.to(self.device)\n",
    "        y.to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        y_out = self(X)\n",
    "        loss = self.loss_func(y_out.view(y.shape), y )\n",
    "        self.val_losses.append(loss.detach().item())\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            batch_size=self.batch_size,\n",
    "            on_epoch=True,\n",
    "            on_step=True,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.val_acc(torch.argmax(y_out, dim=1), torch.argmax(y, dim=1))\n",
    "        self.log('val_acc', self.val_acc, prog_bar=True, on_epoch=True, on_step=True, batch_size=self.batch_size)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.lr_scheduler is None:\n",
    "            return self.optimizer\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": self.optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": self.lr_scheduler,\n",
    "                \"monitor\": \"train_loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def update_learning_rate(self, learning_rate: float):\n",
    "        self.learning_rate = learning_rate\n",
    "        for g in self.optimizer.param_groups:\n",
    "            g[\"lr\"] = learning_rate\n",
    "\n",
    "    def _get_optimizer(self, optimizer):\n",
    "        return (\n",
    "            optimizer\n",
    "            if optimizer is not None\n",
    "            else torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        )\n",
    "\n",
    "    def _get_lr_scheduler(self, lr_scheduler):\n",
    "        return (\n",
    "            lr_scheduler\n",
    "            if lr_scheduler is not None\n",
    "            else torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer, patience=5, factor=0.5, mode=\"min\", min_lr=self.min_lr\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from torchmetrics import ConfusionMatrix\n",
    "\n",
    "def calculate_metrics(cl_model, dataloader):\n",
    "    cm = ConfusionMatrix(task=\"multiclass\", num_classes=len(class_id))\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    cl_model = cl_model.eval()\n",
    "    cl_model.to(device)\n",
    "    for X, y in tqdm(dataloader):\n",
    "        X = X.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_p = cl_model(X)\n",
    "            y_p = y_p.cpu()\n",
    "        y_pred.append(y_p)\n",
    "        y_true.append(y)\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "    y_true = torch.cat(y_true, dim=0)\n",
    "    y_pred2 = torch.argmax(y_pred, dim=1)\n",
    "    y_true2 = torch.argmax(y_true, dim=1)\n",
    "    print(f'classification report: \\n {classification_report(y_true2, y_pred2, digits=4)}')\n",
    "    print(f'confusion matrix:\\n {cm(y_pred2, y_true2)}')\n",
    "    print('================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fardin Rastakhiz @ 2023\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from lightning.pytorch.loggers import Logger, CSVLogger\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from typing import List\n",
    "from pytorch_lightning.core.saving import save_hparams_to_yaml\n",
    "\n",
    "class ModelManager(ABC):\n",
    "\n",
    "    def __init__(self,\n",
    "                 torch_model: torch.nn.Module,\n",
    "                 lightning_model,\n",
    "                 model_save_dir: str = '~/Desktop',\n",
    "                 log_dir: str = 'logs/',\n",
    "                 log_name: str = 'model_logs',\n",
    "                 device='cpu',\n",
    "                 max_epochs = 100,\n",
    "                 ckpt_path: str|None=None,\n",
    "                 accumulate_grad_batches=1):\n",
    "        self.torch_model = torch_model\n",
    "        self.lightning_model = lightning_model\n",
    "        self.log_dir = log_dir\n",
    "        self.log_name = log_name\n",
    "        self.model_save_dir = model_save_dir\n",
    "        self.device = device\n",
    "        self.accelerator = 'cpu' if self.device=='cpu' else 'gpu'\n",
    "        self.max_epochs = max_epochs\n",
    "        self.ckpt_path = ckpt_path\n",
    "\n",
    "        self.logger = self._create_logger()\n",
    "        self.callbacks = self._create_callbacks()\n",
    "        self.trainer: L.Trainer = self._create_trainer(accumulate_grad_batches)\n",
    "        self.tuner = Tuner(self.trainer)\n",
    "        self.tuning_result = None\n",
    "\n",
    "    def tune(self, data_manager=None, train_dataloaders=None, val_dataloaders=None, datamodule=None, draw_result=True, min_lr=0.0000001, max_lr=0.1):\n",
    "        self.tuning_result = self.tuner.lr_find(self.lightning_model, datamodule=data_manager, train_dataloaders=train_dataloaders,\n",
    "                         val_dataloaders=val_dataloaders, min_lr=min_lr,max_lr=max_lr, num_training=150)\n",
    "        if draw_result:\n",
    "            fig = self.tuning_result.plot(suggest=True)\n",
    "            fig.show()\n",
    "        self.update_learning_rate(self.tuning_result.suggestion())\n",
    "        return self.tuning_result.suggestion()\n",
    "    \n",
    "    def update_learning_rate(self, lr):\n",
    "        self.lightning_model.update_learning_rate(lr)\n",
    "\n",
    "    def fit(self, train_dataloaders=None, val_dataloaders=None, datamodule=None, max_epochs = -1, ckpt_path=None):\n",
    "        if ckpt_path is not None and ckpt_path != '':\n",
    "            self.ckpt_path = ckpt_path\n",
    "        if max_epochs>0:\n",
    "            self.trainer.fit_loop.max_epochs = max_epochs\n",
    "            # self.max_epochs = max_epochs\n",
    "            # self.trainer = self._create_trainer()\n",
    "        self.trainer.fit(self.lightning_model,\n",
    "                         datamodule=datamodule,\n",
    "                         train_dataloaders=train_dataloaders,\n",
    "                         val_dataloaders=val_dataloaders,\n",
    "                         ckpt_path = self.ckpt_path\n",
    "                         )\n",
    "\n",
    "    def validate(self, dataloaders=None, datamodule=None):\n",
    "        return self.trainer.validate(self.lightning_model,\n",
    "                             datamodule=datamodule,\n",
    "                             dataloaders=dataloaders)\n",
    "\n",
    "    def predict(self, dataloaders=None, datamodule=None):\n",
    "        return self.trainer.predict(self.lightning_model,\n",
    "                             datamodule=datamodule,\n",
    "                             dataloaders=dataloaders)\n",
    "\n",
    "    def _create_trainer(self, accumulate_grad_batches) -> L.Trainer:\n",
    "        return L.Trainer(\n",
    "            callbacks=self.callbacks,\n",
    "            max_epochs=self.max_epochs,\n",
    "            accelerator=self.accelerator,\n",
    "            logger=self.logger,\n",
    "            num_sanity_val_steps=0,\n",
    "            default_root_dir=self.model_save_dir,\n",
    "            accumulate_grad_batches=accumulate_grad_batches\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def _create_callbacks(self) -> List[Callback]:\n",
    "        pass\n",
    "\n",
    "    def _create_logger(self) -> Logger:\n",
    "        return CSVLogger(save_dir=self.log_dir, name=self.log_name)\n",
    "\n",
    "    @abstractmethod\n",
    "    def draw_summary(self, dataloader):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def plot_csv_logger(self, loss_names, eval_names):\n",
    "        pass\n",
    "    \n",
    "    def save_hyper_parameters(self):\n",
    "        mhparams = {\n",
    "            'start_lr': 0.045,\n",
    "            'ckpt_lrs' :  {51: 0.002, 65: 0.00058},\n",
    "            'last_lr' : 0.0003,\n",
    "            'ac_loss_factor': 0.0002,\n",
    "            'weight_decay': 0.0012\n",
    "        }\n",
    "        save_hparams_to_yaml(config_yaml=r'logs\\hetero_model_17_AG\\version_12\\hparams.yaml',\n",
    "                     hparams=mhparams)\n",
    "        \n",
    "    # def find_best_settings(data_manager,\n",
    "    #                        lrs: List[float]=[0.001], dropouts: List[float]=[0.2], \n",
    "    #                        weight_decays: List[float]=[0.00055], emb_factors: List[float]=[0.1], \n",
    "    #                        batch_sizes: List[int]=[128], log_name='find_best_settings'):\n",
    "    #     for lr in lrs:\n",
    "    #         for dropout in dropouts:\n",
    "    #             for wd in weight_decays:\n",
    "    #                 for emb_factor in emb_factors:\n",
    "    #                     for bs in batch_sizes:\n",
    "    #                         data_manager.update_batch_size(bs)\n",
    "    #                         torch_model = HeteroGcnGatModel1(300, 1, X1.metadata(), 128, dropout=dropout)\n",
    "    #                         lightning_model = HeteroBinaryLightningModel(torch_model,\n",
    "    #                                         torch.optim.Adam(torch_model.parameters(), lr=lr, weight_decay=wd),\n",
    "    #                                             loss_func=HeteroLoss1(exception_keys='word', enc_factor=emb_factor),\n",
    "    #                                             learning_rate=lr,\n",
    "    #                                             batch_size=bs,\n",
    "    #                                             user_lr_scheduler=True\n",
    "    #                                             ).to(device)\n",
    "    #                         model_manager = ClassifierModelManager(torch_model, lightning_model, log_name=log_name, device=device, num_train_epoch=10)\n",
    "    #                         model_manager.fit(datamodule=data_manager)\n",
    "    #                         model_manager.save_plot_csv_logger(name_prepend=f'{lr}_{dropout}_{wd}_{emb_factor}_{bs}', loss_names=['train_loss', 'val_loss'], eval_names=['train_acc_epoch', 'val_acc_epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fardin Rastakhiz @ 2023\n",
    "import torch\n",
    "# from scripts.managers.ModelManager import ModelManager\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from torch_geometric.nn import summary\n",
    "from lightning.pytorch.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from os import path\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, hinge_loss\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "class ClassifierModelManager(ModelManager):\n",
    "\n",
    "    def __init__(self,\n",
    "                 torch_model: torch.nn.Module,\n",
    "                 lightning_model,\n",
    "                 model_save_dir: str = '~/Desktop',\n",
    "                 log_dir: str = 'logs/',\n",
    "                 log_name: str = 'model_logs',\n",
    "                 device='cpu',\n",
    "                 num_train_epoch = 100,\n",
    "                 accumulate_grad_batches=1):\n",
    "        super(ClassifierModelManager, self).__init__(torch_model, lightning_model, model_save_dir, log_dir, log_name, device, num_train_epoch, accumulate_grad_batches=accumulate_grad_batches)\n",
    "\n",
    "    def _create_callbacks(self) -> List[Callback]:\n",
    "        return [\n",
    "            ModelCheckpoint(save_top_k=2, mode='max', monitor='val_acc', save_last=True),\n",
    "            # EarlyStopping(patience=50, mode='max', monitor='val_acc')\n",
    "        ]\n",
    "\n",
    "    def draw_summary(self, dataloader):\n",
    "        X, y = next(iter(dataloader))\n",
    "        print(summary(self.torch_model, X.to(self.device)))\n",
    "\n",
    "    def plot_csv_logger(self, loss_names=['train_loss', 'val_loss'], eval_names=['train_acc', 'val_acc']):\n",
    "        csv_path = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', 'metrics.csv')\n",
    "        metrics = pd.read_csv(csv_path)\n",
    "\n",
    "        aggregation_metrics = []\n",
    "        agg_col = 'epoch'\n",
    "        for i, dfg in metrics.groupby(agg_col):\n",
    "            agg = dict(dfg.mean())\n",
    "            agg[agg_col] = i\n",
    "            aggregation_metrics.append(agg)\n",
    "\n",
    "        df_metrics = pd.DataFrame(aggregation_metrics)\n",
    "        df_metrics[loss_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='loss')\n",
    "        df_metrics[eval_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='accuracy')\n",
    "        plt.show()\n",
    "\n",
    "    def save_plot_csv_logger(self, loss_names=['train_loss', 'val_loss'], eval_names=['train_acc', 'val_acc'], name_prepend: str=\"\"):\n",
    "        csv_path = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', 'metrics.csv')\n",
    "        metrics = pd.read_csv(csv_path)\n",
    "\n",
    "        aggregation_metrics = []\n",
    "        agg_col = 'epoch'\n",
    "        for i, dfg in metrics.groupby(agg_col):\n",
    "            agg = dict(dfg.mean())\n",
    "            agg[agg_col] = i\n",
    "            aggregation_metrics.append(agg)\n",
    "\n",
    "        df_metrics = pd.DataFrame(aggregation_metrics)\n",
    "        df_metrics[loss_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='loss')\n",
    "        \n",
    "        loss_png = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', f'{name_prepend}_loss_metric.png')\n",
    "        plt.savefig(loss_png)\n",
    "        \n",
    "        df_metrics[eval_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='accuracy')\n",
    "        \n",
    "        acc_png = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', f'{name_prepend}_acc_metric.png')\n",
    "        plt.savefig(acc_png)\n",
    "        \n",
    "        plt.close()\n",
    "    \n",
    "    def evaluate(self, eval_dataloader,\n",
    "                 give_confusion_matrix: bool=True, \n",
    "                 give_report: bool=True, \n",
    "                 give_f1_score: bool=False, \n",
    "                 give_accuracy_score: bool=False, \n",
    "                 give_precision_score: bool=False, \n",
    "                 give_recall_score: bool=False, \n",
    "                 give_hinge_loss: bool=False):\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        self.lightning_model.eval()\n",
    "        for X, y in eval_dataloader:\n",
    "            y_p = self.lightning_model(X.to(self.device))\n",
    "            if type(y_p) is tuple:\n",
    "                y_p = y_p[0]\n",
    "            y_pred.append((y_p>0).to(torch.int32).detach().to(y.device))\n",
    "            y_true.append(y.to(torch.int32))\n",
    "        y_true = torch.concat(y_true)\n",
    "        y_pred = torch.concat(y_pred)\n",
    "        if(give_confusion_matrix):\n",
    "            print(f'confusion_matrix: \\n{confusion_matrix(y_true, y_pred)}')\n",
    "        if(give_report):\n",
    "            print(classification_report(y_true, y_pred))\n",
    "        if(give_f1_score):\n",
    "            print(f'f1_score: {f1_score(y_true, y_pred)}')\n",
    "        if(give_accuracy_score):\n",
    "            print(f'accuracy_score: {accuracy_score(y_true, y_pred)}')\n",
    "        if(give_precision_score):\n",
    "            print(f'precision_score: {precision_score(y_true, y_pred)}')\n",
    "        if(give_recall_score):\n",
    "            print(f'recall_score: {recall_score(y_true, y_pred)}')\n",
    "        # if(give_hinge_loss):\n",
    "        #     print(f'hinge_loss: {hinge_loss(y_true, y_pred)}')\n",
    "                \n",
    "    def evaluate_best_models(self, lightning_type: L.LightningModule, eval_dataloader,\n",
    "                             give_confusion_matrix: bool=True, \n",
    "                             give_report: bool=True, \n",
    "                             give_f1_score: bool=False, \n",
    "                             give_accuracy_score: bool=False, \n",
    "                             give_precision_score: bool=False, \n",
    "                             give_recall_score: bool=False, \n",
    "                             give_hinge_loss: bool=False,\n",
    "                             multi_class: bool=False, **kwargs):\n",
    "        self.lightning_model = lightning_type.load_from_checkpoint(rf'{self.trainer.checkpoint_callback.best_model_path}', map_location=None, hparams_file=None, strict=True, **kwargs).eval()\n",
    "        self.save_evaluation(eval_dataloader, 'best_model', give_confusion_matrix, give_report,\n",
    "                             give_f1_score, give_accuracy_score, give_precision_score, give_recall_score, give_hinge_loss, multi_class)\n",
    "            \n",
    "    def save_evaluation(self, eval_dataloader, name_prepend: str='',\n",
    "                    give_confusion_matrix: bool=True, \n",
    "                    give_report: bool=True, \n",
    "                    give_f1_score: bool=False, \n",
    "                    give_accuracy_score: bool=False, \n",
    "                    give_precision_score: bool=False, \n",
    "                    give_recall_score: bool=False, \n",
    "                    give_hinge_loss: bool=False,\n",
    "                    multi_class: bool=False\n",
    "                    ):\n",
    "            \n",
    "            test_metrics_path = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', f'{name_prepend}_test_metrics.txt')\n",
    "            \n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            self.lightning_model.eval()\n",
    "            self.lightning_model.model.eval()\n",
    "            self.torch_model.eval()\n",
    "            self.trainer.model.eval()\n",
    "            for X, y in eval_dataloader:\n",
    "                with torch.no_grad():\n",
    "                    y_p = self.lightning_model(X.to(self.device))\n",
    "                if type(y_p) is tuple:\n",
    "                    y_p = y_p[0]\n",
    "                \n",
    "                if multi_class:\n",
    "                    y_pred.append(y_p.detach().to(y.device))\n",
    "                    y_true.append(y)\n",
    "                else:\n",
    "                    y_pred.append((y_p>0).to(torch.int32).detach().to(y.device))\n",
    "                    y_true.append(y.to(torch.int32))\n",
    "                    \n",
    "            y_true = torch.concat(y_true)\n",
    "            y_pred = torch.concat(y_pred)\n",
    "            print(y_true.shape)\n",
    "            print(y_pred.shape)\n",
    "            if multi_class:\n",
    "                y_true_num = torch.argmax(y_true, dim=1)\n",
    "                y_pred_num = torch.argmax(y_pred, dim=1)\n",
    "            else:\n",
    "                y_true_num = y_true\n",
    "                y_pred_num = y_pred\n",
    "                \n",
    "            print(y_true_num.shape)\n",
    "            print(y_pred_num.shape)\n",
    "            with open(test_metrics_path, 'at+') as f:\n",
    "                if(give_confusion_matrix):\n",
    "                    print(f'confusion_matrix: \\n{confusion_matrix(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_report):\n",
    "                    print(classification_report(y_true_num, y_pred_num), file=f)\n",
    "                if(give_f1_score):\n",
    "                    if multi_class:\n",
    "                        print(f'f1_score: {f1_score(y_true_num, y_pred_num, average=None)}', file=f)\n",
    "                    else:\n",
    "                        print(f'f1_score: {f1_score(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_accuracy_score):\n",
    "                    print(f'accuracy_score: {accuracy_score(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_precision_score):\n",
    "                    if multi_class:\n",
    "                        print(f'precision: {precision_score(y_true_num, y_pred_num, average=None)}', file=f)\n",
    "                    else:\n",
    "                        print(f'precision: {precision_score(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_recall_score):\n",
    "                    if multi_class:\n",
    "                        print(f'recall: {recall_score(y_true_num, y_pred_num, average=None)}', file=f)\n",
    "                    else:\n",
    "                        print(f'recall: {recall_score(y_true_num, y_pred_num)}', file=f)\n",
    "                # if(give_hinge_loss):\n",
    "                #     print(f'hinge_loss: {hinge_loss(y_true_num, y_pred)}', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 128\n",
    "hidden_dim = 64\n",
    "embedding_dim = 64\n",
    "label_size = 1\n",
    "seed = 911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_size = 32\n",
    "# hidden_dim = 16\n",
    "# embedding_dim = 16\n",
    "# label_size = 1\n",
    "# seed = 911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LabelSmoothingLoss(nn.Module):\n",
    "#     \"\"\"\n",
    "#     >> from https://github.com/OpenNMT/\n",
    "#     With label smoothing,\n",
    "#     KL-divergence between q_{smoothed ground truth prob.}(w)\n",
    "#     and p_{prob. computed by model}(w) is minimized.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, label_smoothing, tgt_vocab_size, ignore_index=-100):\n",
    "#         assert 0.0 < label_smoothing <= 1.0\n",
    "#         self.ignore_index = ignore_index\n",
    "#         super(LabelSmoothingLoss, self).__init__()\n",
    "\n",
    "#         smoothing_value = label_smoothing / (tgt_vocab_size - 2)\n",
    "#         one_hot = torch.full((tgt_vocab_size,), smoothing_value)\n",
    "#         one_hot[self.ignore_index] = 0\n",
    "#         self.register_buffer('one_hot', one_hot.unsqueeze(0))\n",
    "\n",
    "#         self.confidence = 1.0 - label_smoothing\n",
    "\n",
    "#     def forward(self, output, target):\n",
    "#         \"\"\"\n",
    "#         output (FloatTensor): batch_size x n_classes\n",
    "#         target (LongTensor): batch_size\n",
    "#         \"\"\"\n",
    "#         model_prob = self.one_hot.repeat(target.size(0), 1)\n",
    "#         model_prob.scatter_(1, target.unsqueeze(1), self.confidence)\n",
    "#         model_prob.masked_fill_((target == self.ignore_index).unsqueeze(1), 0)\n",
    "#         print(output.shape, model_prob.shape)\n",
    "#         return F.kl_div(output, model_prob, reduction='sum')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "def train_model(epochs=30, dropout=0.25, weight_decay=0.000012, lr=0.0002, amsgrad=False, fused=True, use_positional_encoder=[False, False, False]):\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    classifier_torch_model = CNN_for_Text_No_Positional_Encoding(num_embedding=num_embedding, hidden_dim=hidden_dim, embedding_dim=embedding_dim, pos_emb_size=4096, dropout=dropout, num_out_features=len(class_id), seed=seed, random_edges=6, lattice_edges=10, lattice_step=2, virtual_nodes=0, lattice_start_distance=2).to(device)\n",
    "    # optimizer = torch.optim.Adam(classifier_torch_model.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=amsgrad, fused=fused)\n",
    "    optimizer = torch.optim.AdamW(classifier_torch_model.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=amsgrad, fused=fused)\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 100, 150, 200, 250, 300, 350],gamma=0.5, verbose=False)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15, 20, 30, 40, 45,50,55],gamma=0.5, verbose=False)\n",
    "    loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "    classfier_lightning_model = CnnGnnClassifierLightningModel(classifier_torch_model, \n",
    "                                                        num_classes=len(class_id),\n",
    "                                                learning_rate=lr,\n",
    "                                                batch_size=batch_size,\n",
    "                                                optimizer=optimizer,\n",
    "                                                loss_func=loss_func,\n",
    "                                                lr_scheduler=lr_scheduler,\n",
    "                                                user_lr_scheduler=True\n",
    "                                                ).to(device)\n",
    "\n",
    "\n",
    "    model_manager = ClassifierModelManager(classifier_torch_model, classfier_lightning_model, log_name=f'CNN-GNN_{use_positional_encoder[0]}_{use_positional_encoder[1]}_{use_positional_encoder[2]}',device=device, num_train_epoch=epochs, accumulate_grad_batches=1)\n",
    "    # trainer = L.Trainer(\n",
    "    #             # callbacks=callbacks,\n",
    "    #             max_epochs=epochs,\n",
    "    #             accelerator= 'gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    #             logger=CSVLogger(save_dir='logs/', name='log2'), \n",
    "    #             num_sanity_val_steps=0,\n",
    "    #         #     default_root_dir='models\\model2_word_embedding-256-2'\n",
    "    #         )\n",
    "\n",
    "    train_dataset.reset_params()\n",
    "    train_dataset.position_j = 0\n",
    "    test_dataset.reset_params()\n",
    "    test_dataset.position_j = 0\n",
    "    \n",
    "    # train_dataset.section_i = 0\n",
    "    # train_dataset.each_section_i = np.zeros((train_dataset.num_sections, ), dtype=int)\n",
    "    # test_dataset.section_i = 0\n",
    "    # test_dataset.each_section_i = np.zeros((test_dataset.num_sections, ), dtype=int)\n",
    "    \n",
    "    model_manager.fit(train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)\n",
    "    model_manager.save_plot_csv_logger(loss_names=['train_loss_epoch', 'val_loss_epoch'], eval_names=['train_acc_epoch', 'val_acc_epoch'], name_prepend=f'tests_{dropout}_{weight_decay}_{lr}_{amsgrad}_{fused}')\n",
    "    model_manager.torch_model = model_manager.torch_model.to(device)\n",
    "    model_manager.save_evaluation(test_dataloader, f'{dropout}_{weight_decay}_{lr}]',True, True, True, True, True, True, True, multi_class=True)\n",
    "    # trainer.fit(classfier_lightning_model, train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)\n",
    "    classfier_lightning_model.model = classfier_lightning_model.model.eval()\n",
    "    classfier_lightning_model = classfier_lightning_model.eval()\n",
    "    calculate_metrics(classfier_lightning_model, test_dataloader)\n",
    "    model_manager.evaluate_best_models(CnnGnnClassifierLightningModel, test_dataloader,True, True, True, True, True, True, True, multi_class=True, model=classifier_torch_model, num_classes=len(class_id))\n",
    "    return model_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'loss_func' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_func'])`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3080') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                                | Params | Mode \n",
      "--------------------------------------------------------------------------\n",
      "0 | model     | CNN_for_Text_No_Positional_Encoding | 262 K  | train\n",
      "1 | loss_func | LabelSmoothingLoss                  | 0      | train\n",
      "2 | train_acc | MulticlassAccuracy                  | 0      | train\n",
      "3 | val_acc   | MulticlassAccuracy                  | 0      | train\n",
      "4 | test_acc  | MulticlassAccuracy                  | 0      | train\n",
      "--------------------------------------------------------------------------\n",
      "262 K     Trainable params\n",
      "0         Non-trainable params\n",
      "262 K     Total params\n",
      "1.049     Total estimated model params size (MB)\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d9e001f153459fbb329b2078f933ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "scatter(): Expected dtype int64 for index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\Articles\\CGNet\\FindBestModel\\7_FurtherImprovements\\with_positional_encoding_label_smoothing.ipynb Cell 68\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_manager \u001b[39m=\u001b[39m train_model(\u001b[39m70\u001b[39;49m, \u001b[39m0.2\u001b[39;49m, \u001b[39m0.000012\u001b[39;49m, \u001b[39m0.0032\u001b[39;49m, use_positional_encoder\u001b[39m=\u001b[39;49m[\u001b[39mFalse\u001b[39;49;00m, \u001b[39mFalse\u001b[39;49;00m, \u001b[39mFalse\u001b[39;49;00m])\n",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\Articles\\CGNet\\FindBestModel\\7_FurtherImprovements\\with_positional_encoding_label_smoothing.ipynb Cell 68\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m test_dataset\u001b[39m.\u001b[39mposition_j \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# train_dataset.section_i = 0\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# train_dataset.each_section_i = np.zeros((train_dataset.num_sections, ), dtype=int)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# test_dataset.section_i = 0\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# test_dataset.each_section_i = np.zeros((test_dataset.num_sections, ), dtype=int)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m model_manager\u001b[39m.\u001b[39;49mfit(train_dataloaders\u001b[39m=\u001b[39;49mtrain_dataloader, val_dataloaders\u001b[39m=\u001b[39;49mtest_dataloader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m model_manager\u001b[39m.\u001b[39msave_plot_csv_logger(loss_names\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mtrain_loss_epoch\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mval_loss_epoch\u001b[39m\u001b[39m'\u001b[39m], eval_names\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mtrain_acc_epoch\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mval_acc_epoch\u001b[39m\u001b[39m'\u001b[39m], name_prepend\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtests_\u001b[39m\u001b[39m{\u001b[39;00mdropout\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mweight_decay\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mlr\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mamsgrad\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mfused\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m model_manager\u001b[39m.\u001b[39mtorch_model \u001b[39m=\u001b[39m model_manager\u001b[39m.\u001b[39mtorch_model\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\Articles\\CGNet\\FindBestModel\\7_FurtherImprovements\\with_positional_encoding_label_smoothing.ipynb Cell 68\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mmax_epochs \u001b[39m=\u001b[39m max_epochs\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39m# self.max_epochs = max_epochs\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     \u001b[39m# self.trainer = self._create_trainer()\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mfit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_model,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m                  datamodule\u001b[39m=\u001b[39;49mdatamodule,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m                  train_dataloaders\u001b[39m=\u001b[39;49mtrain_dataloaders,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m                  val_dataloaders\u001b[39m=\u001b[39;49mval_dataloaders,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m                  ckpt_path \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m                  )\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[0;32m    542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 543\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    544\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    545\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    573\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    574\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[0;32m    575\u001b[0m     ckpt_path,\n\u001b[0;32m    576\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    577\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    578\u001b[0m )\n\u001b[1;32m--> 579\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[0;32m    581\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    582\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:986\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    983\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 986\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m    988\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    991\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1030\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1028\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1029\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1030\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m   1031\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 205\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance()\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    207\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    362\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[0;32m    139\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(data_fetcher)\n\u001b[0;32m    141\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[0;32m    142\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[0;32m    249\u001b[0m         \u001b[39m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[1;32m--> 250\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mautomatic_optimization\u001b[39m.\u001b[39;49mrun(trainer\u001b[39m.\u001b[39;49moptimizers[\u001b[39m0\u001b[39;49m], batch_idx, kwargs)\n\u001b[0;32m    251\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    252\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_optimization\u001b[39m.\u001b[39mrun(kwargs)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[1;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m         closure()\n\u001b[0;32m    185\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(batch_idx, closure)\n\u001b[0;32m    192\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[0;32m    193\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[1;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_ready()\n\u001b[0;32m    267\u001b[0m \u001b[39m# model hook\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m call\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[0;32m    269\u001b[0m     trainer,\n\u001b[0;32m    270\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    271\u001b[0m     trainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[0;32m    272\u001b[0m     batch_idx,\n\u001b[0;32m    273\u001b[0m     optimizer,\n\u001b[0;32m    274\u001b[0m     train_step_and_backward_closure,\n\u001b[0;32m    275\u001b[0m )\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n\u001b[0;32m    278\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:159\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    156\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[0;32m    158\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 159\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    161\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    162\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\core\\module.py:1308\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[0;32m   1277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[0;32m   1278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1279\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1282\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1283\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1284\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[39m    the optimizer.\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1306\u001b[0m \n\u001b[0;32m   1307\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1308\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\core\\optimizer.py:153\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    152\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strategy\u001b[39m.\u001b[39;49moptimizer_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer, closure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    155\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_after_step()\n\u001b[0;32m    157\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:238\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule)\n\u001b[1;32m--> 238\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision_plugin\u001b[39m.\u001b[39;49moptimizer_step(optimizer, model\u001b[39m=\u001b[39;49mmodel, closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision.py:122\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[1;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[0;32m    121\u001b[0m closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[1;32m--> 122\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     74\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 75\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    392\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adamw.py:165\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m--> 165\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    167\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[0;32m    168\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision.py:108\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[1;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_closure\u001b[39m(\n\u001b[0;32m     96\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     97\u001b[0m     model: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     98\u001b[0m     optimizer: Optimizer,\n\u001b[0;32m     99\u001b[0m     closure: Callable[[], Any],\n\u001b[0;32m    100\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    101\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[39m    hook is called.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m \n\u001b[0;32m    107\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     closure_result \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    109\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer)\n\u001b[0;32m    110\u001b[0m     \u001b[39mreturn\u001b[39;00m closure_result\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39m@override\u001b[39m\n\u001b[0;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclosure(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    145\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:129\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[39m@override\u001b[39m\n\u001b[0;32m    127\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39menable_grad()\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ClosureResult:\n\u001b[1;32m--> 129\u001b[0m     step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_fn()\n\u001b[0;32m    131\u001b[0m     \u001b[39mif\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarning_cache\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:317\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \n\u001b[0;32m    308\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m \n\u001b[0;32m    314\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    315\u001b[0m trainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\n\u001b[1;32m--> 317\u001b[0m training_step_output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mtraining_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[0;32m    318\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mpost_training_step()  \u001b[39m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[39mif\u001b[39;00m training_step_output \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m trainer\u001b[39m.\u001b[39mworld_size \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:311\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 311\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    313\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    314\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module:\n\u001b[0;32m    389\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_redirection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39m\"\u001b[39m\u001b[39mtraining_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 390\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module\u001b[39m.\u001b[39;49mtraining_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\Articles\\CGNet\\FindBestModel\\7_FurtherImprovements\\with_positional_encoding_label_smoothing.ipynb Cell 68\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m y_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(X)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_func(y_out\u001b[39m.\u001b[39;49mview(y\u001b[39m.\u001b[39;49mshape), y )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     loss,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     on_step\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\Articles\\CGNet\\FindBestModel\\7_FurtherImprovements\\with_positional_encoding_label_smoothing.ipynb Cell 68\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39moutput (FloatTensor): batch_size x n_classes\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mtarget (LongTensor): batch_size\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m model_prob \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mone_hot\u001b[39m.\u001b[39mrepeat(target\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m model_prob\u001b[39m.\u001b[39;49mscatter_(\u001b[39m1\u001b[39;49m, target\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfidence)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m model_prob\u001b[39m.\u001b[39mmasked_fill_((target \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignore_index)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), \u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_FurtherImprovements/with_positional_encoding_label_smoothing.ipynb#Y123sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mkl_div(output, model_prob, reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: scatter(): Expected dtype int64 for index"
     ]
    }
   ],
   "source": [
    "model_manager = train_model(70, 0.2, 0.000012, 0.0032, use_positional_encoder=[False, False, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculatge_metrics(chpt_path, target_data_loader, num_embedding):\n",
    "        classifier_torch_model = CNN_for_Text_No_Positional_Encoding(num_embedding=num_embedding, hidden_dim=hidden_dim, embedding_dim=embedding_dim, pos_emb_size=4096, dropout=0.2, num_out_features=len(class_id), seed=seed, random_edges=4, lattice_edges=8, lattice_step=2, virtual_nodes=0, lattice_start_distance=2)\n",
    "        classfier_lightning_model = CnnGnnClassifierLightningModel.load_from_checkpoint(chpt_path, model=classifier_torch_model, num_classes=len(class_id)).eval()\n",
    "        mean_infer_acc = []\n",
    "        mean_infer_f1 = []\n",
    "        mean_infer_prec = []\n",
    "        mean_infer_rec = []\n",
    "        for i in range(5):\n",
    "            all_ys = []\n",
    "            all_y_preds = []\n",
    "            for X, y in target_data_loader:\n",
    "                with torch.no_grad():\n",
    "                    y_pred = classfier_lightning_model(X.to(device))\n",
    "                all_ys.append(torch.argmax(y,dim=1))\n",
    "                all_y_preds.append(torch.argmax(y_pred.cpu(), dim=1))\n",
    "            all_ys = torch.concat(all_ys)\n",
    "            all_y_preds = torch.concat(all_y_preds)\n",
    "            \n",
    "            cm = confusion_matrix(all_ys, all_y_preds)\n",
    "            \n",
    "            accuracy = np.sum(np.diag(cm))/ np.sum(cm)\n",
    "            precision = np.mean(np.diag(cm) / np.sum(cm, axis=0))\n",
    "            recall = np.mean(np.diag(cm) / np.sum(cm, axis=1))\n",
    "            f1_score = (2*precision*recall)/(precision + recall)\n",
    "            \n",
    "            mean_infer_acc.append(accuracy)\n",
    "            mean_infer_f1.append(f1_score)\n",
    "            mean_infer_prec.append(precision)\n",
    "            mean_infer_rec.append(recall)\n",
    "        mean_infer_acc = torch.mean(torch.tensor(mean_infer_acc))\n",
    "        mean_infer_f1 = torch.mean(torch.tensor(mean_infer_f1))\n",
    "        mean_infer_prec = torch.mean(torch.tensor(mean_infer_prec))\n",
    "        mean_infer_rec = torch.mean(torch.tensor(mean_infer_rec))\n",
    "        return mean_infer_acc, mean_infer_f1, mean_infer_prec, mean_infer_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "def get_best_chpt(metrics_path, epoch_numbers):\n",
    "    epoch_data = pd.read_csv(metrics_path)\n",
    "    if 'val_acc_epoch' in epoch_data.columns and epoch_data['val_acc_epoch'].notna().any():\n",
    "        best_chpt = epoch_data.loc[epoch_data['val_acc_epoch'].idxmax()]\n",
    "    elif 'val_loss_epoch' in epoch_data.columns and epoch_data['val_loss_epoch'].notna().any():\n",
    "        best_chpt = epoch_data.loc[epoch_data['val_loss_epoch'].idxmin()]\n",
    "    else:\n",
    "        raise ValueError(f\"No valid validation metrics available for epoch {epoch_numbers}.\")\n",
    "    return np.argwhere(np.array(epoch_numbers)==best_chpt['epoch']).item(), best_chpt['val_loss_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_metrics_mean(num_embedding, base_path = 'logs\\CNN-GNN18_mr2k_seeds', start=0, interval=1):\n",
    "    total_accuracy = []\n",
    "    total_f1 = []\n",
    "    total_prec = []\n",
    "    total_rec = []\n",
    "    total_loss = []\n",
    "    \n",
    "    for i in range(start, start + interval):\n",
    "        version_path = join(base_path, f'version_{i}')\n",
    "        checkpoint_path = join(version_path, f'checkpoints')\n",
    "        onlyfiles  = [f for f in listdir(checkpoint_path) if (isfile(join(checkpoint_path, f)) and 'epoch' in f) ]\n",
    "        epoch_numbers = [int(re.search(r'\\d+', f).group()) for f in onlyfiles]\n",
    "        best_chpt_id, loss = get_best_chpt(join(version_path, 'metrics.csv'), epoch_numbers)\n",
    "        print(onlyfiles[best_chpt_id])\n",
    "        mean_infer_acc, mean_infer_f1, mean_infer_prec, mean_infer_rec = calculatge_metrics(join(checkpoint_path, f'{onlyfiles[best_chpt_id]}'), test_dataloader, num_embedding)\n",
    "            \n",
    "        total_accuracy.append(mean_infer_acc)\n",
    "        total_f1.append(mean_infer_f1)\n",
    "        total_prec.append(mean_infer_prec)\n",
    "        total_rec.append(mean_infer_rec)\n",
    "        total_loss.append(loss)\n",
    "\n",
    "    total_accuracy = torch.mean(torch.tensor(total_accuracy))\n",
    "    total_f1 = torch.mean(torch.tensor(total_f1))\n",
    "    total_prec = torch.mean(torch.tensor(total_prec))\n",
    "    total_rec = torch.mean(torch.tensor(total_rec))\n",
    "    total_loss = torch.mean(torch.tensor(total_loss))\n",
    "    print(f'total_accuracy: {total_accuracy}')\n",
    "    print(f'total_f1: {total_f1}')\n",
    "    print(f'total_prec: {total_prec}')\n",
    "    print(f'total_rec: {total_rec}')\n",
    "    print(f'total_loss: {total_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=16-step=1666.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'loss_func' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_func'])`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_accuracy: 0.49994419642857146\n",
      "total_f1: 0.39090660134256133\n",
      "total_prec: 0.33331539162468354\n",
      "total_rec: 0.49996012377109383\n",
      "total_loss: 0.2354898452758789\n"
     ]
    }
   ],
   "source": [
    "calculate_average_metrics_mean(212, r'logs\\CNN-GNN_False_False_False', start=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_average_metrics_mean(r'logs\\CNN-GNN_False_False_False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import captum\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from captum.attr import LayerIntegratedGradients, TokenReferenceBase, visualization, IntegratedGradients\n",
    "from torch_geometric.nn.models.captum import to_captum_model, CaptumModel\n",
    "from torch_geometric.explain.algorithm import CaptumExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(num_embedding, base_path = 'logs\\CNN-GNN18_mr2k_seeds', version=0, n_steps=50, step_of_test=0):\n",
    "        version_path = join(base_path, f'version_{version}')\n",
    "        checkpoint_path = join(version_path, f'checkpoints')\n",
    "        onlyfiles  = [f for f in listdir(checkpoint_path) if (isfile(join(checkpoint_path, f)) and 'epoch' in f) ]\n",
    "        epoch_numbers = [int(re.search(r'\\d+', f).group()) for f in onlyfiles]\n",
    "        best_chpt_id, loss = get_best_chpt(join(version_path, 'metrics.csv'), epoch_numbers)\n",
    "        chpt_path = join(checkpoint_path, f'{onlyfiles[best_chpt_id]}')\n",
    "        classifier_torch_model = CNN_for_Text_No_Positional_Encoding(num_embedding=num_embedding, hidden_dim=hidden_dim, embedding_dim=embedding_dim, pos_emb_size=4096, dropout=0.2, num_out_features=len(class_id), seed=seed, random_edges=4, lattice_edges=8, lattice_step=2, virtual_nodes=0, lattice_start_distance=2, isXaiTests=True, num_tests=n_steps, step_of_test=step_of_test)\n",
    "        classfier_lightning_model = CnnGnnClassifierLightningModel.load_from_checkpoint(chpt_path, model=classifier_torch_model, num_classes=len(class_id)).eval()\n",
    "        return classfier_lightning_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'loss_func' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_func'])`.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(212, r'logs\\CNN-GNN_False_False_False', 22, n_steps=50, step_of_test=i).eval()\n",
    "model = model.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_for_Text_No_Positional_Encoding(\n",
       "  (embedding): Embedding(212, 64)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (conv1): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (conv3): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (conv4): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (sentiment1): Sentiment_Injection(\n",
       "    (conv1): Conv1d(2, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (sentiment2): Sentiment_Injection(\n",
       "    (conv1): Conv1d(2, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (gcnn1): GCNN(\n",
       "    (gnn): GATv2Conv(64, 8, heads=4)\n",
       "    (conv): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (fc): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (gcnn2): GCNN(\n",
       "    (gnn): GATv2Conv(128, 16, heads=4)\n",
       "    (conv): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (fc): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (graph_generator): GenGraph(\n",
       "    (virtual_node_embeddings): Embedding(0, 64)\n",
       "  )\n",
       "  (fc0): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=32, bias=True)\n",
       "  (fc_out): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accumalate couple samples in this array for visualization purposes\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "def prepare_input_indices(graph, id_char_dict):\n",
    "    \n",
    "    characters = [id_char_dict[i.item()] for i in graph.x]\n",
    "    tokens = ['']*len(graph.token_indices)\n",
    "    for i, j in enumerate(graph.token_indices):\n",
    "        j = j.item()\n",
    "        tokens[j] += id_char_dict[graph.x[i].item()]\n",
    "    ref_graph = deepcopy(graph)\n",
    "    ref_graph.x = torch.ones_like(graph.x) * list(id_char_dict.keys())[-1]\n",
    "\n",
    "    return graph, ref_graph, tokens, characters\n",
    "\n",
    "token_reference = TokenReferenceBase(reference_token_idx=211)\n",
    "\n",
    "def interpret_sentence(lig, model, graph, labels, id_char_dict, min_len = 7, label = 0, visualize_on_tokens=False, n_steps=50, dev='cpu', return_delta=True):\n",
    "    #labels = labels.argmax().item()\n",
    "    graph.cumulative_token_indices = graph.token_indices\n",
    "    graph.edge_index = torch.zeros((2, 0))\n",
    "    # print(f'1: {graph.character_length.shape}')\n",
    "    graph, ref_graph, tokens, characters = prepare_input_indices(graph, id_char_dict)\n",
    "    #[tok.text for tok in tokenizer(sentence.lower())]\n",
    "    \n",
    "    graph = Batch.from_data_list([graph])\n",
    "    ref_graph = Batch.from_data_list([ref_graph])\n",
    "    \n",
    "    model.zero_grad()\n",
    "    # predict\n",
    "    ref_graph = ref_graph.to(dev)\n",
    "    graph = graph.to(dev)\n",
    "    model = model.to(dev)\n",
    "    \n",
    "    \n",
    "    model_output = model(graph.x, graph.edge_index, graph.token_subsampling_probabilities, graph.cumulative_token_indices, graph.token_sentiments, graph.token_lengths, graph.num_tokens, graph.character_length, graph.token_embeddings)\n",
    "    # print(f'graph: {graph}, model out dim: {model.num_out_features}, n_steps: {n_steps}')\n",
    "    # print(f'model_output.shape: {model_output.shape}')\n",
    "    pred = torch.softmax(model_output, dim=1).detach()\n",
    "    pred_ind = torch.argmax(pred).item()\n",
    "    # print(f'pred: {pred}, pred_ind: {pred_ind}')\n",
    "\n",
    "    seq_length = min_len\n",
    "    # generate reference indices for each sample\n",
    "    reference_indices = token_reference.generate_reference(seq_length, device=device).unsqueeze(0)\n",
    "\n",
    "    # compute attributions and approximation delta using layer integrated gradients\n",
    "    output = lig.attribute(graph.x, ref_graph.x, \\\n",
    "                                           additional_forward_args=(graph.edge_index, graph.token_subsampling_probabilities, graph.cumulative_token_indices, graph.token_sentiments, graph.token_lengths, graph.num_tokens, graph.character_length, graph.token_embeddings), \\\n",
    "                                           n_steps=n_steps, return_convergence_delta=return_delta, target=labels)\n",
    "    \n",
    "    if(return_delta):\n",
    "        attributions_ig, delta = output\n",
    "    else:\n",
    "        # print(f'outputoutput: {output}')\n",
    "        attributions_ig = output\n",
    "        # print(f'attributions_ig: {attributions_ig}')\n",
    "        delta = 0\n",
    "        \n",
    "    # captum_exp = CaptumExplainer(IntegratedGradients)\n",
    "\n",
    "    # print(f'true: {id_class[labels]}({labels}), pred: {id_class[pred_ind]}({pred_ind}), max delta: {delta}')\n",
    "    \n",
    "    if(type(attributions_ig) is tuple):\n",
    "        # print(f'attributions_ig: {attributions_ig[0].shape}, {attributions_ig[1].shape}')\n",
    "        attributions_ig = attributions_ig[0]\n",
    "    # else:\n",
    "    #     print(f'attributions_ig: {attributions_ig.shape}')\n",
    "    # print(f'text: {tokens}, attributions_ig: {attributions_ig.shape}, labels: {labels}, delta: {delta}')\n",
    "    # print(f'attributions_ig: {attributions_ig.shape}')\n",
    "    return attributions_ig, tokens, characters, pred, pred_ind, label, delta\n",
    "            \n",
    "def add_attributions_to_visualizer(attributions, text, pred, pred_ind, labels, delta, vis_data_records):\n",
    "    attributions = attributions.sum(dim=1)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    attributions = attributions.cpu().detach().numpy()\n",
    "    # print(attributions.shape)\n",
    "    # storing couple samples in an array for visualization purposes\n",
    "    vis_data_records.append(visualization.VisualizationDataRecord(\n",
    "                            attributions,\n",
    "                            pred,\n",
    "                            id_class[pred_ind],\n",
    "                            id_class[labels],\n",
    "                            id_class[1],\n",
    "                            attributions.sum(),\n",
    "                            text,\n",
    "                            delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10785\n",
      "[10785  5931 21341  2054  4867  6064 14618  6301 20927 10541]\n",
      "The following \"review\" is one from the wrong side of the tracks, meaning two things: You will hear nothing but praises coming from me and don't be fooled by my rating. I also could tell you that this review contains no spoilers, but technically... that's a lie.<br /><br />Well,... Screw the review. I'm just going to ramble a bit. It has been a while since I had so much fun with such a bad film. And if you cannot enjoy this piece of utter drivel, then you simply have no sense of humor. First off, this movie was meant to be taken seriously, and that's the main aspect were the fun is coming from. The story? A doctor's son is terminally ill. Daddy-Doctor decides that a hart-transplant must be the only way of saving his son. So, in true mad-doctor-style, he decides that the heart of a gorilla will do the trick. Of course, the gorilla-heart is \"much too potent for a human\", so sonny-boy transforms into this hideous Ape-Man that immediately breaks free and goes on a killing and raping spree (because that's simply what any horny beast would do, isn't it?). The make-up of our Ape-Man is hilarious. There's simply no other way of putting it: The guys' face looks like a turd! I'm talking human excrement here, the solid brown kind! Beautiful! The gore in this flick is wonderful too: Aside from real footage of an open-heart surgery, we also have incredibly fake (but quite nasty) looking blood & gore effects: a decapitation, an eye-gouging, a throat-ripping, the scalping of someone's skull,... It's hilarious, and indeed it's quite incomprehensible why NIGHT OF THE BLOODY APES ever made it to the notorious UK Video-Nasties list, because all of the nasty things portrayed in this film are simply too ridiculous to be taken seriously. Oh, and there's only one bloody ape running around in it too, by the way. So, needless to say that LA HORRIPILANTE BESTIA HUMANA is a much more accurate title for this terrifying and convincing tale of science gone horribly wrong.<br /><br />When not trying to shock with poorly staged acts of bloody violence or random portrayals of female nudity, this movie manages to be highly entertaining with ingredients like nonsensical dialogues, cheap cardboard sets, plot stupidity and incredibly bad acting. Just a few random examples, maybe? About the sets: One can clearly see that the set-designers just used the same walls, doors, windows (etc.) over and over again to be build various different interior sets (e.g.: One side of the hospital room - the window side - where the unconscious wrestling girl lies, looks suspiciously similar like the window side of the basement-laboratory from where Ape-Man escapes; The set-designers also took one of the side-windows from the laboratory, made it a bit larger and just placed it in the conference-room of the hospital; All the walls in any building are the same grey-ones;...). Then there's the setting of the park. Clearly shot in a studio, you can see (on several occasions) that the grass is loose. Whenever there's some struggling/fighting/raping going on, the grass just shifts and shuffles and you can see the grey concrete from the studio-floor beneath it.<br /><br />Just a few random lines that come out the actors' mouths: <br /><br />-- In the conference-room where all the doctors are debating the disappearance of Unconscious Wrestling Girl (a disappearance that would of course mean bad publicity for the hospital), Daddy-Doctor intelligently utters \"We find ourselves in a situation that is difficult\".<br /><br />-- During that same debate a colleague-doctor cleverly remarks \"A sleepwalker! Any sleepwalker gets up.\", hereby providing a solid excuse for the disappearance of Unconscious Wrestling Girl.<br /><br />-- After our investigating detective, through the amazing process of his own logical deduction, concludes and tells his superior that the murderer must be a half man/half beast, his superior answers that it's absurd, adding the line \"It's more probable that of late, more and more, you're watching on your television many of those pictures of terror\"... Truly one of the best lines of the movie.<br /><br />Other sources of laughter: <br /><br />-- The two scenes were Daddy-Doctor and his Igor-like assistant kidnap the gorilla from the zoo and Unconscious Wrestling Girl from the hospital - these well thought-out acts of abduction are like taking candy from a baby.<br /><br />-- Daddy-Doctor speaks to God a lot, doesn't he?<br /><br />-- Sonny-boy calling Daddy-Doctor \"Papa\" on more than one occasion.<br /><br />-- An old lady screaming \"Aaargh!!! A dead man! A dead Man! A Dead Man!! A DEAD MAN!!!\".<br /><br />-- The plot periodically stops to wallow in scenes of women wrestling, only to go on again and do nothing with that concept. Sure, Daddy-Doctor replaces Sonny-Boy's gorilla-heart a second time with that of Unconscious Wrestling Girl, but do you think something spectacular happens after that? Like Ape-Turd-Man growing breasts or something, trying to rape men this time? Our leading living wrestling beauty (Norma Lazareno) doesn't even go into a climactic wrestling contest with Ape-Turd-Man near the movie's finale... But Ape-Turd-Man does start to show some motherly love near the end... almost (and I say \"almost\") in true KING KONG-style (i.e. the top of a building and people on the ground pointing and screaming).<br /><br />Okay, I think that's enough now. I chipped in more than my two cents here. Vomitron's Rational Rating for this sleazy piece of hilarious dreck: 2/10. Vomitron's rating From the Wrong Side of the Tracks: 8/10. Go see this film, people. It is well worth it!\n",
      "Negative\n",
      "Remnants of an ambushed Army unit hook up with a group of cowboys to fight their way through Indians on the warpath. Sounds like it could be an exciting western, but this one is dull, dull, dull. It moves like molasses, the action scenes are uninspired, the acting is pedestrian, the writing is flat, even the photography isn't very good. Eastwood, in a very early role, plays an ex-Confederate who doesn't like the idea of fighting on the same side as Yankees. That's about the only remotely interesting situation in the whole movie, but Eastwood wasn't experienced enough an actor to pull it off, and his character comes across as petulant rather than angry or embittered. A very ordinary western. Actually, a very less-than-ordinary western. Worth a look if you're a die-hard Eastwood fan and want to see him at the very beginning of his career. Otherwise, don't bother.\n",
      "Negative\n",
      "This movie has too many things going on. Another reviewer comments on the disjointed, episodic nature of the film as reflecting the director's memories - that's fine, if that is how it was written and performed. Instead, what we get is straight-forward narrative - some of the time - that jumps around, under and over, leaves us dangling in some instances, interrupts the flow with unnecessary digressions in other instances, and otherwise simply doesn't work. <br /><br />There are also some plot details that just don't work. For example, why drag a body onto a beach in an urban area in broad daylight, as opposed to night time? Why leave your flat sheet on the body? Why would an artist who knew the Joe character for a brief time decide to leave him \"everything\" (even if it wasn't much)? This sub-plot was poorly developed to make that point work. For that matter, why even have the man be an invalid or an artist other than to provide the money and the gratuitous nude posing scenes? He could just as easily have been a photographer, or a opera composer? For that matter, how does someone rate an apartment in an Opera House - particularly without some clear connection to the Opera? The coincidences are also both too obvious and to unclear and unexplained. Why would the guys take everything in the warehouse and \"disappear.\" If Tim was a 10 year old school mate in a town as small as Bangor, how could Joe lose track of him for 8 years, especially if they knew each other well enough that one would recommend the other for a job. <br /><br />Some of the other subplots (like the mother and her boyfriend(s) and the sister wanting to escape felt like padding. There's some good ideas that might have made a feature with full development or could have been interesting shorts. As completed, this movie made little sense and offers even less.\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "test_lengths = np.array([len(v) for v in test_df.Content.values])\n",
    "test_lengths[test_lengths.argmin()]\n",
    "print(test_lengths.argmin())\n",
    "print(np.argsort(test_lengths, 0)[:10])\n",
    "print(test_df.iloc[1056].Content)\n",
    "print(id_class[test_df.iloc[1056].Topic])\n",
    "print(test_df.iloc[467].Content)\n",
    "print(id_class[test_df.iloc[467].Topic])\n",
    "print(test_df.iloc[284].Content)\n",
    "print(id_class[test_df.iloc[284].Topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_token = True\n",
    "need_transpose = True\n",
    "model_layers = [\n",
    "    (lambda m: m.embedding, not on_token, not need_transpose), \n",
    "    (lambda m: m.conv1, not on_token, need_transpose), \n",
    "    (lambda m: m.conv2, not on_token, need_transpose), \n",
    "    (lambda m: m.conv3, on_token, need_transpose), \n",
    "    (lambda m: m.sentiment1, on_token, not need_transpose),\n",
    "    (lambda m: m.gcnn1, on_token, not need_transpose),\n",
    "    (lambda m: m.gcnn2, on_token, not need_transpose),\n",
    "    # (lambda m: m.fc0, on_token, need_transpose)\n",
    "    ]\n",
    "\n",
    "def create_layers_attributions(content, label, file_name='result_5', n_steps=50):\n",
    "    vis_data_records_ig = []\n",
    "    true_label = torch.tensor(label)\n",
    "    graph = test_dataset.content_to_graph(content, subsampling_equation_sigmoid)\n",
    "    graph.cumulative_token_indices = test_dataset.caluculate_batch_token_positions(torch.tensor([graph.num_tokens]), torch.tensor([graph.character_length]), graph.token_indices)\n",
    "    for i, (ml, on_t, need_transpose) in enumerate(model_layers):\n",
    "        print(f'>>>>>>>>>>>>>>>>>>>> {i} <<<<<<<<<<<<<<<<<<<<<<')\n",
    "        # if i!=6: continue\n",
    "        model = load_model(212, r'logs\\CNN-GNN_False_False_False', 22, n_steps=n_steps, step_of_test=i)\n",
    "        model = model.model\n",
    "        lig = LayerIntegratedGradients(model, ml(model))\n",
    "        attributions_ig, tokens, characters, pred, pred_ind, label, delta = interpret_sentence(lig, model, graph, true_label, vocab_dict_rev, 7, 1, visualize_on_tokens=on_t, n_steps=n_steps, return_delta=False)\n",
    "        \n",
    "        attributions_ig = attributions_ig.T if need_transpose else attributions_ig \n",
    "        # print(on_t, len(tokens), len(characters), {attributions_ig.shape})\n",
    "        if on_t:\n",
    "            add_attributions_to_visualizer(attributions_ig, tokens[:attributions_ig.shape[0]], pred[0,pred_ind], pred_ind, true_label.item(), delta, vis_data_records_ig)\n",
    "        else:\n",
    "            add_attributions_to_visualizer(attributions_ig, characters, pred[0,pred_ind], pred_ind, true_label.item(), delta, vis_data_records_ig)\n",
    "        # time.sleep(1)\n",
    "            \n",
    "    # for i in range(len(vis_data_records_ig)):\n",
    "    #     print(i, vis_data_records_ig[i].word_attributions.shape)\n",
    "        \n",
    "    _ = visualization.visualize_text(vis_data_records_ig)\n",
    "    html = _.data\n",
    "    with open(rf'FindBestModel\\6_ReduceAttentionToStopWords\\VisualizationFiles\\{file_name}.html', 'w') as f:\n",
    "        f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_layers_attributions(\"The Claude Lelouch's movie is a pretty good moment of cinema. One of the most touching films about family and loneliness, and surely the best interpretation of French actor Jean-Paul Belmondo.\", 1, 'result_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_layers_attributions(\"Read the book, forget the movie!\", 0, 'result_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_layers_attributions(\"This is a great movie. Too bad it is not available on home video.\", 1, 'result_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_layers_attributions(\"\"\"Beyond Rangoon is one of the most emotional and intense films ever made. Superbly directed by John Boorman, and intensly acted by Patricia Arquette, this film can easily be called one of the best films of the 90's. The story and vivid characters just grab the audience from the very opening, and never lets go. After seeing the film, the viewer will never be able to forget \"Beyond Rangoon\". The film made little money at the box office, and is little known, but should be high profile. Watching it, you can tell that it was meant to be seen by a large audience. It is a very important and moving film, and should be seen by everyone.<br /><br />\"\"\", 1, 'result_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_layers_attributions(\"\"\"Beyond Rangoon is one of the most emotional and intense films ever made. Superbly directed by John Boorman, intensly acted by Patricia Arquette, this film can easily be called one of the best films of the 90's. The story and vivid characters just grab the audience from the very opening, and never lets go. After seeing the film, the viewer will never be able to forget \"Beyond Rangoon\". The film made little money at the box office, is little known, but should be high profile. Watching it, you can tell that it was meant to be seen by a large audience. It is a very important and moving film, and should be seen by everyone.\"\"\", 1, 'result_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_graph = []\n",
    "for row in test_df.values[:1000]:\n",
    "    rows_graph.append((Batch.from_data_list([test_dataset.content_to_graph(row[1], subsampling_equation_sigmoid)]), torch.tensor(row[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_wrong_samples(base_path, version_id, target_data_list, num_embedding):\n",
    "    \n",
    "        version_path = join(base_path, f'version_{version_id}')\n",
    "        checkpoint_path = join(version_path, f'checkpoints')\n",
    "        onlyfiles  = [f for f in listdir(checkpoint_path) if (isfile(join(checkpoint_path, f)) and 'epoch' in f) ]\n",
    "        epoch_numbers = [int(re.search(r'\\d+', f).group()) for f in onlyfiles]\n",
    "        best_chpt_id, loss = get_best_chpt(join(version_path, 'metrics.csv'), epoch_numbers)\n",
    "        chpt_path = join(checkpoint_path, f'{onlyfiles[best_chpt_id]}')\n",
    "        \n",
    "        classifier_torch_model = CNN_for_Text_No_Positional_Encoding(num_embedding=num_embedding, hidden_dim=hidden_dim, embedding_dim=embedding_dim, pos_emb_size=4096, dropout=0.2, num_out_features=len(class_id), seed=seed, random_edges=4, lattice_edges=8, lattice_step=2, virtual_nodes=0, lattice_start_distance=2).cpu()\n",
    "        classfier_lightning_model = CnnGnnClassifierLightningModel.load_from_checkpoint(chpt_path, model=classifier_torch_model, num_classes=len(class_id)).eval().cpu()\n",
    "\n",
    "        wrong_indices = []\n",
    "        for i, (X, y) in enumerate(target_data_list):\n",
    "            with torch.no_grad():\n",
    "                y_pred = classfier_lightning_model(X)\n",
    "            if torch.argmax(y_pred).item() != y.item():\n",
    "                wrong_indices.append(i)\n",
    "        return wrong_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_indices = find_wrong_samples(r'logs\\CNN-GNN_False_False_False', 22, rows_graph, 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, row in enumerate(test_df.iloc[wrong_indices].values):\n",
    "#     print(row)\n",
    "#     create_layers_attributions(row[1], row[0], f'wrong_result_{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 2), dtype=object)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.iloc[wrong_indices].values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
