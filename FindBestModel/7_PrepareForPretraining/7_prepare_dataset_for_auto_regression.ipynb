{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch_scatter import scatter_max, scatter_mean\n",
    "import lightning as L\n",
    "from torch_geometric.data import Batch, Data\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pickle\n",
    "from torch import nn\n",
    "import time\n",
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 1024\n",
    "folder_path = r'Data\\SSL\\twcs\\twcs\\twcs.csv'\n",
    "# t_tokenizer = TweetTokenizer()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utilities.data_manager.CharacterandTokenLevelCustomDataset import CharacterandTokenLevelCustomDataset\n",
    "from utilities.data_manager.CharacterandTokenLevelDataLoader import CharacterandTokenLevelDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Batch, Data\n",
    "import random\n",
    "\n",
    "class CharacterandTokenLevelCustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, num_classes, token_dict, sentiment_dict, tokenizer, vocab, token_frequencies, sampling_equation, id_class, shuffle=True, batch_size=128) -> None:\n",
    "        super().__init__()\n",
    "        if len(X) % batch_size != 0:\n",
    "            self.shortage = ((len(X) // batch_size)+1)*batch_size - len(X)\n",
    "            empty_labels = [i%2 for i in range(self.shortage)]\n",
    "            empty_strings = [id_class[l] for l in empty_labels]\n",
    "            X = np.concatenate([X, empty_strings])\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "        self.X = X\n",
    "        self.vocab_size = 16384\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "        self.token_dict = token_dict\n",
    "        self.embedding_size = token_vocab_dict[next(iter(token_dict))].shape[0]\n",
    "        self.zero_emb = torch.zeros((self.embedding_size,), dtype=torch.float32)\n",
    "        self.sentiment_dict = sentiment_dict\n",
    "        self.token_frequencies = token_frequencies\n",
    "        self.max_token_count = 0\n",
    "        self.all_data = []\n",
    "        self.token_lengths = []\n",
    "        self.token_embeddign_ids = []\n",
    "        \n",
    "        self.sum_a = 0\n",
    "        \n",
    "        for doc in tqdm(self.X):\n",
    "            g_data = self.content_to_graph(doc, sampling_equation)\n",
    "            self.all_data.append(g_data)\n",
    "        \n",
    "        \n",
    "        self.num_sections = len(X) // batch_size\n",
    "        self.x_lengths = np.array([self.all_data[i].character_length for i in range(len(self.all_data))])\n",
    "        self.x_len_args = np.argsort(self.x_lengths)[::-1]\n",
    "        \n",
    "        self.section_ranges = np.linspace(0, len(self.x_len_args), self.num_sections+1)\n",
    "        self.section_ranges = [(int(self.section_ranges[i-1]), int(self.section_ranges[i])) for i in range(1, len(self.section_ranges))]\n",
    "        \n",
    "\n",
    "        self.position_j = 0\n",
    "        self.section_i = 0\n",
    "        self.epoch = 0\n",
    "        self.each_section_i = np.zeros((self.num_sections, ), dtype=int)\n",
    "        \n",
    "        self.sections, self.section_size = self.split_into_k_groups(self.x_len_args, self.x_lengths, self.num_sections)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        index = self.get_section_index()\n",
    "        ds = self.all_data[index]\n",
    "        cs_pos = ds.cumulative_pos\n",
    "        random_token_id= max(1, random.randint(ds.num_tokens//2 - 1 , ds.num_tokens-2))\n",
    "        new_data = Data(\n",
    "            x=ds.x[:cs_pos[random_token_id-1]+1],\n",
    "            character_length=cs_pos[random_token_id-1]+1,\n",
    "            num_tokens = random_token_id+1,\n",
    "            token_indices = ds.token_indices[:cs_pos[random_token_id-1]+1],\n",
    "            token_lengths = torch.cat([ds.token_lengths[:random_token_id], ds.token_lengths[-1].unsqueeze(0)], dim=0),\n",
    "            token_embeddings = torch.cat([ds.token_embeddings[:random_token_id],ds.token_embeddings[-1].unsqueeze(0)], dim=0),\n",
    "            token_sentiments = torch.cat([ds.token_sentiments[:random_token_id],ds.token_sentiments[-1].unsqueeze(0)], dim=0),\n",
    "            token_subsampling_probabilities = torch.cat([ds.token_subsampling_probabilities[:random_token_id],ds.token_subsampling_probabilities[-1].unsqueeze(0)], dim=0)\n",
    "            ) \n",
    "        new_data.x[-1] = 16300\n",
    "        label = ds.token_ids[random_token_id]\n",
    "        return new_data, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def get_section_index(self):\n",
    "        target_index = self.sections[self.section_i, self.position_j]\n",
    "        \n",
    "        self.position_j = (self.position_j + 1) % self.section_size\n",
    "        if self.position_j == 0:\n",
    "            self.section_i = (self.section_i + 1) % self.num_sections\n",
    "            if self.shuffle and self.section_i == 0:\n",
    "                self.sections, self.section_size = self.split_into_k_groups(self.x_len_args, self.x_lengths, self.num_sections)\n",
    "        return target_index\n",
    "\n",
    "    def reset_params(self):\n",
    "        self.section_i = 0\n",
    "        self.position_j = 0\n",
    "        self.each_section_i = np.zeros((self.num_sections, ), dtype=int)\n",
    "        \n",
    "    def split_into_k_groups(self, len_sorted_args, lengths:np.array, k):\n",
    "        if self.shuffle and self.epoch > 0:\n",
    "            randomize_sections = np.concatenate([np.random.choice(np.arange(r[0], r[1]), size=r[1]-r[0], replace=False) for r in self.section_ranges])\n",
    "            len_sorted_args = len_sorted_args[randomize_sections]\n",
    "        \n",
    "        nums = lengths[len_sorted_args]\n",
    "        groups_size = len(len_sorted_args) // k\n",
    "        \n",
    "        \n",
    "        groups = [[] for _ in range(k)]\n",
    "        group_sums = np.zeros(k, dtype=int)\n",
    "        group_sizes = np.zeros(k, dtype=int)\n",
    "        \n",
    "        for i, num in enumerate(nums):\n",
    "            candidate_indices = np.where(group_sizes<groups_size)[0]\n",
    "            min_group_idx = candidate_indices[np.argmin(group_sums[candidate_indices])]\n",
    "            groups[min_group_idx].append(len_sorted_args[i])\n",
    "            group_sums[min_group_idx] += num\n",
    "            group_sizes[min_group_idx] += 1\n",
    "        self.epoch += 1\n",
    "        \n",
    "        groups = np.array(groups)\n",
    "        group_sums_argsort = np.argsort(group_sums)[::-1]\n",
    "        groups = groups[group_sums_argsort]\n",
    "        return np.array(groups), groups_size\n",
    "        \n",
    "    def content_to_graph(self, doc, sampling_equation):\n",
    "        tokens = self.tokenizer(doc)\n",
    "        if len(tokens) == 0:\n",
    "            tokens = ['empty']\n",
    "        # while len(tokens) < 3:\n",
    "        #     tokens.append('[PAD]')\n",
    "            \n",
    "        token_lengths = [len(t) for t in tokens]\n",
    "        tokens.append('\\x01')\n",
    "        \n",
    "        token_lengths.append(len(tokens[-1])-1)\n",
    "        token_lengths = torch.from_numpy(np.array(token_lengths, dtype=np.longlong))+1\n",
    "        token_embs = [self.token_dict[t] if t in self.token_dict else self.zero_emb for t in tokens]\n",
    "        token_ids = torch.from_numpy(np.array([self.vocab[t] if t in self.vocab else 0 for t in tokens], dtype=np.longlong))\n",
    "        token_sentiments = [self.sentiment_dict[t] if t in self.sentiment_dict else (0.0, 0.0) for t in tokens]\n",
    "        token_embs = torch.from_numpy(np.array(token_embs, dtype=np.float32))\n",
    "        token_sentiments = torch.from_numpy(np.array(token_sentiments, dtype=np.float32))\n",
    "        doc = ' '.join(tokens)\n",
    "        characters = torch.from_numpy(np.array([ord(t) if ord(t)<16383 else 16383 for t in doc], dtype=np.longlong))\n",
    "        token_positions = torch.arange(len(token_lengths), dtype=torch.long)\n",
    "        token_indices = torch.repeat_interleave(token_positions, token_lengths)\n",
    "        token_subsampling_probabilities = sampling_equation(torch.from_numpy(np.array([self.token_frequencies[t] if t in self.token_frequencies else 1 for t in tokens])))\n",
    "        num_tokens = len(token_lengths)\n",
    "        if num_tokens > self.max_token_count:\n",
    "            self.max_token_count = num_tokens\n",
    "        \n",
    "        cumulative_pos = torch.cumsum(token_lengths, dim=0)    \n",
    "        \n",
    "        g_data = Data(x=characters,\n",
    "                        # token_positions=token_positions,\n",
    "                        token_ids = token_ids,\n",
    "                        character_length = len(characters),\n",
    "                        num_tokens = num_tokens,\n",
    "                        token_indices=token_indices,\n",
    "                        token_lengths=token_lengths,\n",
    "                        token_embeddings=token_embs,\n",
    "                        token_sentiments=token_sentiments,\n",
    "                        cumulative_pos=cumulative_pos,\n",
    "                        token_subsampling_probabilities=token_subsampling_probabilities)\n",
    "        return g_data\n",
    " \n",
    "    def caluculate_batch_token_positions(self, num_tokens, character_length, token_indices):\n",
    "        cumsum_vals = torch.cumsum(num_tokens, dim=0).roll(1)\n",
    "        cumsum_vals[0] = 0\n",
    "        additions = torch.repeat_interleave(cumsum_vals, character_length)\n",
    "        cumulative_token_indices = token_indices + additions\n",
    "        return cumulative_token_indices       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1085 tensor([0.7000, 0.6000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "id_vocab = {v:k for k,v in tokenizer.vocab.items()}\n",
    "all_vocab_indices = list(id_vocab.keys())\n",
    "\n",
    "with open(r'Data\\ReducedEmbeddings\\deberta_larg_reduced_embeddings_128.npy', 'rb') as f:\n",
    "    embeddings = np.load(f)\n",
    "embeddings = torch.from_numpy(embeddings)\n",
    "all_vocab_str = []\n",
    "for i in range(len(id_vocab)):\n",
    "    all_vocab_str.append(id_vocab[i])\n",
    "token_vocab_dict = dict(zip(all_vocab_str, embeddings))\n",
    "\n",
    "with open(r'Data\\ReducedEmbeddings\\polarity_debertav3_tokens_gpt_mini_emb.npy', 'rb') as f:\n",
    "    polarities_subjectivities= np.load(f)\n",
    "polarities_subjectivities = torch.from_numpy(polarities_subjectivities)\n",
    "polarity_vocab_dict = dict(zip(all_vocab_str, polarities_subjectivities))\n",
    "polarity_vocab_dict['<n>'] = torch.tensor([0.0, 0.0])\n",
    "len(token_vocab_dict)\n",
    "polarities_subjectivities.shape\n",
    "for i in range(len(all_vocab_str)):\n",
    "    if 'nice' in all_vocab_str[i]:\n",
    "        print(i, polarities_subjectivities[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%time\n",
    "# df = pd.read_csv(folder_path)[['tweet_id', 'text']]\n",
    "# vocabs_lists = list(token_vocab_dict.keys())\n",
    "# term_frequencies = {t:1 for t in vocabs_lists}\n",
    "# temp_term_frequencies = {}\n",
    "\n",
    "# for doc in df.text.values:\n",
    "#     tokens_list = tokenizer.tokenize(doc)\n",
    "#     new_tokens = {t.strip('▁').lower() for t in tokens_list}\n",
    "#     for t in new_tokens:\n",
    "#         if t not in temp_term_frequencies:\n",
    "#             temp_term_frequencies[t] = 0\n",
    "#         temp_term_frequencies[t] += 1\n",
    "        \n",
    "# for k, v in term_frequencies.items():\n",
    "#     stripped_token = k.strip('▁').lower()\n",
    "#     term_frequencies[k] = temp_term_frequencies[stripped_token] if stripped_token in temp_term_frequencies else 1\n",
    "\n",
    "# with open(r'Data\\term_frequencies\\term_frequencies.pkl', 'wb') as fp:\n",
    "#     pickle.dump(term_frequencies, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'Data\\term_frequencies\\term_frequencies.pkl', 'rb') as fp:\n",
    "    term_frequencies = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.00001\n",
    "total_token_count = np.array(list(term_frequencies.values())).sum()\n",
    "one_tensor = torch.tensor(1)\n",
    "def subsampling_equation_linear(x: torch.Tensor):\n",
    "    f_x = x/total_token_count\n",
    "    x = torch.min(one_tensor, torch.sqrt_(threshold/f_x))\n",
    "    return x\n",
    "\n",
    "def subsampling_equation_sigmoid(x: torch.Tensor):\n",
    "    f_x = x/total_token_count\n",
    "    x = 1-0.95*F.sigmoid(0.05*((f_x/threshold)-90))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "tensor([0.9894])\n"
     ]
    }
   ],
   "source": [
    "the_word = 'bat'\n",
    "print(subsampling_equation_linear(torch.tensor([term_frequencies[the_word]])))\n",
    "print(subsampling_equation_sigmoid(torch.tensor([term_frequencies[the_word]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.model_layers.GCNN import GCNN\n",
    "from utilities.model_layers.GenGraph2 import GenGraph\n",
    "from utilities.model_layers.SentimentInjection import SentimentInjection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_newlines = len(pd.read_csv(folder_path, usecols=['tweet_id']))\n",
    "np.random.seed(42)\n",
    "all_ids = np.random.choice(num_newlines, size=num_newlines, replace=False)\n",
    "columns_index = pd.read_csv(folder_path,nrows=1).columns.get_indexer(['tweet_id', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_range(lower_b: 0, upper_b: 0.05):\n",
    "    load_ids = set(all_ids[int(lower_b*num_newlines): int(upper_b*num_newlines)]+1)\n",
    "    df = pd.read_csv(folder_path, skiprows=lambda x: x not in load_ids, header=None, usecols=columns_index)\n",
    "    df.columns = ['tweet_id', 'Content']\n",
    "    df.dropna(inplace=True)\n",
    "    train_df, test_df = train_test_split(df, test_size=0.1, shuffle=True, random_state=42)\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    \n",
    "    train_dataset = CharacterandTokenLevelCustomDataset(train_df.Content.values, len(id_vocab), token_vocab_dict, polarity_vocab_dict, tokenizer.tokenize, tokenizer.vocab, token_frequencies=term_frequencies, sampling_equation=subsampling_equation_linear, id_class=id_vocab, batch_size=batch_size)\n",
    "    test_dataset = CharacterandTokenLevelCustomDataset(test_df.Content.values, len(id_vocab), token_vocab_dict, polarity_vocab_dict, tokenizer.tokenize, tokenizer.vocab, token_frequencies=term_frequencies, sampling_equation=subsampling_equation_linear, id_class=id_vocab, batch_size=batch_size)\n",
    "    # max_token_count = max(train_dataset.max_token_count, test_dataset.max_token_count)\n",
    "    train_dataloader = CharacterandTokenLevelDataLoader(train_dataset, batch_size=batch_size, drop_last=False, shuffle=True)\n",
    "    test_dataloader = CharacterandTokenLevelDataLoader(test_dataset, batch_size=batch_size, drop_last=False, shuffle=False)\n",
    "    return train_dataset, test_dataset, train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset, test_dataset, train_dataloader, test_dataloader = load_data_range(0, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5_restructure_for_contrastive_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch_geometric.data import Batch, Data\n",
    "\n",
    "class GenGraph(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, virtual_nodes, lattice_step, lattice_pattern=None, head=4, *args, **kwargs):\n",
    "        super(GenGraph, self).__init__(*args, **kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head = head\n",
    "        self.virtual_nodes = virtual_nodes\n",
    "        self.lattice_step = lattice_step\n",
    "        self.lp = lattice_pattern if lattice_pattern is None else torch.tensor(lattice_pattern)\n",
    "        self.virtual_node_embeddings = nn.Embedding(self.virtual_nodes, hidden_dim)\n",
    "        \n",
    "    def gen_graph(self, x, token_subsampling_probabilities, total_token_counts, token_counts, random_edges, lattice_edges, lattice_start_distance=2):\n",
    "        random_links, lattice_links, tc_range = self.calculate_graph(x, total_token_counts, token_counts, random_edges, lattice_edges, lattice_start_distance)\n",
    "        v_n_e_counts = total_token_counts*self.virtual_nodes\n",
    "        base_numel = random_links.numel() + lattice_links.numel()*2\n",
    "        edge_indices = torch.empty((2, base_numel + v_n_e_counts*2), dtype=torch.int64, device=x.device)\n",
    "        self.fill_lattice_and_random_edges(edge_indices, random_links, lattice_links, tc_range)\n",
    "            \n",
    "        if self.virtual_nodes > 0:\n",
    "            virtual_nodes_range = torch.arange(self.virtual_nodes, device=x.device).view(1, -1)\n",
    "            virtual_nodes_ids = torch.repeat_interleave(virtual_nodes_range, len(token_counts), dim=0)\n",
    "            v_n_idx = (virtual_nodes_ids + torch.arange(0, len(token_counts)*self.virtual_nodes, self.virtual_nodes, device=x.device).view(-1, 1) + total_token_counts )\n",
    "            virtual_edge_ids = torch.repeat_interleave(v_n_idx.view(-1), token_counts.view(-1, 1).expand(len(token_counts), self.virtual_nodes).reshape(-1), dim=0).view(1, -1)\n",
    "            \n",
    "            embs = self.virtual_node_embeddings(virtual_nodes_ids.T).view(-1, self.hidden_dim)\n",
    "            x_extended = torch.cat([x, embs], dim=0)\n",
    "            x_index = torch.arange(total_token_counts, device=x.device).repeat(self.virtual_nodes).view(1, -1)\n",
    "            edge_indices[:, base_numel:base_numel+v_n_e_counts] = torch.cat([x_index, virtual_edge_ids], dim=0)\n",
    "            edge_indices[:, base_numel+v_n_e_counts:] = torch.cat([virtual_edge_ids, x_index], dim=0)\n",
    "            x = x_extended\n",
    "        \n",
    "        edge_indices = self.subsample_edges(edge_indices, token_subsampling_probabilities)\n",
    "        return Batch.from_data_list([Data(x=x, edge_index=edge_indices)])\n",
    "        \n",
    "    def re_gen_graph(self, x, edge_indices, token_subsampling_probabilities, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance=2):\n",
    "        random_links, lattice_links, tc_range = self.calculate_graph(x, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance)\n",
    "        base_numel = random_links.numel() + lattice_links.numel()*2\n",
    "        \n",
    "        self.fill_lattice_and_random_edges(edge_indices, random_links, lattice_links, tc_range)\n",
    "        # for i in range(base.shape[1]):\n",
    "        #     edge_indices[:, i*base.shape[0]:(i+1)*base.shape[0]] = torch.cat([tc_range, base[:,i].view(1,-1)], dim=0)\n",
    "        edge_indices = self.subsample_edges(edge_indices, token_subsampling_probabilities)\n",
    "        return Batch.from_data_list([Data(x=x, edge_index=edge_indices)])\n",
    "    \n",
    "    def replace_unimportant_edges(self, edge_weights, x, edge_indices, token_subsampling_probabilities, total_token_counts, token_counts, random_edges, lattice_edges, p_keep=1, lattice_start_distance=2):\n",
    "        v_n_e_counts = total_token_counts*self.virtual_nodes\n",
    "        # if v_n_e_counts>0:\n",
    "        #     important_indices = torch.topk(edge_weights[:-2*v_n_e_counts].view(-1, total_token_coutns), p_keep, dim=0).indices\n",
    "        # else:\n",
    "        #     important_indices = torch.topk(edge_weights.view(-1, total_token_coutns), p_keep, dim=0).indices\n",
    "        # important_indices = torch.topk(edge_weights[:-1*total_token_coutns].view(-1, total_token_coutns), 1, dim=0).indices.squeeze()\n",
    "        important_indices = torch.topk(edge_weights.squeeze(), p_keep*total_token_counts, dim=0).indices\n",
    "\n",
    "        # important_indices = torch.arange(total_token_counts, dtype=torch.int64, device=x.device)\n",
    "        # important_indices = important_indices.view(-1)\n",
    "        random_links, lattice_links, tc_range = self.calculate_graph(x, total_token_counts, token_counts, random_edges, lattice_edges, lattice_start_distance)\n",
    "        base_numel = random_links.numel() + lattice_links.numel()*2\n",
    "        \n",
    "        new_edge_index = torch.empty((2, base_numel + important_indices.shape[0] + 2*v_n_e_counts), dtype=torch.int64, device=x.device)\n",
    "        self.fill_lattice_and_random_edges(new_edge_index, random_links, lattice_links, tc_range)\n",
    "        new_edge_index[:, base_numel:base_numel+important_indices.shape[0]] = edge_indices[:, important_indices]\n",
    "\n",
    "        if(self.virtual_nodes>0):\n",
    "            new_edge_index[:, -2*v_n_e_counts:] = edge_indices[:, -2*v_n_e_counts:]\n",
    "            \n",
    "        # for i in range(base.shape[1]):\n",
    "        #     new_edge_index[:, i*base.shape[0]:(i+1)*base.shape[0]] = torch.cat([tc_range, base[:,i].view(1,-1)], dim=0)\n",
    "        new_edge_index = self.subsample_edges(new_edge_index, token_subsampling_probabilities)\n",
    "        \n",
    "        return Batch.from_data_list([Data(x=x, edge_index=new_edge_index)])\n",
    "    \n",
    "        \n",
    "    def calculate_graph(self, x, total_token_counts, token_counts, random_edges, lattice_edges, lattice_start_distance):\n",
    "\n",
    "        tc_extended = torch.repeat_interleave(token_counts, token_counts, dim=0).view(-1,1)\n",
    "        tc_lower_bound = torch.empty((len(token_counts)+1), dtype=torch.long, device=x.device) #torch.cuda.IntTensor(len(token_counts)+1) #\n",
    "        tc_lower_bound[0] = 0\n",
    "        tc_lower_bound[1:] = torch.cumsum(token_counts, dim=0)\n",
    "        tc_lower_bound_extended = torch.repeat_interleave(tc_lower_bound[:-1], token_counts, dim=0).view(-1,1)\n",
    "        tc_range = torch.arange(tc_lower_bound[-1], device=x.device).view(-1,1)\n",
    "        # torch.arange(tc_lower_bound[-1], dtype=torch.int32, device=x.device).view(-1,1)\n",
    "        \n",
    "        random_ints = torch.randint(0, 2*total_token_counts, (total_token_counts, random_edges), device=x.device) # torch.cuda.IntTensor(len(token_lengths), random_edges).random_()\n",
    "        lattice = self.lp.to(x.device) if self.lp is not None else torch.arange(lattice_start_distance, max(lattice_start_distance, self.lattice_step*lattice_edges+1), self.lattice_step, device=x.device).view(1, -1)\n",
    "        \n",
    "\n",
    "        # exponentials = torch.pow(2, torch.arange(1, self.exp_edges+1, device=x.device)).view(1, -1)\n",
    "        tc_local_range = tc_range - tc_lower_bound_extended\n",
    "        random_links = (((random_ints % (tc_extended - 1))+1 + tc_local_range) % tc_extended)+tc_lower_bound_extended\n",
    "        lattice_links = ((lattice + tc_local_range) % tc_extended)+tc_lower_bound_extended\n",
    "        \n",
    "        # base = torch.cat([base1, base2], dim=1)\n",
    "        tc_range = tc_range.view(1,-1)\n",
    "        return random_links, lattice_links, tc_range\n",
    "    \n",
    "    def fill_lattice_and_random_edges(self, edge_indices, random_links, lattice_links, tc_range):\n",
    "        for i in range(0, lattice_links.shape[1]*2, 2):\n",
    "            edge_indices[:, i*lattice_links.shape[0]:(i+1)*lattice_links.shape[0]] = torch.cat([lattice_links[:,i//2].view(1,-1), tc_range], dim=0)\n",
    "            edge_indices[:, (i+1)*lattice_links.shape[0]:(i+2)*lattice_links.shape[0]] = edge_indices[:, i*lattice_links.shape[0]:(i+1)*lattice_links.shape[0]][[1, 0]]\n",
    "            \n",
    "        for i in range(random_links.shape[1]):\n",
    "            j = i + lattice_links.shape[1]*2\n",
    "            edge_indices[:, j*random_links.shape[0]:(j+1)*random_links.shape[0]] = torch.cat([random_links[:,i].view(1,-1), tc_range], dim=0)\n",
    "            \n",
    "    def subsample_edges(self, edge_indices, token_subsampling_probabilities, keep_ratio=0.5):\n",
    "        top_k_indices = torch.topk(torch.sum(token_subsampling_probabilities[edge_indices], dim=0), int(keep_ratio*edge_indices.shape[1]/self.head), dim=0).indices\n",
    "        edge_indices = edge_indices[:, top_k_indices]\n",
    "        # print(f'edge_indices.shape: {edge_indices.shape}:\\n {edge_indices}')\n",
    "        # return edge_indices.permute(2, 0, 1)\n",
    "        return edge_indices.reshape(2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Parameter, Module\n",
    "from torch_scatter.scatter import scatter_sum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadGraphAttention(Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features, heads=1, *args, **kwargs) -> None:\n",
    "        super(MultiHeadGraphAttention, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.num_heads = heads\n",
    "        \n",
    "        self.head_in_features = in_features//heads\n",
    "        self.head_out_features = out_features//heads\n",
    "        \n",
    "        self.query_w, self.key_w, self.agg_value_w, self.update_value_w = self.create_weights()\n",
    "        \n",
    "        \n",
    "    def forward(self, x, edge_index, return_attention_weights=False):\n",
    "        if(len(edge_index.shape)==2):\n",
    "            edge_index = edge_index.reshape(2, edge_index.shape[1]//self.num_heads, self.num_heads).permute(2,0,1)\n",
    "        elif len(edge_index.shape==3) and edge_index.shape[0] != self.num_heads:\n",
    "            raise RuntimeError(\"The first dimension of edge_index should be equal to the number of heads.\")\n",
    "        x1_h = x.reshape(self.num_heads, x.shape[0], self.in_features//self.num_heads)\n",
    "        \n",
    "        batch_indices = torch.arange(self.num_heads).unsqueeze(1).unsqueeze(2).repeat(1, edge_index.shape[1], edge_index.shape[2]) \n",
    "        source_target_features = x1_h[batch_indices, edge_index]\n",
    "        \n",
    "        query = torch.sum(torch.einsum('ijkl,ilm->ijkm', source_target_features, self.query_w), dim=1)\n",
    "        key = torch.sum(torch.einsum('ijkl,ilm->ijkm', source_target_features, self.key_w), dim=1)\n",
    "        a = torch.einsum('ijk,ijk->ij', query, key)\n",
    "        attention = torch.softmax(a, dim=1) / torch.sqrt(torch.tensor(self.head_out_features))\n",
    "        \n",
    "        value_update = torch.einsum('ikl,ilm->ikm', x1_h, self.update_value_w)\n",
    "        value_aggregate = torch.sum(torch.einsum('ijkl,ilm->ijkm', source_target_features, self.agg_value_w), dim=1)\n",
    "        value_aggregate = torch.einsum('ij, ijk->ijk', attention, value_aggregate)\n",
    "        \n",
    "        agg_sum = torch.zeros_like(value_update, device=value_update.device)\n",
    "        scatter_sum(value_aggregate, index=edge_index.permute(0, 2, 1)[:, :, :1], dim=1, out=agg_sum)\n",
    "        scatter_sum(value_aggregate, index=edge_index.permute(0, 2, 1)[:, :, 1:], dim=1, out=agg_sum)\n",
    "        out = value_update + agg_sum\n",
    "        \n",
    "        return out.permute(1, 0, 2).reshape(x.shape[0], -1), attention.view(-1)\n",
    "        \n",
    "    def create_weights(self):\n",
    "        query_w = torch.randn((self.num_heads, self.head_in_features, self.head_out_features))\n",
    "        key_w = torch.randn((self.num_heads, self.head_in_features, self.head_out_features))\n",
    "        agg_value_w = torch.randn((self.num_heads, self.head_in_features, self.head_out_features))\n",
    "        update_value_w = torch.randn((self.num_heads, self.head_in_features, self.head_out_features))\n",
    "        return Parameter(query_w), Parameter(key_w), Parameter(agg_value_w), Parameter(update_value_w)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATv2Conv, SimpleConv\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Normalization on each feature of all tokens, for this we used batch norm class but with tokens at batch dimention\n",
    "class GCNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, heads, *args, **kwargs):\n",
    "        super(GCNN, self).__init__(*args, **kwargs)\n",
    "        # self.gnn = GATv2Conv(hidden_dim, hidden_dim//8, heads=4, add_self_loops=False)\n",
    "        self.gnn = MultiHeadGraphAttention(hidden_dim, hidden_dim//2, heads = heads)\n",
    "        self.conv = nn.Conv1d(hidden_dim, hidden_dim//2, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(hidden_dim//2, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim//2)\n",
    "        \n",
    "    def forward(self, x, edge_data, return_attention_weights = False):\n",
    "        x1, edge_weights = self.gnn(x, edge_data, return_attention_weights=return_attention_weights) \n",
    "        x2 = F.relu(self.conv(x.T).T)\n",
    "        x1 = F.leaky_relu_(self.bn1(x1))\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        return x, edge_weights, edge_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGNetTokenLevelEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim=64, hidden_dim=64, dropout=0.2, seed=-1, random_edges=4, lattice_edges=10, lattice_step=2, lattice_start_distance=2, inject_embedding_dim=64, step_of_test = 0, head=1, gnn_layers=2, *args, **kwargs):\n",
    "        super(CGNetTokenLevelEmbedding, self).__init__(*args, **kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.base_random_edges = random_edges\n",
    "        self.base_lattice_edges = lattice_edges\n",
    "        self.lattice_start_distance = lattice_start_distance\n",
    "        self.step_of_test = step_of_test\n",
    "        self.gnn_layers = gnn_layers\n",
    "        if seed>-1:\n",
    "            torch.manual_seed(seed)\n",
    "        self.embedding = nn.Embedding(16384, embedding_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.conv1 = nn.Conv1d(embedding_dim, embedding_dim, kernel_size=5, padding=2)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(embedding_dim, embedding_dim, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv1d(2*embedding_dim + 2, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.sentiment1  = SentimentInjection(hidden_dim, embedding_dim)\n",
    "        # self.sentiment2  = SentimentInjection(hidden_dim, embedding_dim)\n",
    "        self.p_layers = nn.ModuleList([nn.Linear(hidden_dim, head) for i in range(gnn_layers)])\n",
    "        self.gat_layers = nn.ModuleList([GCNN(hidden_dim, head) for i in range(gnn_layers)])\n",
    "        \n",
    "        # self.p_layer_1 = nn.Linear(hidden_dim, head)\n",
    "        # self.gcnn1 = GCNN(hidden_dim, head)\n",
    "        # self.p_layer_2 = nn.Linear(hidden_dim, head)\n",
    "        # self.gcnn2 = GCNN(hidden_dim+inject_embedding_dim, head)\n",
    "        \n",
    "        self.graph_generator = GenGraph(hidden_dim, 0, lattice_step, head=head)\n",
    "        self.token_embedding_fc = nn.Linear(hidden_dim + inject_embedding_dim , hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim , hidden_dim * 4)\n",
    "    \n",
    "    def forward(self, x, edge_index, token_subsampling_probabilities, token_indices, token_sentiments, token_lengths, num_tokens, character_length, token_embeddings):\n",
    "        cumulative_token_indices = self.caluculate_batch_token_positions(num_tokens, character_length, token_indices)\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.T\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        x1 = scatter_max(x, cumulative_token_indices, dim=1)[0]\n",
    "        x2 = scatter_mean(x, cumulative_token_indices, dim=1)\n",
    "        x = torch.cat([x1, x2, token_sentiments.T], dim=0)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.sentiment1(x.T, token_sentiments)\n",
    "        \n",
    "        p = self.p_layers[0](x.T)\n",
    "        p = F.softmax(p, dim=1)\n",
    "        ids = torch.argmax(p, dim=1, keepdim=True)\n",
    "        p = torch.zeros_like(p).scatter_(1, ids, torch.ones_like(p)) * token_subsampling_probabilities.unsqueeze(1)\n",
    "        graph = self.graph_generator.gen_graph(x, p, len(token_lengths), num_tokens, self.base_random_edges, self.base_lattice_edges, lattice_start_distance=self.lattice_start_distance)\n",
    "    \n",
    "        x, edge_weights, edge_index = self.gat_layers[0](graph.x.T, graph.edge_index, return_attention_weights = True)\n",
    "        x = F.relu(self.token_embedding_fc(torch.cat([x, token_embeddings], dim=1)))\n",
    "        \n",
    "        for i in range(1, self.gnn_layers):\n",
    "            p = self.p_layers[i](x)\n",
    "            p = F.softmax(p, dim=1)\n",
    "            ids = torch.argmax(p, dim=1, keepdim=True)\n",
    "            p = torch.zeros_like(p).scatter_(1, ids, torch.ones_like(p)) * token_subsampling_probabilities.unsqueeze(1)\n",
    "            graph = self.graph_generator.replace_unimportant_edges(edge_weights, x, edge_index, p, len(token_lengths), num_tokens, self.base_random_edges-1, self.base_lattice_edges-1, p_keep=2, lattice_start_distance=self.lattice_start_distance+1)\n",
    "            x, edge_weights, edge_index = self.gat_layers[i](graph.x, graph.edge_index)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def caluculate_batch_token_positions(self, num_tokens, character_length, token_indices):\n",
    "        cumsum_vals = torch.cumsum(num_tokens, dim=0).roll(1)\n",
    "        cumsum_vals[0] = 0\n",
    "        additions = torch.repeat_interleave(cumsum_vals, character_length)\n",
    "        cumulative_token_indices = token_indices + additions\n",
    "        return cumulative_token_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CGNetTokenLevelEmbedding(nn.Module):\n",
    "#     def __init__(self, embedding_dim=64, hidden_dim=64, dropout=0.2, seed=-1, random_edges=4, lattice_edges=10, lattice_step=2, lattice_start_distance=2, inject_embedding_dim=64, step_of_test = 0, head=1, *args, **kwargs):\n",
    "#         super(CGNetTokenLevelEmbedding, self).__init__(*args, **kwargs)\n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.base_random_edges = random_edges\n",
    "#         self.base_lattice_edges = lattice_edges\n",
    "#         self.lattice_start_distance = lattice_start_distance\n",
    "#         self.step_of_test = step_of_test\n",
    "#         if seed>-1:\n",
    "#             torch.manual_seed(seed)\n",
    "#         self.embedding = nn.Embedding(16384, embedding_dim)\n",
    "#         self.dropout = nn.Dropout(p=dropout)\n",
    "#         self.conv1 = nn.Conv1d(embedding_dim, embedding_dim, kernel_size=5, padding=2)\n",
    "#         self.pool1 = nn.MaxPool1d(2)\n",
    "#         self.conv2 = nn.Conv1d(embedding_dim, embedding_dim, kernel_size=5, padding=2)\n",
    "#         self.conv3 = nn.Conv1d(2*embedding_dim + 2, hidden_dim, kernel_size=3, padding=1)\n",
    "#         self.conv4 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "#         self.sentiment1  = SentimentInjection(hidden_dim, embedding_dim)\n",
    "#         # self.sentiment2  = SentimentInjection(hidden_dim, embedding_dim)\n",
    "#         self.p_layer_1 = nn.Linear(hidden_dim, head)\n",
    "#         self.gcnn1 = GCNN(hidden_dim, head)\n",
    "#         self.p_layer_2 = nn.Linear(hidden_dim, head)\n",
    "#         self.gcnn2 = GCNN(hidden_dim+inject_embedding_dim, head)\n",
    "#         self.graph_generator = GenGraph(hidden_dim, 0, lattice_step, head=head)\n",
    "#         self.fc0 = nn.Linear(hidden_dim , hidden_dim+inject_embedding_dim)\n",
    "#         self.fc1 = nn.Linear(hidden_dim+inject_embedding_dim , hidden_dim * 4)\n",
    "    \n",
    "#     def forward(self, x, edge_index, token_subsampling_probabilities, token_indices, token_sentiments, token_lengths, num_tokens, character_length, token_embeddings):\n",
    "#         cumulative_token_indices = self.caluculate_batch_token_positions(num_tokens, character_length, token_indices)\n",
    "#         x = self.embedding(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = x.T\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x1 = scatter_max(x, cumulative_token_indices, dim=1)[0]\n",
    "#         x2 = scatter_mean(x, cumulative_token_indices, dim=1)\n",
    "#         x = torch.cat([x1, x2, token_sentiments.T], dim=0)\n",
    "#         x = F.relu(self.conv3(x))\n",
    "#         x = self.sentiment1(x.T, token_sentiments)\n",
    "#         p = self.p_layer_1(x.T)\n",
    "#         p = F.softmax(p, dim=1)\n",
    "#         ids = torch.argmax(p, dim=1, keepdim=True)\n",
    "#         p = torch.zeros_like(p).scatter_(1, ids, torch.ones_like(p)) * token_subsampling_probabilities.unsqueeze(1)\n",
    "#         graph = self.graph_generator.gen_graph(x, p, len(token_lengths), num_tokens, self.base_random_edges, self.base_lattice_edges, lattice_start_distance=self.lattice_start_distance)\n",
    "        \n",
    "#         x, edge_weights, edge_index = self.gcnn1(graph.x.T, graph.edge_index, return_attention_weights = True)\n",
    "\n",
    "        \n",
    "#         p = self.p_layer_2(x)\n",
    "#         p = F.softmax(p, dim=1)\n",
    "#         ids = torch.argmax(p, dim=1, keepdim=True)\n",
    "#         p = torch.zeros_like(p).scatter_(1, ids, torch.ones_like(p)) * token_subsampling_probabilities.unsqueeze(1)\n",
    "#         graph = self.graph_generator.replace_unimportant_edges(edge_weights, x, edge_index, p, len(token_lengths), num_tokens, self.base_random_edges-1, self.base_lattice_edges-1, p_keep=2, lattice_start_distance=self.lattice_start_distance+1)\n",
    "#         # x = self.sentiment2(x, token_sentiments)\n",
    "#         xa = graph.x[:token_embeddings.shape[0]]\n",
    "#         xb = token_embeddings\n",
    "#         x = torch.cat([xa, xb], dim=1)\n",
    "#         x1 = F.relu(self.fc0(graph.x[token_embeddings.shape[0]:]))\n",
    "#         x = torch.cat([x, x1], dim=0)\n",
    "#         x, edge_weights, edge_index = self.gcnn2(x, graph.edge_index)\n",
    "#         x = self.fc1(x)\n",
    "#         return x\n",
    "    \n",
    "#     def caluculate_batch_token_positions(self, num_tokens, character_length, token_indices):\n",
    "#         cumsum_vals = torch.cumsum(num_tokens, dim=0).roll(1)\n",
    "#         cumsum_vals[0] = 0\n",
    "#         additions = torch.repeat_interleave(cumsum_vals, character_length)\n",
    "#         cumulative_token_indices = token_indices + additions\n",
    "#         return cumulative_token_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGNetEmbedding(nn.Module):\n",
    "    def __init__(self, token_embedding_model, hidden_dim=64, output_dim=512, *args, **kwargs):\n",
    "        super(CGNetEmbedding, self).__init__(*args, **kwargs)\n",
    "        self.token_embedding_model = token_embedding_model\n",
    "        self.fc2 = nn.Linear(hidden_dim * 8 , output_dim)\n",
    "    \n",
    "    def forward(self, x, edge_index, token_subsampling_probabilities, token_indices, token_sentiments, token_lengths, num_tokens, character_length, token_embeddings):\n",
    "        x = self.token_embedding_model(x, edge_index, token_subsampling_probabilities, token_indices, token_sentiments, token_lengths, num_tokens, character_length, token_embeddings)\n",
    "        x = F.elu_(x)\n",
    "        doc_token_index = torch.repeat_interleave(torch.arange(len(num_tokens), device=x.device), num_tokens)\n",
    "        x1 = scatter_max(x[:len(token_lengths)], doc_token_index, dim=0)[0]\n",
    "        x2 = scatter_mean(x[:len(token_lengths)], doc_token_index, dim=0)\n",
    "        x_for_cat = [x1, x2]\n",
    "        x = torch.cat(x_for_cat, dim=1)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_for_Text_No_Positional_Encoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_model: CGNetEmbedding, hidden_dim=64, dropout=0.3, num_out_features=len(id_vocab), *args, **kwargs) -> None:\n",
    "        super(CNN_for_Text_No_Positional_Encoding, self).__init__(*args, **kwargs)\n",
    "        self.embedding_model = embedding_model\n",
    "        self.num_out_features= num_out_features\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim, self.num_out_features)\n",
    "    \n",
    "    def forward(self, x, edge_index, token_subsampling_probabilities, token_indices, token_sentiments, token_lengths, num_tokens, character_length, token_embeddings):\n",
    "        x = self.embedding_model(x, edge_index, token_subsampling_probabilities, token_indices, token_sentiments, token_lengths=token_lengths, num_tokens=num_tokens, character_length=character_length, token_embeddings=token_embeddings)\n",
    "        # x = F.elu_(x)\n",
    "        # x = self.dropout(x)\n",
    "        return self.fc_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for p1 in [False, True]:\n",
    "# #     for p2 in [False, True]:\n",
    "# #         for p3 in [False, True]:\n",
    "# # print(f'\\n{p1}, {p2}, {p3}: \\n')\n",
    "# token_embedding_model = CGNetTokenLevelEmbedding(embedding_dim=64, hidden_dim=128, dropout=0.2,  seed=911, random_edges=4, lattice_edges=4, lattice_step=2, inject_embedding_dim=128, lattice_start_distance=2, head=4, gnn_layers=6)\n",
    "# embedding_model = CGNetEmbedding(token_embedding_model, hidden_dim=128, output_dim=256).eval()\n",
    "\n",
    "# # embedding_model = CGNetEmbedding(embedding_dim=64, hidden_dim=128, dropout=0.2,  seed=911, random_edges=4, lattice_edges=4, lattice_step=2, lattice_start_distance=2)\n",
    "# classifier_torch_model = CNN_for_Text_No_Positional_Encoding(embedding_model, hidden_dim=256, dropout=0.2, num_out_features=len(id_vocab)).eval()\n",
    "# flopt_counter = FlopCounterMode(classifier_torch_model)\n",
    "# with flopt_counter:\n",
    "#     # embedding_model(X.x, torch.zeros((2, 0)), X.token_subsampling_probabilities, X.token_indices, X.token_sentiments, token_lengths=X.token_lengths, num_tokens=X.num_tokens, character_length=X.character_length, token_embeddings=X.token_embeddings)\n",
    "#     sample_output = classifier_torch_model(X.x, torch.zeros((2, 0)), X.token_subsampling_probabilities, X.token_indices, X.token_sentiments, X.token_lengths, X.num_tokens, X.character_length, X.token_embeddings)\n",
    "\n",
    "# # graph: torch.Size([128, 22260]), torch.Size([2, 133560])\n",
    "# # edge_weights: torch.Size([133560, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_y = F.one_hot(y, sample_output.shape[1]).float()\n",
    "# print(f\"y_out = {sample_output.shape}\")\n",
    "# print(f\"new_y = {new_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(sample_output[0].detach().numpy())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_embedding_model = CGNetTokenLevelEmbedding(embedding_dim=embedding_dim, hidden_dim=hidden_dim, inject_embedding_dim=128, dropout=0.2,  seed=seed, random_edges=6, lattice_edges=10, lattice_step=2, lattice_start_distance=2).to(device)\n",
    "# embedding_model = CGNetEmbedding(token_embedding_model, hidden_dim=hidden_dim, output_dim=output_size).to(device)\n",
    "# classifier_torch_model = CNN_for_Text_No_Positional_Encoding(embedding_model, hidden_dim=output_size, dropout=0.2, num_out_features=len(id_vocab)).to(device)\n",
    "# #model_manager.lightning_model.model\n",
    "# loaded_lightning_model = classifier_torch_model.load_state_dict(torch.load(r'logs\\Self_Supervised\\version_0\\checkpoints\\epoch=214-step=5375.pth', weights_only=True, map_location=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier_torch_model = classifier_torch_model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.argmax(torch.mm(classifier_torch_model.fc_out.weight, torch.rand((256, 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = self.loss_func(y_out, F.one_hot(y, y_out.shape[1]).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "# from torchmetrics import ConfusionMatrix\n",
    "\n",
    "# def calculate_metrics(cl_model, dataloader):\n",
    "#     cm = ConfusionMatrix(task=\"multiclass\", num_classes=len(id_vocab))\n",
    "\n",
    "#     y_pred = []\n",
    "#     y_true = []\n",
    "\n",
    "#     cl_model = cl_model.eval()\n",
    "#     cl_model.to(device)\n",
    "#     for X, y in tqdm(dataloader):\n",
    "#         X = X.to(device)\n",
    "#         with torch.no_grad():\n",
    "#             y_p = cl_model(X)\n",
    "#             y_p = y_p.cpu()\n",
    "#         y_pred.append(y_p)\n",
    "#         y_true.append(y)\n",
    "#     y_pred = torch.cat(y_pred, dim=0)\n",
    "#     y_true = torch.cat(y_true, dim=0)\n",
    "#     y_pred2 = torch.argmax(y_pred, dim=1)\n",
    "#     y_true2 = torch.argmax(y_true, dim=1)\n",
    "#     print(f'classification report: \\n {classification_report(y_true2, y_pred2, digits=4)}')\n",
    "#     print(f'confusion matrix:\\n {cm(y_pred2, y_true2)}')\n",
    "#     print('================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fardin Rastakhiz @ 2023\n",
    "import torch\n",
    "# from scripts.managers.ModelManager import ModelManager\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from torch_geometric.nn import summary\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "from os import path\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, hinge_loss\n",
    "\n",
    "import lightning as L\n",
    "from utilities.managers.ModelManager import ModelManager\n",
    "from utilities.callbacks.CustomModelCheckpoint import CustomModelCheckpoint\n",
    "\n",
    "class ClassifierModelManager(ModelManager):\n",
    "\n",
    "    def __init__(self,\n",
    "                 torch_model: torch.nn.Module,\n",
    "                 lightning_model,\n",
    "                 model_save_dir: str = '~/Desktop',\n",
    "                 log_dir: str = 'logs/',\n",
    "                 log_name: str = 'model_logs',\n",
    "                 device='cpu',\n",
    "                 num_train_epoch = 100,\n",
    "                 accumulate_grad_batches=1):\n",
    "        super(ClassifierModelManager, self).__init__(torch_model, lightning_model, model_save_dir, log_dir, log_name, device, num_train_epoch, accumulate_grad_batches=accumulate_grad_batches)\n",
    "\n",
    "    def _create_callbacks(self) -> List[Callback]:\n",
    "        return [\n",
    "            CustomModelCheckpoint(save_top_k=2, mode='max', monitor='val_acc', save_last=True, save_weights_only=True),\n",
    "            # EarlyStopping(patience=50, mode='max', monitor='val_acc')\n",
    "        ]\n",
    "\n",
    "    def draw_summary(self, dataloader):\n",
    "        X, y = next(iter(dataloader))\n",
    "        print(summary(self.torch_model, X.to(self.device)))\n",
    "\n",
    "    def plot_csv_logger(self, loss_names=['train_loss', 'val_loss'], eval_names=['train_acc', 'val_acc']):\n",
    "        csv_path = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', 'metrics.csv')\n",
    "        metrics = pd.read_csv(csv_path)\n",
    "\n",
    "        aggregation_metrics = []\n",
    "        agg_col = 'epoch'\n",
    "        for i, dfg in metrics.groupby(agg_col):\n",
    "            agg = dict(dfg.mean())\n",
    "            agg[agg_col] = i\n",
    "            aggregation_metrics.append(agg)\n",
    "\n",
    "        df_metrics = pd.DataFrame(aggregation_metrics)\n",
    "        df_metrics[loss_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='loss')\n",
    "        df_metrics[eval_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='accuracy')\n",
    "        plt.show()\n",
    "\n",
    "    def save_plot_csv_logger(self, loss_names=['train_loss', 'val_loss'], eval_names=['train_acc', 'val_acc'], name_prepend: str=\"\"):\n",
    "        csv_path = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', 'metrics.csv')\n",
    "        metrics = pd.read_csv(csv_path)\n",
    "\n",
    "        aggregation_metrics = []\n",
    "        agg_col = 'epoch'\n",
    "        for i, dfg in metrics.groupby(agg_col):\n",
    "            agg = dict(dfg.mean())\n",
    "            agg[agg_col] = i\n",
    "            aggregation_metrics.append(agg)\n",
    "\n",
    "        df_metrics = pd.DataFrame(aggregation_metrics).iloc[6:]\n",
    "        df_metrics[loss_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='loss')\n",
    "        \n",
    "        loss_png = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', f'{name_prepend}_loss_metric.png')\n",
    "        plt.savefig(loss_png)\n",
    "        \n",
    "        df_metrics[eval_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='accuracy')\n",
    "        \n",
    "        acc_png = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', f'{name_prepend}_acc_metric.png')\n",
    "        plt.savefig(acc_png)\n",
    "        \n",
    "        plt.close()\n",
    "    \n",
    "    def evaluate(self, eval_dataloader,\n",
    "                 give_confusion_matrix: bool=True, \n",
    "                 give_report: bool=True, \n",
    "                 give_f1_score: bool=False, \n",
    "                 give_accuracy_score: bool=False, \n",
    "                 give_precision_score: bool=False, \n",
    "                 give_recall_score: bool=False, \n",
    "                 give_hinge_loss: bool=False):\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        self.lightning_model.eval()\n",
    "        for X, y in eval_dataloader:\n",
    "            y_p = self.lightning_model(X.to(self.device))\n",
    "            if type(y_p) is tuple:\n",
    "                y_p = y_p[0]\n",
    "            y_pred.append((y_p>0).to(torch.int32).detach().to(y.device))\n",
    "            y_true.append(y.to(torch.int32))\n",
    "        y_true = torch.concat(y_true)\n",
    "        y_pred = torch.concat(y_pred)\n",
    "        if(give_confusion_matrix):\n",
    "            print(f'confusion_matrix: \\n{confusion_matrix(y_true, y_pred)}')\n",
    "        if(give_report):\n",
    "            print(classification_report(y_true, y_pred))\n",
    "        if(give_f1_score):\n",
    "            print(f'f1_score: {f1_score(y_true, y_pred)}')\n",
    "        if(give_accuracy_score):\n",
    "            print(f'accuracy_score: {accuracy_score(y_true, y_pred)}')\n",
    "        if(give_precision_score):\n",
    "            print(f'precision_score: {precision_score(y_true, y_pred)}')\n",
    "        if(give_recall_score):\n",
    "            print(f'recall_score: {recall_score(y_true, y_pred)}')\n",
    "        # if(give_hinge_loss):\n",
    "        #     print(f'hinge_loss: {hinge_loss(y_true, y_pred)}')\n",
    "                \n",
    "    def evaluate_best_models(self, eval_dataloader,\n",
    "                             give_confusion_matrix: bool=True, \n",
    "                             give_report: bool=True, \n",
    "                             give_f1_score: bool=False, \n",
    "                             give_accuracy_score: bool=False, \n",
    "                             give_precision_score: bool=False, \n",
    "                             give_recall_score: bool=False, \n",
    "                             give_hinge_loss: bool=False,\n",
    "                             multi_class: bool=False, **kwargs):\n",
    "        \n",
    "        model = self.trainer.model.model\n",
    "        # self.lightning_model = lightning_type.load_from_checkpoint(rf'{self.trainer.checkpoint_callback.best_model_path}', map_location=None, hparams_file=None, strict=True, **kwargs).eval()\n",
    "        self.lightning_model.model.load_state_dict(torch.load(rf'{self.trainer.checkpoint_callback.best_model_path}', weights_only=True, map_location=None))\n",
    "        self.save_evaluation(self.lightning_model, eval_dataloader, 'best_model', give_confusion_matrix, give_report,\n",
    "                             give_f1_score, give_accuracy_score, give_precision_score, give_recall_score, give_hinge_loss, multi_class)\n",
    "            \n",
    "    def save_evaluation(self, model, eval_dataloader, name_prepend: str='',\n",
    "                    give_confusion_matrix: bool=True, \n",
    "                    give_report: bool=True, \n",
    "                    give_f1_score: bool=False, \n",
    "                    give_accuracy_score: bool=False, \n",
    "                    give_precision_score: bool=False, \n",
    "                    give_recall_score: bool=False, \n",
    "                    give_hinge_loss: bool=False,\n",
    "                    multi_class: bool=False\n",
    "                    ):\n",
    "            \n",
    "            test_metrics_path = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', f'{name_prepend}_test_metrics.txt')\n",
    "            \n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            model = model.eval()\n",
    "            for X, y in eval_dataloader:\n",
    "                with torch.no_grad():\n",
    "                    y_p = model(X.to(self.device))\n",
    "                if type(y_p) is tuple:\n",
    "                    y_p = y_p[0]\n",
    "                \n",
    "                if multi_class:\n",
    "                    y_pred.append(y_p.detach().to(y.device))\n",
    "                    y_true.append(y)\n",
    "                else:\n",
    "                    y_pred.append((y_p>0).to(torch.int32).detach().to(y.device))\n",
    "                    y_true.append(y.to(torch.int32))\n",
    "                    \n",
    "            y_true = torch.concat(y_true)\n",
    "            y_pred = torch.concat(y_pred)\n",
    "            print(y_true.shape)\n",
    "            print(y_pred.shape)\n",
    "            if multi_class:\n",
    "                y_true_num = torch.argmax(y_true, dim=1)\n",
    "                y_pred_num = torch.argmax(y_pred, dim=1)\n",
    "            else:\n",
    "                y_true_num = y_true\n",
    "                y_pred_num = y_pred\n",
    "                \n",
    "            print(y_true_num.shape)\n",
    "            print(y_pred_num.shape)\n",
    "            with open(test_metrics_path, 'at+') as f:\n",
    "                if(give_confusion_matrix):\n",
    "                    print(f'confusion_matrix: \\n{confusion_matrix(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_report):\n",
    "                    print(classification_report(y_true_num, y_pred_num), file=f)\n",
    "                if(give_f1_score):\n",
    "                    if multi_class:\n",
    "                        print(f'f1_score: {f1_score(y_true_num, y_pred_num, average=None)}', file=f)\n",
    "                    else:\n",
    "                        print(f'f1_score: {f1_score(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_accuracy_score):\n",
    "                    print(f'accuracy_score: {accuracy_score(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_precision_score):\n",
    "                    if multi_class:\n",
    "                        print(f'precision: {precision_score(y_true_num, y_pred_num, average=None)}', file=f)\n",
    "                    else:\n",
    "                        print(f'precision: {precision_score(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_recall_score):\n",
    "                    if multi_class:\n",
    "                        print(f'recall: {recall_score(y_true_num, y_pred_num, average=None)}', file=f)\n",
    "                    else:\n",
    "                        print(f'recall: {recall_score(y_true_num, y_pred_num)}', file=f)\n",
    "                # if(give_hinge_loss):\n",
    "                #     print(f'hinge_loss: {hinge_loss(y_true_num, y_pred)}', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utilities.managers.ModelManager import ModelManager\n",
    "from utilities.managers.ClassifierModelManager import ClassifierModelManager\n",
    "# from utilities.lightning_models.CGNetEmbeddingLightningModel import CGNetEmbeddingLightningModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 256\n",
    "hidden_dim = 256\n",
    "embedding_dim = 64\n",
    "seed = 911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import lightning as L\n",
    "import torchmetrics\n",
    "\n",
    "class CGNetEmbeddingLightningModel(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        num_classes,\n",
    "        optimizer=None,\n",
    "        loss_func=None,\n",
    "        learning_rate=0.01,\n",
    "        batch_size=64,\n",
    "        lr_scheduler=None,\n",
    "        user_lr_scheduler=False,\n",
    "        min_lr=0.0,\n",
    "    ):\n",
    "        super(CGNetEmbeddingLightningModel, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "        self.min_lr = min_lr\n",
    "        # self.save_hyperparameters(ignore=[\"model\"])\n",
    "        self.save_hyperparameters(logger=False)\n",
    "        self.optimizer = self._get_optimizer(optimizer)\n",
    "        self.lr_scheduler = (\n",
    "            self._get_lr_scheduler(lr_scheduler) if user_lr_scheduler else None\n",
    "        )\n",
    "        self.loss_func = loss_func\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.model(x.x, torch.zeros((2, 0)), x.token_subsampling_probabilities, x.token_indices, x.token_sentiments, x.token_lengths, x.num_tokens, x.character_length, x.token_embeddings)\n",
    "\n",
    "    def on_train_epoch_start(self) -> None:\n",
    "        param_groups = next(iter(self.optimizer.param_groups))\n",
    "        if \"lr\" in param_groups and param_groups[\"lr\"] is not None:\n",
    "            current_learning_rate = float(param_groups[\"lr\"])\n",
    "            self.log(\n",
    "                \"lr\",\n",
    "                current_learning_rate,\n",
    "                batch_size=self.batch_size,\n",
    "                on_epoch=True,\n",
    "                on_step=False,\n",
    "            )\n",
    "\n",
    "    def training_step(self, batch, *args, **kwargs):\n",
    "        X, y = batch\n",
    "        X.to(self.device)\n",
    "        y.to(self.device)\n",
    "        \n",
    "        self.model.train()\n",
    "        y_out = self(X)\n",
    "        \n",
    "        loss = self.loss_func(y_out, F.one_hot(y, y_out.shape[1]).float())\n",
    "        self.train_losses.append(loss.detach().item())\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            batch_size=self.batch_size,\n",
    "            on_epoch=True,\n",
    "            on_step=True,\n",
    "        )\n",
    "        \n",
    "        self.train_acc(torch.argmax(y_out, dim=1), y)\n",
    "        self.log('train_acc', self.train_acc, prog_bar=True, on_epoch=True, on_step=True, batch_size=self.batch_size)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, *args, **kwargs):\n",
    "        X, y = batch\n",
    "        X.to(self.device)\n",
    "        y.to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        y_out = self(X)\n",
    "        loss = self.loss_func(y_out, F.one_hot(y, y_out.shape[1]).float())\n",
    "        self.val_losses.append(loss.detach().item())\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            batch_size=self.batch_size,\n",
    "            on_epoch=True,\n",
    "            on_step=True,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.val_acc(torch.argmax(y_out, dim=1), y)\n",
    "        self.log('val_acc', self.val_acc, prog_bar=True, on_epoch=True, on_step=True, batch_size=self.batch_size)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.lr_scheduler is None:\n",
    "            return self.optimizer\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": self.optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": self.lr_scheduler,\n",
    "                \"monitor\": \"train_loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def update_learning_rate(self, learning_rate: float):\n",
    "        self.learning_rate = learning_rate\n",
    "        for g in self.optimizer.param_groups:\n",
    "            g[\"lr\"] = learning_rate\n",
    "\n",
    "    def _get_optimizer(self, optimizer):\n",
    "        return (\n",
    "            optimizer\n",
    "            if optimizer is not None\n",
    "            else torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        )\n",
    "\n",
    "    def _get_lr_scheduler(self, lr_scheduler):\n",
    "        return (\n",
    "            lr_scheduler\n",
    "            if lr_scheduler is not None\n",
    "            else torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer, patience=5, factor=0.5, mode=\"min\", min_lr=self.min_lr\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequency_tensor = torch.from_numpy(np.array([term_frequencies[id_vocab[i]] for i in range(len(id_vocab))]))\n",
    "term_weights = subsampling_equation_linear(term_frequency_tensor)\n",
    "term_weights[0] = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(id_vocab)):\n",
    "#     if id_vocab[i] == 'significant':\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term_weights[30917]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_manager(epochs=30, dropout=0.25, weight_decay=0.000012, lr=0.0002, amsgrad=False, fused=True, num_gnn_layers=6):\n",
    "    token_embedding_model = CGNetTokenLevelEmbedding(embedding_dim=embedding_dim, hidden_dim=hidden_dim, inject_embedding_dim=128, dropout=dropout,  seed=seed, random_edges=6, lattice_edges=10, lattice_step=2, lattice_start_distance=2, head=8, gnn_layers=num_gnn_layers).to(device)\n",
    "    embedding_model = CGNetEmbedding(token_embedding_model, hidden_dim=hidden_dim, output_dim=output_size).to(device)\n",
    "    classifier_torch_model = CNN_for_Text_No_Positional_Encoding(embedding_model, hidden_dim=output_size, dropout=dropout, num_out_features=len(id_vocab)).to(device)\n",
    "        \n",
    "    # optimizer = torch.optim.Adam(classifier_torch_model.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=amsgrad, fused=fused)\n",
    "    optimizer = torch.optim.AdamW(classifier_torch_model.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=amsgrad, fused=fused)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 200, 300, 400, 500, 600, 700],gamma=0.9, verbose=False)\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15, 20, 30, 40, 45,50,55],gamma=0.5, verbose=False)\n",
    "    loss_func = torch.nn.CrossEntropyLoss(weight=term_weights)\n",
    "    classfier_lightning_model = CGNetEmbeddingLightningModel(classifier_torch_model, \n",
    "                                                        num_classes=len(id_vocab),\n",
    "                                                learning_rate=lr,\n",
    "                                                batch_size=batch_size,\n",
    "                                                optimizer=optimizer,\n",
    "                                                loss_func=loss_func,\n",
    "                                                lr_scheduler=lr_scheduler,\n",
    "                                                user_lr_scheduler=True\n",
    "                                                ).to(device)\n",
    "\n",
    "\n",
    "    model_manager = ClassifierModelManager(classifier_torch_model, classfier_lightning_model, log_name=f'Self_Supervised',device=device, num_train_epoch=epochs, accumulate_grad_batches=1)\n",
    "    return model_manager\n",
    "\n",
    "def train_model(model_manager, train_dataloader, test_dataloader, max_epochs=30, dropout=0.25, weight_decay=0.000012, lr=0.0002):\n",
    "    \n",
    "    model_manager.fit(train_dataloaders=train_dataloader, val_dataloaders=test_dataloader, max_epochs=max_epochs)\n",
    "    name_prepend=f'tests_{dropout}_{weight_decay}_{lr}_{max_epochs}'\n",
    "    model_manager.save_plot_csv_logger(loss_names=['train_loss_epoch', 'val_loss_epoch'], eval_names=['train_acc_epoch', 'val_acc_epoch'], name_prepend=name_prepend, start_from=5)\n",
    "    model_manager.lightning_model.model = model_manager.torch_model.to(device)\n",
    "    model_manager.save_evaluation(model_manager.lightning_model, test_dataloader, name_prepend,True, True, True, True, True, True, True, multi_class=True)\n",
    "    # trainer.fit(classfier_lightning_model, train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)\n",
    "    classfier_lightning_model = model_manager.lightning_model.eval()\n",
    "    classfier_lightning_model.model = classfier_lightning_model.model.eval()\n",
    "    # calculate_metrics(classfier_lightning_model, test_dataloader)\n",
    "    model_manager.evaluate_best_models(test_dataloader,True, True, True, True, True, True, True, multi_class=True, model=classfier_lightning_model.model, num_classes=len(id_vocab))\n",
    "    return model_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_manager = create_model_manager(300, 0.2, 0.000012, 0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ranges = np.linspace(0, 1, 101)[:, None]\n",
    "data_ranges = np.concatenate([data_ranges[:-1, :], data_ranges[1:, :]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'loss_func' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_func'])`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "100%|██████████| 25600/25600 [01:04<00:00, 396.90it/s]\n",
      "100%|██████████| 3072/3072 [00:07<00:00, 402.25it/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                                | Params | Mode \n",
      "--------------------------------------------------------------------------\n",
      "0 | model     | CNN_for_Text_No_Positional_Encoding | 36.6 M | train\n",
      "1 | loss_func | CrossEntropyLoss                    | 0      | train\n",
      "2 | train_acc | MulticlassAccuracy                  | 0      | train\n",
      "3 | val_acc   | MulticlassAccuracy                  | 0      | train\n",
      "4 | test_acc  | MulticlassAccuracy                  | 0      | train\n",
      "--------------------------------------------------------------------------\n",
      "36.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "36.6 M    Total params\n",
      "146.468   Total estimated model params size (MB)\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7bef5ac75441869d358e17ece01132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e4aa2eb5ba4ff3811c8d7e5dba338d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f33ee1fd5f24976952cf93d0c011172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe9aa10cb6947fb890902a789a9f527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bafabad140f645f4aff747a575c79b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab9e13d717a48ecb8c327531d83fa4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f61071cf904b8ca668f92d5787d2ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274b25282ec4444cafdacb2dc21e7743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9af4cd3be7c462f9e74eabde13bbb99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954c31f89a23413ca1f8b60036881aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b3bced38c542f5b81b9f944f4c7f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3072])\n",
      "torch.Size([3072, 128001])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3072])\n",
      "torch.Size([3072, 128001])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "100%|██████████| 25600/25600 [01:10<00:00, 361.02it/s]\n",
      "100%|██████████| 3072/3072 [00:07<00:00, 431.16it/s]\n",
      "c:\\Users\\fardin\\Projects\\Articles\\CGNet\\utilities\\callbacks\\CustomModelCheckpoint.py:659: UserWarning: Checkpoint directory logs/Self_Supervised\\version_14\\checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                                | Params | Mode\n",
      "-------------------------------------------------------------------------\n",
      "0 | model     | CNN_for_Text_No_Positional_Encoding | 36.6 M | eval\n",
      "1 | loss_func | CrossEntropyLoss                    | 0      | eval\n",
      "2 | train_acc | MulticlassAccuracy                  | 0      | eval\n",
      "3 | val_acc   | MulticlassAccuracy                  | 0      | eval\n",
      "4 | test_acc  | MulticlassAccuracy                  | 0      | eval\n",
      "-------------------------------------------------------------------------\n",
      "36.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "36.6 M    Total params\n",
      "146.468   Total estimated model params size (MB)\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c22b761b614a7c9459fcee09452c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a65f60ac660d4f48af8fec82b9f0c833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f611c31cc3e14beda1344089cb4ffd94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ae4ef7888c4974a3eba11a5584fc4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6285320f0c745a5a28795c4b936af93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d554b9e4924d1bbec928274a468ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe9d077ad3e4dd2be8ae3eaf804719c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9205d3072064cb6a2df4802b2d52d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50cf0de8804d467fbc6ed98467b2640d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3072])\n",
      "torch.Size([3072, 128001])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "max_epochs = 10\n",
    "model_manager = create_model_manager(max_epochs, 0.2, 0.000012, 0.0002, num_gnn_layers=8)\n",
    "\n",
    "for i in range(len(data_ranges)):\n",
    "    train_dataset, test_dataset, train_dataloader, test_dataloader = load_data_range(data_ranges[i][0], data_ranges[i][1])\n",
    "    # load data\n",
    "    train_dataset.reset_params()\n",
    "    train_dataset.position_j = 0\n",
    "    test_dataset.reset_params()\n",
    "    test_dataset.position_j = 0\n",
    "    model_manager = train_model(model_manager, train_dataloader, test_dataloader, max_epochs, 0.2, 0.000012, 0.0002)\n",
    "    max_epochs += 15\n",
    "    time.sleep(10)\n",
    "    torch.cuda.empty_cache()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, hinge_loss\n",
    "\n",
    "def calculatge_metrics(chpt_path, target_data_loader, num_classes):\n",
    "    \n",
    "    # embedding_model = CGNetEmbedding(embedding_dim=embedding_dim, hidden_dim=hidden_dim, dropout=0.2,  seed=seed, random_edges=6, lattice_edges=10, lattice_step=2, lattice_start_distance=2)\n",
    "    # classifier_torch_model = CNN_for_Text_No_Positional_Encoding(embedding_model, hidden_dim=hidden_dim, dropout=0.2, num_out_features=num_classes)\n",
    "    \n",
    "    token_embedding_model = CGNetTokenLevelEmbedding(embedding_dim=embedding_dim, hidden_dim=hidden_dim, inject_embedding_dim=128, dropout=0.2,  seed=seed, random_edges=6, lattice_edges=10, lattice_step=2, lattice_start_distance=2).to(device)\n",
    "    embedding_model = CGNetEmbedding(token_embedding_model, hidden_dim=hidden_dim, output_dim=output_size).to(device)\n",
    "    classifier_torch_model = CNN_for_Text_No_Positional_Encoding(embedding_model, hidden_dim=output_size, dropout=0.2, num_out_features=len(id_vocab)).to(device)\n",
    "    \n",
    "    classifier_torch_model.load_state_dict(torch.load(chpt_path, weights_only=True, map_location=\"cuda:0\"))\n",
    "    classfier_lightning_model = CGNetEmbeddingLightningModel(classifier_torch_model, \n",
    "                                                    num_classes=num_classes,\n",
    "                                            batch_size=batch_size,\n",
    "                                            user_lr_scheduler=True\n",
    "                                            ).to(device).eval()\n",
    "    \n",
    "    mean_infer_acc = []\n",
    "    mean_infer_f1 = []\n",
    "    mean_infer_prec = []\n",
    "    mean_infer_rec = []\n",
    "    for i in range(1):\n",
    "        all_ys = []\n",
    "        all_y_preds = []\n",
    "        for X, y in target_data_loader:\n",
    "            with torch.no_grad():\n",
    "                y_pred = classfier_lightning_model(X.to(device))\n",
    "            all_ys.append(y)\n",
    "            all_y_preds.append(torch.argmax(y_pred.cpu(), dim=1))\n",
    "        all_ys = torch.concat(all_ys)\n",
    "        all_y_preds = torch.concat(all_y_preds)\n",
    "        \n",
    "        # cm = confusion_matrix(all_ys, all_y_preds, labels=range(num_classes))\n",
    "        \n",
    "        accuracy = accuracy_score(all_ys, all_y_preds)# np.sum(np.diag(cm))/ np.sum(cm)\n",
    "        precision = precision_score(all_ys, all_y_preds, average=None)# np.mean(np.diag(cm) / (np.sum(cm, axis=0)+0.000001))\n",
    "        recall = recall_score(all_ys, all_y_preds, average=None)# np.mean(np.diag(cm) / (np.sum(cm, axis=1)+0.000001))\n",
    "        f1 = f1_score(all_ys, all_y_preds, average=None) # (2*precision*recall)/(precision + recall+0.000001)\n",
    "        \n",
    "        mean_infer_acc.append(accuracy)\n",
    "        mean_infer_f1.append(f1)\n",
    "        mean_infer_prec.append(precision)\n",
    "        mean_infer_rec.append(recall)\n",
    "    mean_infer_acc = torch.mean(torch.tensor(mean_infer_acc))\n",
    "    mean_infer_f1 = torch.mean(torch.tensor(mean_infer_f1))\n",
    "    mean_infer_prec = torch.mean(torch.tensor(mean_infer_prec))\n",
    "    mean_infer_rec = torch.mean(torch.tensor(mean_infer_rec))\n",
    "    return mean_infer_acc, mean_infer_f1, mean_infer_prec, mean_infer_rec, classfier_lightning_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "mean_infer_acc, mean_infer_f1, mean_infer_prec, mean_infer_rec, classfier_lightning_model = calculatge_metrics(r'logs\\Self_Supervised\\version_0\\checkpoints\\epoch=214-step=5375.pth', test_dataloader, len(id_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classfier_lightning_model = classfier_lightning_model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = classfier_lightning_model.model(X.x, torch.zeros((2, 0)), X.token_subsampling_probabilities, X.token_indices, X.token_sentiments, X.token_lengths, X.num_tokens, X.character_length, X.token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 128001])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -6.9436, -16.4197, -12.7505,  ..., -23.0256, -16.8616, -28.7879])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_output.detach()[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13553)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(F.one_hot(y, sample_output.shape[1]).float()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0022, -0.0063,  0.0009,  0.0302, -0.0343,  0.0555,  0.0661, -0.0372,\n",
       "         0.0439, -0.0066,  0.0107,  0.0522, -0.0200,  0.0545,  0.0447, -0.0375,\n",
       "         0.0140, -0.0465, -0.0287,  0.0421,  0.0427,  0.0174,  0.0284,  0.0190,\n",
       "         0.0498, -0.0525,  0.0202,  0.0085, -0.0230,  0.0427,  0.0333,  0.0679,\n",
       "        -0.0088, -0.0265,  0.0445,  0.0134,  0.0087, -0.0306, -0.0455,  0.0115,\n",
       "         0.0506,  0.0005,  0.0204,  0.0669, -0.0558,  0.0243,  0.0543, -0.0283,\n",
       "        -0.0289, -0.0215, -0.0054, -0.0066,  0.0014,  0.0415,  0.0501,  0.0450,\n",
       "         0.0024, -0.0526, -0.0389, -0.0275,  0.0203,  0.0536, -0.0297, -0.0581,\n",
       "         0.0222, -0.0285, -0.0389,  0.0411,  0.0147, -0.0101,  0.0404,  0.0212,\n",
       "         0.0115, -0.0264,  0.0150,  0.0164,  0.0201,  0.0085,  0.0457, -0.0487,\n",
       "        -0.0340, -0.0640,  0.0256, -0.0346,  0.0479,  0.0232,  0.0382, -0.0190,\n",
       "         0.0140,  0.0124,  0.0123,  0.0684, -0.0061,  0.0497, -0.0285,  0.0190,\n",
       "         0.0275,  0.0507,  0.0302, -0.0038, -0.0492, -0.0157,  0.0454, -0.0333,\n",
       "         0.0374, -0.0211, -0.0381,  0.0640,  0.0285, -0.0260, -0.0145,  0.0323,\n",
       "         0.0180,  0.0400,  0.0048, -0.0099, -0.0419, -0.0057, -0.0292, -0.0005,\n",
       "        -0.0283, -0.0141,  0.0401,  0.0257,  0.0158,  0.0676,  0.0047, -0.0443,\n",
       "         0.0056, -0.0640,  0.0485, -0.0150, -0.0220,  0.0040, -0.0481,  0.0473,\n",
       "         0.0639,  0.0419, -0.0005, -0.0449, -0.0308, -0.0276,  0.0492, -0.0263,\n",
       "         0.0305, -0.0602, -0.0045, -0.0375,  0.0239, -0.0665,  0.0612, -0.0203,\n",
       "         0.0076,  0.0163, -0.0141,  0.0175,  0.0288, -0.0092, -0.0381, -0.0078,\n",
       "        -0.0320, -0.0507,  0.0564, -0.0266,  0.0189, -0.0023,  0.0121,  0.0128,\n",
       "         0.0341,  0.0389,  0.0163, -0.0190, -0.0145, -0.0652,  0.0624,  0.0244,\n",
       "        -0.0118,  0.0396, -0.0210, -0.0007,  0.0107, -0.0324, -0.0103, -0.0321,\n",
       "         0.0676, -0.0023, -0.0609,  0.0038, -0.0587,  0.0123, -0.0198, -0.0119,\n",
       "         0.0376,  0.0113,  0.0512, -0.0553, -0.0429, -0.0257,  0.0614, -0.0321,\n",
       "        -0.0126,  0.0009, -0.0480,  0.0481,  0.0523, -0.0148,  0.0328, -0.0555,\n",
       "        -0.0386,  0.0345,  0.0267,  0.0165, -0.0376,  0.0501,  0.0327, -0.0217,\n",
       "         0.0164,  0.0252, -0.0134,  0.0474,  0.0172,  0.0337, -0.0411, -0.0381,\n",
       "        -0.0542, -0.0562,  0.0347,  0.0003,  0.0564, -0.0024,  0.0243,  0.0074,\n",
       "         0.0506, -0.0379, -0.0466, -0.0565, -0.0382, -0.0291, -0.0117,  0.0034,\n",
       "        -0.0102,  0.0173, -0.0531, -0.0363, -0.0177,  0.0357,  0.0438, -0.0180,\n",
       "        -0.0478, -0.0510, -0.0058, -0.0402, -0.0170, -0.0148,  0.0079,  0.0160],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classfier_lightning_model.model.fc_out.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "def get_best_chpt(metrics_path, epoch_numbers):\n",
    "    epoch_data = pd.read_csv(metrics_path)\n",
    "    if 'val_acc_epoch' in epoch_data.columns and epoch_data['val_acc_epoch'].notna().any():\n",
    "        best_chpt = epoch_data.loc[epoch_data['val_acc_epoch'].idxmax()]\n",
    "    elif 'val_loss_epoch' in epoch_data.columns and epoch_data['val_loss_epoch'].notna().any():\n",
    "        best_chpt = epoch_data.loc[epoch_data['val_loss_epoch'].idxmin()]\n",
    "    else:\n",
    "        raise ValueError(f\"No valid validation metrics available for epoch {epoch_numbers}.\")\n",
    "    return np.argwhere(np.array(epoch_numbers)==best_chpt['epoch']).item(), best_chpt['val_loss_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_metrics_mean(base_path = 'logs\\CNN-GNN18_mr2k_seeds', start=0, interval=1):\n",
    "    total_accuracy = []\n",
    "    total_f1 = []\n",
    "    total_prec = []\n",
    "    total_rec = []\n",
    "    total_loss = []\n",
    "    \n",
    "    for i in range(start, start + interval):\n",
    "        version_path = join(base_path, f'version_{i}')\n",
    "        checkpoint_path = join(version_path, f'checkpoints')\n",
    "        onlyfiles  = [f for f in listdir(checkpoint_path) if (isfile(join(checkpoint_path, f)) and 'epoch' in f) ]\n",
    "        epoch_numbers = [int(re.search(r'\\d+', f).group()) for f in onlyfiles]\n",
    "        best_chpt_id, loss = get_best_chpt(join(version_path, 'metrics.csv'), epoch_numbers)\n",
    "        print(onlyfiles[best_chpt_id])\n",
    "        mean_infer_acc, mean_infer_f1, mean_infer_prec, mean_infer_rec, classfier_lightning_model = calculatge_metrics(join(checkpoint_path, f'{onlyfiles[best_chpt_id]}'), test_dataloader)\n",
    "            \n",
    "        total_accuracy.append(mean_infer_acc)\n",
    "        total_f1.append(mean_infer_f1)\n",
    "        total_prec.append(mean_infer_prec)\n",
    "        total_rec.append(mean_infer_rec)\n",
    "        total_loss.append(loss)\n",
    "\n",
    "    total_accuracy = torch.mean(torch.tensor(total_accuracy))\n",
    "    total_f1 = torch.mean(torch.tensor(total_f1))\n",
    "    total_prec = torch.mean(torch.tensor(total_prec))\n",
    "    total_rec = torch.mean(torch.tensor(total_rec))\n",
    "    total_loss = torch.mean(torch.tensor(total_loss))\n",
    "    print(f'total_accuracy: {total_accuracy}')\n",
    "    print(f'total_f1: {total_f1}')\n",
    "    print(f'total_prec: {total_prec}')\n",
    "    print(f'total_rec: {total_rec}')\n",
    "    print(f'total_loss: {total_loss}')\n",
    "    return classfier_lightning_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'logs\\\\CNN-GNN_False_False_False\\\\version_27\\\\checkpoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\Articles\\CGNet\\FindBestModel\\7_PrepareForPretraining\\7_prepare_dataset_for_auto_regression.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_PrepareForPretraining/7_prepare_dataset_for_auto_regression.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m classfier_lightning_model \u001b[39m=\u001b[39m calculate_average_metrics_mean(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlogs\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mCNN-GNN_False_False_False\u001b[39;49m\u001b[39m'\u001b[39;49m, start\u001b[39m=\u001b[39;49m\u001b[39m27\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\Articles\\CGNet\\FindBestModel\\7_PrepareForPretraining\\7_prepare_dataset_for_auto_regression.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_PrepareForPretraining/7_prepare_dataset_for_auto_regression.ipynb#X41sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m version_path \u001b[39m=\u001b[39m join(base_path, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mversion_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_PrepareForPretraining/7_prepare_dataset_for_auto_regression.ipynb#X41sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m checkpoint_path \u001b[39m=\u001b[39m join(version_path, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcheckpoints\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_PrepareForPretraining/7_prepare_dataset_for_auto_regression.ipynb#X41sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m onlyfiles  \u001b[39m=\u001b[39m [f \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m listdir(checkpoint_path) \u001b[39mif\u001b[39;00m (isfile(join(checkpoint_path, f)) \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m f) ]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_PrepareForPretraining/7_prepare_dataset_for_auto_regression.ipynb#X41sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m epoch_numbers \u001b[39m=\u001b[39m [\u001b[39mint\u001b[39m(re\u001b[39m.\u001b[39msearch(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+\u001b[39m\u001b[39m'\u001b[39m, f)\u001b[39m.\u001b[39mgroup()) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m onlyfiles]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Articles/CGNet/FindBestModel/7_PrepareForPretraining/7_prepare_dataset_for_auto_regression.ipynb#X41sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m best_chpt_id, loss \u001b[39m=\u001b[39m get_best_chpt(join(version_path, \u001b[39m'\u001b[39m\u001b[39mmetrics.csv\u001b[39m\u001b[39m'\u001b[39m), epoch_numbers)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'logs\\\\CNN-GNN_False_False_False\\\\version_27\\\\checkpoints'"
     ]
    }
   ],
   "source": [
    "classfier_lightning_model = calculate_average_metrics_mean(r'logs\\CNN-GNN_False_False_False', start=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=52-step=20723.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_accuracy: 0.9022218670076727\n",
      "total_f1: 0.9022369648498394\n",
      "total_prec: 0.9022530633260486\n",
      "total_rec: 0.9022218669355642\n",
      "total_loss: 0.250955730676651\n"
     ]
    }
   ],
   "source": [
    "classfier_lightning_model = calculate_average_metrics_mean(r'logs\\CNN-GNN_False_False_False', start=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
