{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uninterrupted character-level input with token representation using GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas:<br>\n",
    " - Continuous unpadded/untruncated character sequences\n",
    " - Generate graph after token representation inside model\n",
    " - Using dynamic sparse GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Results:\n",
    "| metric | test 1 | test 2 | test 3 | test 4 | test 5 | Average |\n",
    "|:---|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "| f1-score | \n",
    "| accuracy | \n",
    "| precision |\n",
    "| recall | \n",
    "| loss | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch_scatter import scatter_max, scatter_mean, scatter_sum, scatter_std\n",
    "import torchmetrics\n",
    "import lightning as L\n",
    "from torch_geometric.data import Batch, Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from copy import copy\n",
    "import spacy\n",
    "from umap import UMAP\n",
    "\n",
    "import os\n",
    "import PyPDF2\n",
    "from bs4 import BeautifulSoup\n",
    "# import textract\n",
    "import json\n",
    "import gzip\n",
    "import re\n",
    "import collections\n",
    "import csv\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\CrawledWeb\\\\TopicClassification\\\\URLClassification\\\\url_classes2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\CGNet\\FindBestModel\\1_WhereToAddPositionalEmbedding\\CNNGNN_tweet_tokenizer_spacy_positional_embedding_html.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/1_WhereToAddPositionalEmbedding/CNNGNN_tweet_tokenizer_spacy_positional_embedding_html.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/1_WhereToAddPositionalEmbedding/CNNGNN_tweet_tokenizer_spacy_positional_embedding_html.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m folder_path \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mCrawledWeb\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mTopicClassification\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mHTMLClassification\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mhtmls\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/1_WhereToAddPositionalEmbedding/CNNGNN_tweet_tokenizer_spacy_positional_embedding_html.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m urls_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mCrawledWeb\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mTopicClassification\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mURLClassification\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39murl_classes2.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/1_WhereToAddPositionalEmbedding/CNNGNN_tweet_tokenizer_spacy_positional_embedding_html.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m device\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1024\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1011\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1012\u001b[0m     dialect,\n\u001b[0;32m   1013\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[0;32m   1022\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1024\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:618\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    615\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    617\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 618\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    620\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    621\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1618\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1615\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1617\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1618\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1878\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1876\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1877\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1878\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1879\u001b[0m     f,\n\u001b[0;32m   1880\u001b[0m     mode,\n\u001b[0;32m   1881\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1882\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1883\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1884\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1885\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1886\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1887\u001b[0m )\n\u001b[0;32m   1888\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1889\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data\\\\CrawledWeb\\\\TopicClassification\\\\URLClassification\\\\url_classes2.csv'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 1\n",
    "folder_path = r'data\\CrawledWeb\\TopicClassification\\HTMLClassification\\htmls'\n",
    "urls_df = pd.read_csv(r'data\\CrawledWeb\\TopicClassification\\URLClassification\\url_classes2.csv')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "771064"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(r'data\\reduced_embeddings\\spacy_lg_reduced_embeddings.npy', 'rb') as f:\n",
    "    embeddings = np.load(f)\n",
    "nlp = spacy.load('en_core_web_lg', disable=['tok2vec','tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
    "t_tokenizer = TweetTokenizer()\n",
    "nlp.max_length = len(' '.join(list(nlp.vocab.strings)))+1\n",
    "all_vocab_doc = nlp(' '.join(list(nlp.vocab.strings)))\n",
    "all_vocab_str = [f'{t}' for t in all_vocab_doc]\n",
    "embeddings = torch.from_numpy(embeddings)\n",
    "embeddings = (embeddings - torch.min(embeddings)) / (torch.max(embeddings)-torch.min(embeddings))\n",
    "token_vocab_dict = dict(zip(all_vocab_str, embeddings))\n",
    "token_vocab_dict['<n>'] = token_vocab_dict['newline']\n",
    "len(token_vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2744"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open(rf'data\\CrawledWeb\\TopicClassification\\ExtractedWebTexts\\web_contents_reduced.json.gz', 'rt') as file:\n",
    "    web_contents = json.load(file)\n",
    "web_contents = {int(k):v for k, v in web_contents.items()}\n",
    "doc_lengths = np.array([len(web_contents[i]) for i in web_contents])\n",
    "list(web_contents.keys())[torch.topk(torch.from_numpy(doc_lengths), 5).indices[1].item()]\n",
    "# torch.topk(torch.from_numpy(doc_lengths), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'countable y related activities are limited solely to required conditioning activities and / or <n> in skil - related instruction are not required to count toward the 132 - day season ; <n> During any we k in which practice or competition oc urs , a student - involvement in countable y <n> related ac'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_contents[list(web_contents.keys())[torch.topk(torch.from_numpy(doc_lengths), 5).indices[1].item()]][1000000:1000300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_indices = np.array(list(web_contents.keys()), dtype=np.int32)\n",
    "df = pd.read_csv(r'data\\CrawledWeb\\TopicClassification\\URLClassification\\url_classes2.csv')\n",
    "df = df.iloc[active_indices]\n",
    "train_indices, val_indices = train_test_split(active_indices, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = df.Topic.unique()\n",
    "class_id = {t:i for i, t in enumerate(class_list)}\n",
    "id_class = {i:t for i, t in enumerate(class_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_chars = string.ascii_letters + string.digits + string.punctuation + ' '\n",
    "all_chars = set(allowed_chars)\n",
    "vocab_dict = {c:i for i, c in enumerate(allowed_chars)}\n",
    "if '\\x01' not in vocab_dict:\n",
    "    vocab_dict['\\x01'] = len(vocab_dict)\n",
    "char_Set = set(vocab_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7571"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(web_contents.keys())[torch.topk(torch.from_numpy(doc_lengths), 1000).indices[999].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52091"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(web_contents[7571][:138000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterandTokenLevelCustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, indices, num_classes, char_dict, token_dict, tokenizer, max_doc_length=10000) -> None:\n",
    "        super().__init__()\n",
    "        y = y[indices].values\n",
    "        y = torch.from_numpy(np.array([class_id[c] for c in y], dtype=np.longlong))\n",
    "        self.y = torch.nn.functional.one_hot(y, num_classes=num_classes).float()\n",
    "        \n",
    "        self.char_dict = char_dict\n",
    "        self.char_Set = set(char_dict.keys())\n",
    "        self.vocab_size = len(self.char_dict)\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.all_data = []# torch.zeros((len(X), self.doc_char_length), dtype=torch.long)\n",
    "        # self.token_lengths = torch.zeros((len(X), self.doc_teken_length), dtype=torch.int)\n",
    "        # self.token_indices = torch.zeros((len(X), self.doc_char_length), dtype=torch.long)\n",
    "        self.token_lengths = []\n",
    "        \n",
    "        self.token_embeddign_ids = []\n",
    "        for i in indices:\n",
    "            doc=X[i][:max_doc_length]\n",
    "            \n",
    "            tokens = tokenizer(''.join(c for c in doc if c in self.char_Set))\n",
    "            tokens = [t for t in tokens] if len(tokens) > 0 else ['empty']\n",
    "            tokens.append('\\x01')\n",
    "            token_embs = [token_dict[t] if t in token_dict else torch.zeros((64, ), dtype=torch.float32) for t in tokens]\n",
    "            token_embs = torch.from_numpy(np.array(token_embs, dtype=np.float32))\n",
    "            doc = ' '.join(tokens)\n",
    "            \n",
    "            # for t in set(doc):\n",
    "            #     self.char_dict.setdefault(t, len(self.char_dict))\n",
    "\n",
    "            characters = torch.from_numpy(np.array([self.char_dict[t] for t in doc], dtype=np.longlong))\n",
    "            # char_pad_size = max(self.doc_char_length - len(indices), 0)\n",
    "            # indices = torch.nn.functional.pad(indices[:self.doc_char_length], (0,char_pad_size))\n",
    "            \n",
    "            token_lengths = torch.from_numpy(np.array([len(t) for t in tokens], dtype=np.longlong))+1\n",
    "            token_lengths[-1] -= 1\n",
    "            token_positions = torch.arange(len(token_lengths), dtype=torch.long)\n",
    "            token_indices = torch.repeat_interleave(token_positions, token_lengths)\n",
    "            # token_indices = torch.nn.functional.pad(token_indices[:self.doc_char_length], (0,char_pad_size), value=len(token_lengths)-1)\n",
    "            \n",
    "            # self.token_embeddign_ids.append(token_embs)\n",
    "            g_data = Data(x=characters,\n",
    "                          token_positions=token_positions,\n",
    "                          character_length = len(characters),\n",
    "                          num_tokens = len(token_lengths),\n",
    "                          token_indices=token_indices,\n",
    "                          token_lengths=token_lengths,\n",
    "                          token_embeddings=token_embs)\n",
    "            \n",
    "                        #   character_token_lengths = torch.repeat_interleave(token_lengths, token_lengths)\n",
    "            \n",
    "            self.all_data.append(g_data)\n",
    "            # self.token_lengths.append(token_lengths)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # emb_data = Data(x=self.token_embeddigns[self.token_embeddign_ids[index]])\n",
    "        return self.all_data[index], self.y[index]#, Batch.from_data_list([emb_data])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# from scripts.data.datasets.GraphConstructorDataset3 import WordGraphLabeledDataset\n",
    "\n",
    "class CharacterandTokenLevelDataLoader(DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size: int = 1,\n",
    "        shuffle: bool = False,\n",
    "        follow_batch: List[str] | None = None,\n",
    "        exclude_keys: List[str] | None = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(CharacterandTokenLevelDataLoader, self).__init__(\n",
    "            dataset, batch_size, shuffle, follow_batch, exclude_keys, **kwargs\n",
    "        )\n",
    "\n",
    "    def __iter__(self):\n",
    "        base_iterator = super(CharacterandTokenLevelDataLoader, self).__iter__()\n",
    "        for batch in base_iterator:\n",
    "            cumsum_vals = torch.cumsum(batch[0].num_tokens, dim=0).roll(1)\n",
    "            cumsum_vals[0] = 0\n",
    "            additions = torch.repeat_interleave(cumsum_vals, batch[0].character_length)\n",
    "            batch[0].cumulative_token_indices = batch[0].token_indices + additions\n",
    "            yield batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1736,  7107,  9739, ..., 46382, 19626,  7334])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a histogram\n",
    "plt.hist(doc_lengths, bins='auto', edgecolor='black')\n",
    "plt.xlim([0,64000])\n",
    "# Add labels and title\n",
    "plt.xlabel('String Lengths')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of String Lengths')\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "max_doc_length=2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CharacterandTokenLevelCustomDataset(web_contents, df.Topic, train_indices, len(class_id), vocab_dict, token_vocab_dict, t_tokenizer.tokenize, max_doc_length=max_doc_length)\n",
    "test_dataset = CharacterandTokenLevelCustomDataset(web_contents, df.Topic, val_indices, len(class_id), vocab_dict, token_vocab_dict, t_tokenizer.tokenize, max_doc_length=max_doc_length)\n",
    "train_dataloader = CharacterandTokenLevelDataLoader(train_dataset, batch_size=batch_size, drop_last=True, shuffle=False)\n",
    "test_dataloader = CharacterandTokenLevelDataLoader(test_dataset, batch_size=batch_size, drop_last=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embedding = len(vocab_dict)\n",
    "# del web_contents\n",
    "# del train_indices\n",
    "# del vocab_dict\n",
    "# del token_vocab_dict\n",
    "# del nlp\n",
    "# del val_indices\n",
    "# del df\n",
    "# del active_indices\n",
    "# del doc_lengths\n",
    "# del embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 64\n",
    "# train_dataloader = CharacterandTokenLevelDataLoader(train_dataset, batch_size=batch_size, drop_last=True, shuffle=False)\n",
    "# test_dataloader = CharacterandTokenLevelDataLoader(test_dataset, batch_size=batch_size, drop_last=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X,y = next(iter(test_dataset))\n",
    "# print(f'X.character_length: {X.character_length}')\n",
    "# print(f'X.num_tokens: {X.num_tokens}')\n",
    "# print(f'X.token_indices: {X.token_indices}')\n",
    "# print(f'len(X.token_indices): {len(X.token_indices)}')\n",
    "# print(f'sum(X.token_lengths): {sum(X.token_lengths)}')\n",
    "# print(f'X.token_embeddings.shape: {X.token_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[255066], token_positions=[49831], character_length=[128], num_tokens=[128], token_indices=[255066], token_lengths=[49831], token_embeddings=[49831, 64], batch=[255066], ptr=[129], cumulative_token_indices=[255066])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATv2Conv, SimpleConv\n",
    "\n",
    "class GCNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, *args, **kwargs):\n",
    "        super(GCNN, self).__init__(*args, **kwargs)\n",
    "        # self.hidden_dim = hidden_dim\n",
    "        self.gnn = GATv2Conv(hidden_dim, hidden_dim//8, heads=4)\n",
    "        # self.gnn = SimpleConv(aggr='mean')\n",
    "        self.conv = nn.Conv1d(hidden_dim, hidden_dim//2, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(hidden_dim//2, hidden_dim)\n",
    "        # self.out_fc = nn.Linear(2*hidden_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim//2)\n",
    "        # self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim)\n",
    "        # self.bn4 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "    def forward(self, x, graph, total_token_count, return_attention_weights = False):\n",
    "        x1, edge_weights = self.gnn(x, graph.edge_index, return_attention_weights=return_attention_weights) \n",
    "        x2 = F.relu(self.conv(x[:total_token_count].T).T)#self.bn2(self.conv(x[:total_token_count].T).T))\n",
    "        x3 =  F.leaky_relu_(self.bn3(self.fc(x1[total_token_count:])))\n",
    "        x1 = F.leaky_relu_(self.bn1(x1[:total_token_count]))\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        x = torch.cat([x, x3], dim=0)\n",
    "        # x = self.bn4(x)\n",
    "        return x, edge_weights #F.leaky_relu_(self.bn4(self.out_fc(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx, to_undirected\n",
    "\n",
    "class GenGraph(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, virtual_nodes, lattice_step, *args, **kwargs):\n",
    "        super(GenGraph, self).__init__(*args, **kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.virtual_nodes = virtual_nodes\n",
    "        self.lattice_step = lattice_step\n",
    "        self.virtual_node_embeddings = nn.Embedding(self.virtual_nodes, hidden_dim)\n",
    "        \n",
    "    def gen_graph(self, x, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance=2):\n",
    "        random_links, lattice_links, tc_range = self.calculate_graph(x, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance)\n",
    "        v_n_e_counts = total_token_coutns*self.virtual_nodes\n",
    "        base_numel = random_links.numel() + lattice_links.numel()*2\n",
    "        edge_index = torch.empty((2, base_numel + v_n_e_counts*2), dtype=torch.int64, device=x.device)\n",
    "        self.fill_lattice_and_random_edges(edge_index, random_links, lattice_links, tc_range)\n",
    "        # for i in range(0, lattice_links.shape[1]*2, step=2):\n",
    "        #     edge_index[:, i*lattice_links.shape[0]:(i+1)*lattice_links.shape[0]] = torch.cat([lattice_links[:,i].view(1,-1), tc_range], dim=0)\n",
    "        #     edge_index[:, (i+1)*lattice_links.shape[0]:(i+2)*lattice_links.shape[0]] = edge_index[:, i*lattice_links.shape[0]:(i+1)*lattice_links.shape[0]][[1, 0]]\n",
    "        # for i in range(lattice_links.shape[1]*2, lattice_links.shape[1]*2+random_links.shape[1]):\n",
    "        #     edge_index[:, i*random_links.shape[0]:(i+1)*random_links.shape[0]] = torch.cat([random_links[:,i].view(1,-1), tc_range], dim=0)\n",
    "            \n",
    "        if self.virtual_nodes > 0:\n",
    "            virtual_nodes_range = torch.arange(self.virtual_nodes, device=x.device).view(1, -1)\n",
    "            virtual_nodes_ids = torch.repeat_interleave(virtual_nodes_range, len(token_counts), dim=0)\n",
    "            v_n_idx = (virtual_nodes_ids + torch.arange(0, len(token_counts)*self.virtual_nodes, self.virtual_nodes, device=x.device).view(-1, 1) + total_token_coutns )\n",
    "            virtual_edge_ids = torch.repeat_interleave(v_n_idx.view(-1), token_counts.view(-1, 1).expand(len(token_counts), self.virtual_nodes).reshape(-1), dim=0).view(1, -1)\n",
    "            \n",
    "            embs = self.virtual_node_embeddings(virtual_nodes_ids.T).view(-1, self.hidden_dim)\n",
    "            x_extended = torch.cat([x, embs], dim=0)\n",
    "            x_index = torch.arange(total_token_coutns, device=x.device).repeat(self.virtual_nodes).view(1, -1)\n",
    "            edge_index[:, base_numel:base_numel+v_n_e_counts] = torch.cat([x_index, virtual_edge_ids], dim=0)\n",
    "            edge_index[:, base_numel+v_n_e_counts:] = torch.cat([virtual_edge_ids, x_index], dim=0)\n",
    "            x = x_extended\n",
    "        \n",
    "        return Batch.from_data_list([Data(x=x, edge_index=edge_index)])\n",
    "        \n",
    "    def re_gen_graph(self, x, edge_index, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance=2):\n",
    "        random_links, lattice_links, tc_range = self.calculate_graph(x, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance)\n",
    "        base_numel = random_links.numel() + lattice_links.numel()*2\n",
    "        \n",
    "        self.fill_lattice_and_random_edges(edge_index, random_links, lattice_links, tc_range)\n",
    "        # for i in range(base.shape[1]):\n",
    "        #     edge_index[:, i*base.shape[0]:(i+1)*base.shape[0]] = torch.cat([tc_range, base[:,i].view(1,-1)], dim=0)\n",
    "            \n",
    "        return Batch.from_data_list([Data(x=x, edge_index=edge_index)])\n",
    "    \n",
    "    def replace_unimportant_edges(self, edge_weights, x, edge_index, total_token_coutns, token_counts, random_edges, lattice_edges, p_keep=1, lattice_start_distance=2):\n",
    "        v_n_e_counts = total_token_coutns*self.virtual_nodes\n",
    "        if v_n_e_counts>0:\n",
    "            important_indices = torch.topk(edge_weights[:-2*v_n_e_counts].view(-1, total_token_coutns), p_keep, dim=0).indices\n",
    "        else:\n",
    "            important_indices = torch.topk(edge_weights.view(-1, total_token_coutns), p_keep, dim=0).indices\n",
    "        # important_indices = torch.topk(edge_weights[:-1*total_token_coutns].view(-1, total_token_coutns), 1, dim=0).indices.squeeze()\n",
    "\n",
    "        important_indices = torch.arange(total_token_coutns, dtype=torch.int64, device=x.device) + important_indices*total_token_coutns\n",
    "        important_indices = important_indices.view(-1)\n",
    "        random_links, lattice_links, tc_range = self.calculate_graph(x, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance)\n",
    "        base_numel = random_links.numel() + lattice_links.numel()*2\n",
    "        \n",
    "        new_edge_index = torch.empty((2, base_numel + important_indices.shape[0] + 2*v_n_e_counts), dtype=torch.int64, device=x.device)\n",
    "        self.fill_lattice_and_random_edges(new_edge_index, random_links, lattice_links, tc_range)\n",
    "        new_edge_index[:, base_numel:base_numel+important_indices.shape[0]] = edge_index[:, important_indices]\n",
    "        if(self.virtual_nodes>0):\n",
    "            new_edge_index[:, -2*v_n_e_counts:] = edge_index[:, -2*v_n_e_counts:]\n",
    "            \n",
    "        # for i in range(base.shape[1]):\n",
    "        #     new_edge_index[:, i*base.shape[0]:(i+1)*base.shape[0]] = torch.cat([tc_range, base[:,i].view(1,-1)], dim=0)\n",
    "        \n",
    "        return Batch.from_data_list([Data(x=x, edge_index=new_edge_index)])\n",
    "         \n",
    "    def calculate_graph(self, x, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance):\n",
    "        tc_extended = torch.repeat_interleave(token_counts, token_counts, dim=0).view(-1,1)\n",
    "        tc_lower_bound = torch.empty((len(token_counts)+1), dtype=torch.long, device=x.device) #torch.cuda.IntTensor(len(token_counts)+1) #\n",
    "        tc_lower_bound[0] = 0\n",
    "        tc_lower_bound[1:] = torch.cumsum(token_counts, dim=0)\n",
    "        tc_lower_bound_extended = torch.repeat_interleave(tc_lower_bound[:-1], token_counts, dim=0).view(-1,1)\n",
    "        tc_range = torch.arange(tc_lower_bound[-1], device=x.device).view(-1,1)\n",
    "        # torch.arange(tc_lower_bound[-1], dtype=torch.int32, device=x.device).view(-1,1)\n",
    "        random_ints = torch.randint(0, 2*total_token_coutns, (total_token_coutns, random_edges), device=x.device) # torch.cuda.IntTensor(len(token_lengths), random_edges).random_()\n",
    "        lattice = torch.arange(lattice_start_distance, self.lattice_step*lattice_edges+1, self.lattice_step, device=x.device).view(1, -1)\n",
    "\n",
    "        # exponentials = torch.pow(2, torch.arange(1, self.exp_edges+1, device=x.device)).view(1, -1)\n",
    "        tc_local_range = tc_range - tc_lower_bound_extended\n",
    "        random_links = (((random_ints % (tc_extended - 1))+1 + tc_local_range) % tc_extended)+tc_lower_bound_extended\n",
    "        lattice_links = ((lattice + tc_local_range) % tc_extended)+tc_lower_bound_extended\n",
    "        # base = torch.cat([base1, base2], dim=1)\n",
    "        tc_range = tc_range.view(1,-1)\n",
    "        return random_links, lattice_links, tc_range\n",
    "    \n",
    "    def fill_lattice_and_random_edges(self, edge_index, random_links, lattice_links, tc_range):\n",
    "        for i in range(0, lattice_links.shape[1]*2, 2):\n",
    "            edge_index[:, i*lattice_links.shape[0]:(i+1)*lattice_links.shape[0]] = torch.cat([lattice_links[:,i//2].view(1,-1), tc_range], dim=0)\n",
    "            edge_index[:, (i+1)*lattice_links.shape[0]:(i+2)*lattice_links.shape[0]] = edge_index[:, i*lattice_links.shape[0]:(i+1)*lattice_links.shape[0]][[1, 0]]\n",
    "            \n",
    "        for i in range(random_links.shape[1]):\n",
    "            j = i + lattice_links.shape[1]*2\n",
    "            edge_index[:, j*random_links.shape[0]:(j+1)*random_links.shape[0]] = torch.cat([random_links[:,i].view(1,-1), tc_range], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.max(X.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding = nn.Embedding(len(vocab_dict), 64)\n",
    "# x = embedding(X.x)\n",
    "# x = scatter_mean(x.T, X.cumulative_token_indices, dim=1)\n",
    "\n",
    "# graph_generator = GenGraph(64, 1, 2)\n",
    "# graph = graph_generator.gen_graph(x.T, len(X.token_lengths), X.num_tokens, 2, 2)\n",
    "# gcnn1 = GCNN(64)\n",
    "# x, edge_weights = gcnn1(graph.x, graph, len(X.token_lengths), return_attention_weights = True)\n",
    "# edge_weights = edge_weights[1][:graph.edge_index.shape[1], 0]\n",
    "# # edge_weights = torch.sum(edge_weights[1][:graph.edge_index.shape[1], :],dim=1)\n",
    "# graph = graph_generator.replace_unimportant_edges(edge_weights, x, graph.edge_index, len(X.token_lengths), X.num_tokens, 4, 4, p_keep=2, lattice_start_distance=3)\n",
    "# graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights.T[0].view(-1, len(X.token_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r_v = torch.randn(5, 6)\n",
    "# torch.topk(r_v, 2, dim=0)\n",
    "# r_v\n",
    "# len(graph.x)\n",
    "# edge_weights.view(-1, len(X.token_lengths))\n",
    "# important_indices = torch.topk(weights.T[0][:-1*len(X.token_lengths)].view(-1, len(X.token_lengths)), 1, dim=0).indices.squeeze()\n",
    "# print(important_indices)\n",
    "# print(torch.arange(important_indices.shape[0]) + important_indices*important_indices.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATv2Conv, SimpleConv, summary\n",
    "\n",
    "class CNN_for_Text(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embedding, batch_size, max_token_count, embedding_dim=64, hidden_dim=64, dropout=0.3, num_out_features=4, seed=-1, random_edges=4, lattice_edges=10, virtual_nodes=1, lattice_step=2, lattice_start_distance=2, *args, **kwargs) -> None:\n",
    "        super(CNN_for_Text, self).__init__(*args, **kwargs)\n",
    "        self.batch_size = batch_size\n",
    "        self.max_token_count = max_token_count\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.virtual_nodes = virtual_nodes\n",
    "        self.base_random_edges = random_edges\n",
    "        self.base_lattice_edges = lattice_edges\n",
    "        self.lattice_start_distance = lattice_start_distance\n",
    "        if seed>-1:\n",
    "            torch.manual_seed(seed)\n",
    " \n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embedding, embedding_dim)\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.positional_encoding = nn.Embedding(max_token_count, embedding_dim)\n",
    "        self.positional_encoding.weight = self.create_positional_encoding()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(embedding_dim, hidden_dim, kernel_size=5, padding=2)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv1d(2*hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "\n",
    "        # self.conv5 = nn.Conv1d(2*hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.gcnn1 = GCNN(hidden_dim)\n",
    "        self.gcnn2 = GCNN(2*hidden_dim)\n",
    "        self.graph_generator = GenGraph(hidden_dim, virtual_nodes, lattice_step)\n",
    "        \n",
    "        k = 32\n",
    "        self.fc0 =  nn.Linear(hidden_dim , 2*hidden_dim)\n",
    "        self.fc1 = nn.Linear(2*hidden_dim , hidden_dim * k)\n",
    "        self.fc2 = nn.Linear(hidden_dim * (2+virtual_nodes) * k , 32)\n",
    "        # self.fc3 = nn.Linear(hidden_dim * k, 32)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(32, num_out_features)\n",
    "    \n",
    "    def forward(self, g_data):\n",
    "        # rand_edges = self.get_random_edge_count(g_data.x.shape[0])\n",
    "        x = self.embedding(g_data.x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.T\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        x1 = scatter_max(x, g_data.cumulative_token_indices, dim=1)[0]\n",
    "        x2 = scatter_mean(x, g_data.cumulative_token_indices, dim=1)\n",
    "\n",
    "        x = torch.cat([x1, x2], dim=0)\n",
    "\n",
    "\n",
    "        x = F.relu(self.conv3(x)) \n",
    "        x = x.T + self.positional_encoding(g_data.token_positions)\n",
    "        # x = self.dropout(x)\n",
    "        rand_edges, lattice_edges = self.base_random_edges, self.base_lattice_edges\n",
    "        graph = self.graph_generator.gen_graph(x, len(g_data.token_lengths), g_data.num_tokens, rand_edges, lattice_edges, lattice_start_distance=self.lattice_start_distance)\n",
    "        rand_edges = rand_edges-1 # max(1, rand_edges-1)\n",
    "        lattice_edges = lattice_edges-1 # max(1, lattice_edges-1)\n",
    "        # self.gen_graph(x.T, len(g_data.token_lengths), g_data.num_tokens)\n",
    "        # x = torch.cat([graph.x, g_data.token_embeddings], dim=1)\n",
    "        \n",
    "        x, edge_weights = self.gcnn1(graph.x, graph, len(g_data.token_lengths), return_attention_weights = True)\n",
    "        # edge_weights = torch.sum(edge_weights[1][:graph.edge_index.shape[1], :],dim=1)\n",
    "        edge_weights = edge_weights[1][:graph.edge_index.shape[1], 0]\n",
    "        \n",
    "        graph = self.graph_generator.replace_unimportant_edges(edge_weights, x, graph.edge_index, len(g_data.token_lengths), g_data.num_tokens, rand_edges, lattice_edges, p_keep=2, lattice_start_distance=self.lattice_start_distance+1)\n",
    "        \n",
    "        x = torch.cat([\n",
    "            graph.x[:g_data.token_embeddings.shape[0]] + self.positional_encoding(g_data.token_positions), \n",
    "            g_data.token_embeddings + self.positional_encoding(g_data.token_positions)], dim=1)\n",
    "        x1 = F.relu(self.fc0(graph.x[g_data.token_embeddings.shape[0]:]))\n",
    "        x = torch.cat([x, x1], dim=0)\n",
    "        \n",
    "        # x = x + graph.x  /////\n",
    "        # graph = self.graph_generator.re_gen_graph(x, graph.edge_index, len(g_data.token_lengths), g_data.num_tokens)    //////\n",
    "        #self.re_gen_graph(x, graph.edge_index, len(g_data.token_lengths), g_data.num_tokens)\n",
    "        x, edge_weights = self.gcnn2(x, graph, len(g_data.token_lengths))\n",
    "        # x = x + x1  /////\n",
    "        x = F.elu_(self.fc1(x))\n",
    "        doc_token_index = torch.repeat_interleave(torch.arange(len(g_data.num_tokens), device=x.device), g_data.num_tokens)\n",
    "        x1 = scatter_max(x[:len(g_data.token_lengths)], doc_token_index, dim=0)[0]\n",
    "        x2 = scatter_mean(x[:len(g_data.token_lengths)], doc_token_index, dim=0)\n",
    "        vn_embs = x[len(g_data.token_lengths):]\n",
    "        x_for_cat = [x1, x2]\n",
    "        x_for_cat.extend([vn_embs[i*x1.shape[0]:(i+1)*x1.shape[0]] for i in range(self.virtual_nodes)])\n",
    "        x = torch.cat(x_for_cat, dim=1)\n",
    "        \n",
    "        x = F.elu_(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def create_positional_encoding(self):\n",
    "        position = torch.arange(self.max_token_count).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.hidden_dim, 2) * (-math.log(10000.0) / self.hidden_dim))\n",
    "        pe = torch.zeros(self.max_token_count, self.hidden_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return torch.nn.Parameter(pe, requires_grad=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATv2Conv, SimpleConv, summary\n",
    "\n",
    "class CNN_for_Text(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embedding, batch_size, max_token_count, embedding_dim=64, hidden_dim=64, dropout=0.3, num_out_features=4, seed=-1, random_edges=4, lattice_edges=10, virtual_nodes=1, lattice_step=2, lattice_start_distance=2, use_positional_encoder=[False, False, False], *args, **kwargs) -> None:\n",
    "        super(CNN_for_Text, self).__init__(*args, **kwargs)\n",
    "        self.batch_size = batch_size\n",
    "        self.max_token_count = max_token_count\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.virtual_nodes = virtual_nodes\n",
    "        self.base_random_edges = random_edges\n",
    "        self.base_lattice_edges = lattice_edges\n",
    "        self.lattice_start_distance = lattice_start_distance\n",
    "        self.use_positional_encoder = use_positional_encoder\n",
    "        if seed>-1:\n",
    "            torch.manual_seed(seed)\n",
    " \n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embedding, embedding_dim)\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.positional_encoding = nn.Embedding(max_token_count, embedding_dim)\n",
    "        self.positional_encoding.weight = self.create_positional_encoding()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(embedding_dim, hidden_dim, kernel_size=5, padding=2)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv1d(2*hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "\n",
    "        # self.conv5 = nn.Conv1d(2*hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.gcnn1 = GCNN(hidden_dim)\n",
    "        self.gcnn2 = GCNN(2*hidden_dim)\n",
    "        self.graph_generator = GenGraph(hidden_dim, virtual_nodes, lattice_step)\n",
    "        \n",
    "        k = 32\n",
    "        self.fc0 =  nn.Linear(hidden_dim , 2*hidden_dim)\n",
    "        self.fc1 = nn.Linear(2*hidden_dim , hidden_dim * k)\n",
    "        self.fc2 = nn.Linear(hidden_dim * (2+virtual_nodes) * k , 32)\n",
    "        # self.fc3 = nn.Linear(hidden_dim * k, 32)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(32, num_out_features)\n",
    "    \n",
    "    def forward(self, g_data):\n",
    "        # rand_edges = self.get_random_edge_count(g_data.x.shape[0])\n",
    "        x = self.embedding(g_data.x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.T\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        x1 = scatter_max(x, g_data.cumulative_token_indices, dim=1)[0]\n",
    "        x2 = scatter_mean(x, g_data.cumulative_token_indices, dim=1)\n",
    "\n",
    "        x = torch.cat([x1, x2], dim=0)\n",
    "\n",
    "\n",
    "        x = F.relu(self.conv3(x)).T\n",
    "        \n",
    "        if self.use_positional_encoder[0]:\n",
    "            x = x + self.positional_encoding(g_data.token_positions)\n",
    "        # x = self.dropout(x)\n",
    "        rand_edges, lattice_edges = self.base_random_edges, self.base_lattice_edges\n",
    "        graph = self.graph_generator.gen_graph(x, len(g_data.token_lengths), g_data.num_tokens, rand_edges, lattice_edges, lattice_start_distance=self.lattice_start_distance)\n",
    "        rand_edges = rand_edges-1 # max(1, rand_edges-1)\n",
    "        lattice_edges = lattice_edges-1 # max(1, lattice_edges-1)\n",
    "        # self.gen_graph(x.T, len(g_data.token_lengths), g_data.num_tokens)\n",
    "        # x = torch.cat([graph.x, g_data.token_embeddings], dim=1)\n",
    "        \n",
    "        x, edge_weights = self.gcnn1(graph.x, graph, len(g_data.token_lengths), return_attention_weights = True)\n",
    "        # edge_weights = torch.sum(edge_weights[1][:graph.edge_index.shape[1], :],dim=1)\n",
    "        edge_weights = edge_weights[1][:graph.edge_index.shape[1], 0]\n",
    "        \n",
    "        graph = self.graph_generator.replace_unimportant_edges(edge_weights, x, graph.edge_index, len(g_data.token_lengths), g_data.num_tokens, rand_edges, lattice_edges, p_keep=2, lattice_start_distance=self.lattice_start_distance+1)\n",
    "        \n",
    "        # x = torch.cat([graph.x[:g_data.token_embeddings.shape[0]] + self.positional_encoding(g_data.token_positions), g_data.token_embeddings + self.positional_encoding(g_data.token_positions)], dim=1)\n",
    "        \n",
    "        if self.use_positional_encoder[1]:\n",
    "            xa = graph.x[:g_data.token_embeddings.shape[0]] + self.positional_encoding(g_data.token_positions)\n",
    "        else:\n",
    "            xa = graph.x[:g_data.token_embeddings.shape[0]]\n",
    "        if self.use_positional_encoder[2]:\n",
    "            xb = g_data.token_embeddings + self.positional_encoding(g_data.token_positions)\n",
    "        else:\n",
    "            xb = g_data.token_embeddings\n",
    "        \n",
    "        x = torch.cat([xa, xb], dim=1)\n",
    "        # x = torch.cat([graph.x[:g_data.token_embeddings.shape[0]], g_data.token_embeddings], dim=1)\n",
    "        x1 = F.relu(self.fc0(graph.x[g_data.token_embeddings.shape[0]:]))\n",
    "        x = torch.cat([x, x1], dim=0)\n",
    "        \n",
    "        # x = x + graph.x  /////\n",
    "        # graph = self.graph_generator.re_gen_graph(x, graph.edge_index, len(g_data.token_lengths), g_data.num_tokens)    //////\n",
    "        #self.re_gen_graph(x, graph.edge_index, len(g_data.token_lengths), g_data.num_tokens)\n",
    "        x, edge_weights = self.gcnn2(x, graph, len(g_data.token_lengths))\n",
    "        # x = x + x1  /////\n",
    "        x = F.elu_(self.fc1(x))\n",
    "        doc_token_index = torch.repeat_interleave(torch.arange(len(g_data.num_tokens), device=x.device), g_data.num_tokens)\n",
    "        x1 = scatter_max(x[:len(g_data.token_lengths)], doc_token_index, dim=0)[0]\n",
    "        x2 = scatter_mean(x[:len(g_data.token_lengths)], doc_token_index, dim=0)\n",
    "        vn_embs = x[len(g_data.token_lengths):]\n",
    "        x_for_cat = [x1, x2]\n",
    "        x_for_cat.extend([vn_embs[i*x1.shape[0]:(i+1)*x1.shape[0]] for i in range(self.virtual_nodes)])\n",
    "        x = torch.cat(x_for_cat, dim=1)\n",
    "        \n",
    "        x = F.elu_(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def create_positional_encoding(self):\n",
    "        position = torch.arange(self.max_token_count).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.hidden_dim, 2) * (-math.log(10000.0) / self.hidden_dim))\n",
    "        pe = torch.zeros(self.max_token_count, self.hidden_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return torch.nn.Parameter(pe, requires_grad=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+------------------------------+----------------+----------+\n",
      "| Layer                                     | Input Shape                  | Output Shape   | #Param   |\n",
      "|-------------------------------------------+------------------------------+----------------+----------|\n",
      "| CNN_for_Text                              | [255066, 255066]             | [128, 12]      | 593,932  |\n",
      "| ├─(embedding)Embedding                    | [255066]                     | [255066, 128]  | 12,288   |\n",
      "| ├─(dropout)Dropout                        | [255066, 128]                | [255066, 128]  | --       |\n",
      "| ├─(positional_encoding)Embedding          | --                           | --             | 16,384   |\n",
      "| ├─(conv1)Conv1d                           | [128, 255066]                | [64, 255066]   | 41,024   |\n",
      "| ├─(pool1)MaxPool1d                        | --                           | --             | --       |\n",
      "| ├─(conv2)Conv1d                           | [64, 255066]                 | [64, 255066]   | 20,544   |\n",
      "| ├─(conv3)Conv1d                           | [128, 49831]                 | [64, 49831]    | 24,640   |\n",
      "| ├─(conv4)Conv1d                           | --                           | --             | 12,352   |\n",
      "| ├─(gcnn1)GCNN                             | [49831, 64], [49831, 49831]  | [49831, 64]    | 12,704   |\n",
      "| │    └─(gnn)GATv2Conv                     | [49831, 64], [2, 597972]     | [49831, 32]    | 4,224    |\n",
      "| │    └─(conv)Conv1d                       | [64, 49831]                  | [32, 49831]    | 6,176    |\n",
      "| │    └─(fc)Linear                         | [0, 32]                      | [0, 64]        | 2,112    |\n",
      "| │    └─(bn1)BatchNorm1d                   | [49831, 32]                  | [49831, 32]    | 64       |\n",
      "| │    └─(bn3)BatchNorm1d                   | [0, 64]                      | [0, 64]        | 128      |\n",
      "| ├─(gcnn2)GCNN                             | [49831, 128], [49831, 49831] | [49831, 128]   | 49,984   |\n",
      "| │    └─(gnn)GATv2Conv                     | [49831, 128], [2, 448479]    | [49831, 64]    | 16,640   |\n",
      "| │    └─(conv)Conv1d                       | [128, 49831]                 | [64, 49831]    | 24,640   |\n",
      "| │    └─(fc)Linear                         | [0, 64]                      | [0, 128]       | 8,320    |\n",
      "| │    └─(bn1)BatchNorm1d                   | [49831, 64]                  | [49831, 64]    | 128      |\n",
      "| │    └─(bn3)BatchNorm1d                   | [0, 128]                     | [0, 128]       | 256      |\n",
      "| ├─(graph_generator)GenGraph               | --                           | --             | --       |\n",
      "| │    └─(virtual_node_embeddings)Embedding | --                           | --             | --       |\n",
      "| ├─(fc0)Linear                             | [0, 64]                      | [0, 128]       | 8,320    |\n",
      "| ├─(fc1)Linear                             | [49831, 128]                 | [49831, 2048]  | 264,192  |\n",
      "| ├─(fc2)Linear                             | [128, 4096]                  | [128, 32]      | 131,104  |\n",
      "| ├─(fc_out)Linear                          | [128, 32]                    | [128, 12]      | 396      |\n",
      "+-------------------------------------------+------------------------------+----------------+----------+\n"
     ]
    }
   ],
   "source": [
    "model = CNN_for_Text(num_embedding=num_embedding, batch_size=batch_size, max_token_count=256, embedding_dim=128, hidden_dim=64, num_out_features=len(class_id), random_edges=4, lattice_edges=4, virtual_nodes=0)#.to(device)\n",
    "print(summary(model, X.cpu()))\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.eval()\n",
    "# with torch.no_grad():\n",
    "#     model(X.cpu()).shape #.to(device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CnnGnnClassifierLightningModel(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        num_classes,\n",
    "        optimizer=None,\n",
    "        loss_func=None,\n",
    "        learning_rate=0.01,\n",
    "        batch_size=64,\n",
    "        lr_scheduler=None,\n",
    "        user_lr_scheduler=False,\n",
    "        min_lr=0.0,\n",
    "    ):\n",
    "        super(CnnGnnClassifierLightningModel, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "        self.min_lr = min_lr\n",
    "        # self.save_hyperparameters(ignore=[\"model\"])\n",
    "        self.save_hyperparameters(\"model\", logger=False)\n",
    "        self.optimizer = self._get_optimizer(optimizer)\n",
    "        self.lr_scheduler = (\n",
    "            self._get_lr_scheduler(lr_scheduler) if user_lr_scheduler else None\n",
    "        )\n",
    "        self.loss_func = loss_func\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.model(x)\n",
    "\n",
    "    def on_train_epoch_start(self) -> None:\n",
    "        param_groups = next(iter(self.optimizer.param_groups))\n",
    "        if \"lr\" in param_groups and param_groups[\"lr\"] is not None:\n",
    "            current_learning_rate = float(param_groups[\"lr\"])\n",
    "            self.log(\n",
    "                \"lr\",\n",
    "                current_learning_rate,\n",
    "                batch_size=self.batch_size,\n",
    "                on_epoch=True,\n",
    "                on_step=False,\n",
    "            )\n",
    "\n",
    "    def training_step(self, batch, *args, **kwargs):\n",
    "        X, y = batch\n",
    "        X.to(self.device)\n",
    "        y.to(self.device)\n",
    "        \n",
    "        self.model.train()\n",
    "        y_out = self(X)\n",
    "\n",
    "        loss = self.loss_func(y_out.view(y.shape), y )\n",
    "        self.train_losses.append(loss.detach().item())\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            batch_size=self.batch_size,\n",
    "            on_epoch=True,\n",
    "            on_step=True,\n",
    "        )\n",
    "        \n",
    "        self.train_acc(torch.argmax(y_out, dim=1), torch.argmax(y, dim=1))\n",
    "        self.log('train_acc', self.train_acc, prog_bar=True, on_epoch=True, on_step=True, batch_size=self.batch_size)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, *args, **kwargs):\n",
    "        X, y = batch\n",
    "        X.to(self.device)\n",
    "        y.to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        y_out = self(X)\n",
    "        loss = self.loss_func(y_out.view(y.shape), y )\n",
    "        self.val_losses.append(loss.detach().item())\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            batch_size=self.batch_size,\n",
    "            on_epoch=True,\n",
    "            on_step=True,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.val_acc(torch.argmax(y_out, dim=1), torch.argmax(y, dim=1))\n",
    "        self.log('val_acc', self.val_acc, prog_bar=True, on_epoch=True, on_step=True, batch_size=self.batch_size)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.lr_scheduler is None:\n",
    "            return self.optimizer\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": self.optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": self.lr_scheduler,\n",
    "                \"monitor\": \"train_loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def update_learning_rate(self, learning_rate: float):\n",
    "        self.learning_rate = learning_rate\n",
    "        for g in self.optimizer.param_groups:\n",
    "            g[\"lr\"] = learning_rate\n",
    "\n",
    "    def _get_optimizer(self, optimizer):\n",
    "        return (\n",
    "            optimizer\n",
    "            if optimizer is not None\n",
    "            else torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        )\n",
    "\n",
    "    def _get_lr_scheduler(self, lr_scheduler):\n",
    "        return (\n",
    "            lr_scheduler\n",
    "            if lr_scheduler is not None\n",
    "            else torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer, patience=5, factor=0.5, mode=\"min\", min_lr=self.min_lr\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from torchmetrics import ConfusionMatrix\n",
    "\n",
    "\n",
    "def calculate_metrics(cl_model, dataloader):\n",
    "    cm = ConfusionMatrix(task=\"multiclass\", num_classes=len(class_id))\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    cl_model = cl_model.eval()\n",
    "    cl_model.to(device)\n",
    "    for X, y in tqdm(dataloader):\n",
    "        X = X.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_p = cl_model(X)\n",
    "            y_p = y_p.cpu()\n",
    "        y_pred.append(y_p)\n",
    "        y_true.append(y)\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "    y_true = torch.cat(y_true, dim=0)\n",
    "    y_pred2 = torch.argmax(y_pred, dim=1)\n",
    "    y_true2 = torch.argmax(y_true, dim=1)\n",
    "    print(f'classification report: \\n {classification_report(y_true2, y_pred2, digits=4)}')\n",
    "    print(f'confusion matrix:\\n {cm(y_pred2, y_true2)}')\n",
    "    print('================================')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 128\n",
    "hidden_dim = 64\n",
    "embedding_dim = 64\n",
    "label_size = 1\n",
    "seed = 911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\Projects\\Form&ColorIntelligence\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scripts.managers.ClassifierModelManager import ClassifierModelManager\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import lightning as L\n",
    "# from scripts.managers.ClassifierModelManager import ClassifierModelManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fardin Rastakhiz @ 2023\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from lightning.pytorch.loggers import Logger, CSVLogger\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from typing import List\n",
    "from pytorch_lightning.core.saving import save_hparams_to_yaml\n",
    "\n",
    "class ModelManager(ABC):\n",
    "\n",
    "    def __init__(self,\n",
    "                 torch_model: torch.nn.Module,\n",
    "                 lightning_model,\n",
    "                 model_save_dir: str = '~/Desktop',\n",
    "                 log_dir: str = 'logs/',\n",
    "                 log_name: str = 'model_logs',\n",
    "                 device='cpu',\n",
    "                 max_epochs = 100,\n",
    "                 ckpt_path: str|None=None,\n",
    "                 accumulate_grad_batches=1):\n",
    "        self.torch_model = torch_model\n",
    "        self.lightning_model = lightning_model\n",
    "        self.log_dir = log_dir\n",
    "        self.log_name = log_name\n",
    "        self.model_save_dir = model_save_dir\n",
    "        self.device = device\n",
    "        self.accelerator = 'cpu' if self.device=='cpu' else 'gpu'\n",
    "        self.max_epochs = max_epochs\n",
    "        self.ckpt_path = ckpt_path\n",
    "\n",
    "        self.logger = self._create_logger()\n",
    "        self.callbacks = self._create_callbacks()\n",
    "        self.trainer: L.Trainer = self._create_trainer(accumulate_grad_batches)\n",
    "        self.tuner = Tuner(self.trainer)\n",
    "        self.tuning_result = None\n",
    "\n",
    "    def tune(self, data_manager=None, train_dataloaders=None, val_dataloaders=None, datamodule=None, draw_result=True, min_lr=0.0000001, max_lr=0.1):\n",
    "        self.tuning_result = self.tuner.lr_find(self.lightning_model, datamodule=data_manager, train_dataloaders=train_dataloaders,\n",
    "                         val_dataloaders=val_dataloaders, min_lr=min_lr,max_lr=max_lr, num_training=150)\n",
    "        if draw_result:\n",
    "            fig = self.tuning_result.plot(suggest=True)\n",
    "            fig.show()\n",
    "        self.update_learning_rate(self.tuning_result.suggestion())\n",
    "        return self.tuning_result.suggestion()\n",
    "    \n",
    "    def update_learning_rate(self, lr):\n",
    "        self.lightning_model.update_learning_rate(lr)\n",
    "\n",
    "    def fit(self, train_dataloaders=None, val_dataloaders=None, datamodule=None, max_epochs = -1, ckpt_path=None):\n",
    "        if ckpt_path is not None and ckpt_path != '':\n",
    "            self.ckpt_path = ckpt_path\n",
    "        if max_epochs>0:\n",
    "            self.trainer.fit_loop.max_epochs = max_epochs\n",
    "            # self.max_epochs = max_epochs\n",
    "            # self.trainer = self._create_trainer()\n",
    "        self.trainer.fit(self.lightning_model,\n",
    "                         datamodule=datamodule,\n",
    "                         train_dataloaders=train_dataloaders,\n",
    "                         val_dataloaders=val_dataloaders,\n",
    "                         ckpt_path = self.ckpt_path\n",
    "                         )\n",
    "\n",
    "    def validate(self, dataloaders=None, datamodule=None):\n",
    "        return self.trainer.validate(self.lightning_model,\n",
    "                             datamodule=datamodule,\n",
    "                             dataloaders=dataloaders)\n",
    "\n",
    "    def predict(self, dataloaders=None, datamodule=None):\n",
    "        return self.trainer.predict(self.lightning_model,\n",
    "                             datamodule=datamodule,\n",
    "                             dataloaders=dataloaders)\n",
    "\n",
    "    def _create_trainer(self, accumulate_grad_batches) -> L.Trainer:\n",
    "        return L.Trainer(\n",
    "            callbacks=self.callbacks,\n",
    "            max_epochs=self.max_epochs,\n",
    "            accelerator=self.accelerator,\n",
    "            logger=self.logger,\n",
    "            num_sanity_val_steps=0,\n",
    "            default_root_dir=self.model_save_dir,\n",
    "            accumulate_grad_batches=accumulate_grad_batches\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def _create_callbacks(self) -> List[Callback]:\n",
    "        pass\n",
    "\n",
    "    def _create_logger(self) -> Logger:\n",
    "        return CSVLogger(save_dir=self.log_dir, name=self.log_name)\n",
    "\n",
    "    @abstractmethod\n",
    "    def draw_summary(self, dataloader):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def plot_csv_logger(self, loss_names, eval_names):\n",
    "        pass\n",
    "    \n",
    "    def save_hyper_parameters(self):\n",
    "        mhparams = {\n",
    "            'start_lr': 0.045,\n",
    "            'ckpt_lrs' :  {51: 0.002, 65: 0.00058},\n",
    "            'last_lr' : 0.0003,\n",
    "            'ac_loss_factor': 0.0002,\n",
    "            'weight_decay': 0.0012\n",
    "        }\n",
    "        save_hparams_to_yaml(config_yaml=r'logs\\hetero_model_17_AG\\version_12\\hparams.yaml',\n",
    "                     hparams=mhparams)\n",
    "        \n",
    "    # def find_best_settings(data_manager,\n",
    "    #                        lrs: List[float]=[0.001], dropouts: List[float]=[0.2], \n",
    "    #                        weight_decays: List[float]=[0.00055], emb_factors: List[float]=[0.1], \n",
    "    #                        batch_sizes: List[int]=[128], log_name='find_best_settings'):\n",
    "    #     for lr in lrs:\n",
    "    #         for dropout in dropouts:\n",
    "    #             for wd in weight_decays:\n",
    "    #                 for emb_factor in emb_factors:\n",
    "    #                     for bs in batch_sizes:\n",
    "    #                         data_manager.update_batch_size(bs)\n",
    "    #                         torch_model = HeteroGcnGatModel1(300, 1, X1.metadata(), 128, dropout=dropout)\n",
    "    #                         lightning_model = HeteroBinaryLightningModel(torch_model,\n",
    "    #                                         torch.optim.Adam(torch_model.parameters(), lr=lr, weight_decay=wd),\n",
    "    #                                             loss_func=HeteroLoss1(exception_keys='word', enc_factor=emb_factor),\n",
    "    #                                             learning_rate=lr,\n",
    "    #                                             batch_size=bs,\n",
    "    #                                             user_lr_scheduler=True\n",
    "    #                                             ).to(device)\n",
    "    #                         model_manager = ClassifierModelManager(torch_model, lightning_model, log_name=log_name, device=device, num_train_epoch=10)\n",
    "    #                         model_manager.fit(datamodule=data_manager)\n",
    "    #                         model_manager.save_plot_csv_logger(name_prepend=f'{lr}_{dropout}_{wd}_{emb_factor}_{bs}', loss_names=['train_loss', 'val_loss'], eval_names=['train_acc_epoch', 'val_acc_epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fardin Rastakhiz @ 2023\n",
    "import torch\n",
    "# from scripts.managers.ModelManager import ModelManager\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from torch_geometric.nn import summary\n",
    "from lightning.pytorch.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from os import path\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, hinge_loss\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "class ClassifierModelManager(ModelManager):\n",
    "\n",
    "    def __init__(self,\n",
    "                 torch_model: torch.nn.Module,\n",
    "                 lightning_model,\n",
    "                 model_save_dir: str = '~/Desktop',\n",
    "                 log_dir: str = 'logs/',\n",
    "                 log_name: str = 'model_logs',\n",
    "                 device='cpu',\n",
    "                 num_train_epoch = 100,\n",
    "                 accumulate_grad_batches=1):\n",
    "        super(ClassifierModelManager, self).__init__(torch_model, lightning_model, model_save_dir, log_dir, log_name, device, num_train_epoch, accumulate_grad_batches=accumulate_grad_batches)\n",
    "\n",
    "    def _create_callbacks(self) -> List[Callback]:\n",
    "        return [\n",
    "            ModelCheckpoint(save_top_k=2, mode='max', monitor='val_acc', save_last=True),\n",
    "            # EarlyStopping(patience=50, mode='max', monitor='val_acc')\n",
    "        ]\n",
    "\n",
    "    def draw_summary(self, dataloader):\n",
    "        X, y = next(iter(dataloader))\n",
    "        print(summary(self.torch_model, X.to(self.device)))\n",
    "\n",
    "    def plot_csv_logger(self, loss_names=['train_loss', 'val_loss'], eval_names=['train_acc', 'val_acc']):\n",
    "        csv_path = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', 'metrics.csv')\n",
    "        metrics = pd.read_csv(csv_path)\n",
    "\n",
    "        aggregation_metrics = []\n",
    "        agg_col = 'epoch'\n",
    "        for i, dfg in metrics.groupby(agg_col):\n",
    "            agg = dict(dfg.mean())\n",
    "            agg[agg_col] = i\n",
    "            aggregation_metrics.append(agg)\n",
    "\n",
    "        df_metrics = pd.DataFrame(aggregation_metrics)\n",
    "        df_metrics[loss_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='loss')\n",
    "        df_metrics[eval_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='accuracy')\n",
    "        plt.show()\n",
    "\n",
    "    def save_plot_csv_logger(self, loss_names=['train_loss', 'val_loss'], eval_names=['train_acc', 'val_acc'], name_prepend: str=\"\"):\n",
    "        csv_path = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', 'metrics.csv')\n",
    "        metrics = pd.read_csv(csv_path)\n",
    "\n",
    "        aggregation_metrics = []\n",
    "        agg_col = 'epoch'\n",
    "        for i, dfg in metrics.groupby(agg_col):\n",
    "            agg = dict(dfg.mean())\n",
    "            agg[agg_col] = i\n",
    "            aggregation_metrics.append(agg)\n",
    "\n",
    "        df_metrics = pd.DataFrame(aggregation_metrics)\n",
    "        df_metrics[loss_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='loss')\n",
    "        \n",
    "        loss_png = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', f'{name_prepend}_loss_metric.png')\n",
    "        plt.savefig(loss_png)\n",
    "        \n",
    "        df_metrics[eval_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='accuracy')\n",
    "        \n",
    "        acc_png = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', f'{name_prepend}_acc_metric.png')\n",
    "        plt.savefig(acc_png)\n",
    "        \n",
    "        plt.close()\n",
    "    \n",
    "    def evaluate(self, eval_dataloader,\n",
    "                 give_confusion_matrix: bool=True, \n",
    "                 give_report: bool=True, \n",
    "                 give_f1_score: bool=False, \n",
    "                 give_accuracy_score: bool=False, \n",
    "                 give_precision_score: bool=False, \n",
    "                 give_recall_score: bool=False, \n",
    "                 give_hinge_loss: bool=False):\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        self.lightning_model.eval()\n",
    "        for X, y in eval_dataloader:\n",
    "            y_p = self.lightning_model(X.to(self.device))\n",
    "            if type(y_p) is tuple:\n",
    "                y_p = y_p[0]\n",
    "            y_pred.append((y_p>0).to(torch.int32).detach().to(y.device))\n",
    "            y_true.append(y.to(torch.int32))\n",
    "        y_true = torch.concat(y_true)\n",
    "        y_pred = torch.concat(y_pred)\n",
    "        if(give_confusion_matrix):\n",
    "            print(f'confusion_matrix: \\n{confusion_matrix(y_true, y_pred)}')\n",
    "        if(give_report):\n",
    "            print(classification_report(y_true, y_pred))\n",
    "        if(give_f1_score):\n",
    "            print(f'f1_score: {f1_score(y_true, y_pred)}')\n",
    "        if(give_accuracy_score):\n",
    "            print(f'accuracy_score: {accuracy_score(y_true, y_pred)}')\n",
    "        if(give_precision_score):\n",
    "            print(f'precision_score: {precision_score(y_true, y_pred)}')\n",
    "        if(give_recall_score):\n",
    "            print(f'recall_score: {recall_score(y_true, y_pred)}')\n",
    "        if(give_hinge_loss):\n",
    "            print(f'hinge_loss: {hinge_loss(y_true, y_pred)}')\n",
    "                \n",
    "    def evaluate_best_models(self, lightning_type: L.LightningModule, eval_dataloader,\n",
    "                             give_confusion_matrix: bool=True, \n",
    "                             give_report: bool=True, \n",
    "                             give_f1_score: bool=False, \n",
    "                             give_accuracy_score: bool=False, \n",
    "                             give_precision_score: bool=False, \n",
    "                             give_recall_score: bool=False, \n",
    "                             give_hinge_loss: bool=False,\n",
    "                             multi_class: bool=False, **kwargs):\n",
    "        self.lightning_model = lightning_type.load_from_checkpoint(rf'{self.trainer.checkpoint_callback.best_model_path}', map_location=None, hparams_file=None, strict=True, **kwargs).eval()\n",
    "        self.save_evaluation(eval_dataloader, 'best_model', give_confusion_matrix, give_report,\n",
    "                             give_f1_score, give_accuracy_score, give_precision_score, give_recall_score, give_hinge_loss, multi_class)\n",
    "            \n",
    "    def save_evaluation(self, eval_dataloader, name_prepend: str='',\n",
    "                    give_confusion_matrix: bool=True, \n",
    "                    give_report: bool=True, \n",
    "                    give_f1_score: bool=False, \n",
    "                    give_accuracy_score: bool=False, \n",
    "                    give_precision_score: bool=False, \n",
    "                    give_recall_score: bool=False, \n",
    "                    give_hinge_loss: bool=False,\n",
    "                    multi_class: bool=False\n",
    "                    ):\n",
    "            \n",
    "            test_metrics_path = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', f'{name_prepend}_test_metrics.txt')\n",
    "            \n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            self.lightning_model.eval()\n",
    "            self.lightning_model.model.eval()\n",
    "            self.torch_model.eval()\n",
    "            self.trainer.model.eval()\n",
    "            for X, y in eval_dataloader:\n",
    "                with torch.no_grad():\n",
    "                    y_p = self.lightning_model(X.to(self.device))\n",
    "                if type(y_p) is tuple:\n",
    "                    y_p = y_p[0]\n",
    "                \n",
    "                if multi_class:\n",
    "                    y_pred.append(y_p.detach().to(y.device))\n",
    "                    y_true.append(y)\n",
    "                else:\n",
    "                    y_pred.append((y_p>0).to(torch.int32).detach().to(y.device))\n",
    "                    y_true.append(y.to(torch.int32))\n",
    "                    \n",
    "            y_true = torch.concat(y_true)\n",
    "            y_pred = torch.concat(y_pred)\n",
    "            print(y_true.shape)\n",
    "            print(y_pred.shape)\n",
    "            if multi_class:\n",
    "                y_true_num = torch.argmax(y_true, dim=1)\n",
    "                y_pred_num = torch.argmax(y_pred, dim=1)\n",
    "            else:\n",
    "                y_true_num = y_true\n",
    "                y_pred_num = y_pred\n",
    "                \n",
    "            print(y_true_num.shape)\n",
    "            print(y_pred_num.shape)\n",
    "            with open(test_metrics_path, 'at+') as f:\n",
    "                if(give_confusion_matrix):\n",
    "                    print(f'confusion_matrix: \\n{confusion_matrix(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_report):\n",
    "                    print(classification_report(y_true_num, y_pred_num), file=f)\n",
    "                if(give_f1_score):\n",
    "                    if multi_class:\n",
    "                        print(f'f1_score: {f1_score(y_true_num, y_pred_num, average=None)}', file=f)\n",
    "                    else:\n",
    "                        print(f'f1_score: {f1_score(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_accuracy_score):\n",
    "                    print(f'accuracy_score: {accuracy_score(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_precision_score):\n",
    "                    if multi_class:\n",
    "                        print(f'precision: {precision_score(y_true_num, y_pred_num, average=None)}', file=f)\n",
    "                    else:\n",
    "                        print(f'precision: {precision_score(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_recall_score):\n",
    "                    if multi_class:\n",
    "                        print(f'recall: {recall_score(y_true_num, y_pred_num, average=None)}', file=f)\n",
    "                    else:\n",
    "                        print(f'recall: {recall_score(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_hinge_loss):\n",
    "                    print(f'hinge_loss: {hinge_loss(y_true_num, y_pred)}', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "def train_model(epochs=30, dropout=0.2, weight_decay=0.00001, lr=0.0002, use_positional_encoder=[False, False, False]):\n",
    "    classifier_torch_model = CNN_for_Text(num_embedding=num_embedding, batch_size=batch_size, hidden_dim=hidden_dim, embedding_dim=embedding_dim, max_token_count=max_doc_length, dropout=dropout, num_out_features=len(class_id), seed=seed, random_edges=4, lattice_edges=4, lattice_step=2, virtual_nodes=0, lattice_start_distance=2, use_positional_encoder=use_positional_encoder).to(device)\n",
    "    optimizer = torch.optim.Adam(classifier_torch_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 100, 150, 200, 250, 300, 350],gamma=0.5, verbose=False)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 30, 32, 40, 42, 45, 50, 75, 100],gamma=0.5, verbose=False)\n",
    "    loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "    classfier_lightning_model = CnnGnnClassifierLightningModel(classifier_torch_model, \n",
    "                                                        num_classes=len(class_id),\n",
    "                                                learning_rate=lr,\n",
    "                                                batch_size=batch_size,\n",
    "                                                optimizer=optimizer,\n",
    "                                                loss_func=loss_func,\n",
    "                                                lr_scheduler=lr_scheduler,\n",
    "                                                user_lr_scheduler=True\n",
    "                                                ).to(device)\n",
    "\n",
    "\n",
    "    model_manager = ClassifierModelManager(classifier_torch_model, classfier_lightning_model, log_name='CNN-GNN9_positional_encoding',device=device, num_train_epoch=epochs, accumulate_grad_batches=1)\n",
    "    # trainer = L.Trainer(\n",
    "    #             # callbacks=callbacks,\n",
    "    #             max_epochs=epochs,\n",
    "    #             accelerator= 'gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    #             logger=CSVLogger(save_dir='logs/', name='log2'), \n",
    "    #             num_sanity_val_steps=0,\n",
    "    #         #     default_root_dir='models\\model2_word_embedding-256-2'\n",
    "    #         )\n",
    "\n",
    "    model_manager.fit(train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)\n",
    "    model_manager.save_plot_csv_logger(loss_names=['train_loss_epoch', 'val_loss_epoch'], eval_names=['train_acc_epoch', 'val_acc_epoch'], name_prepend=f'spacy_embedding_tests_{dropout}_{weight_decay}_{lr}')\n",
    "    model_manager.torch_model = model_manager.torch_model.to(device)\n",
    "    model_manager.save_evaluation(test_dataloader, f'{dropout}_{weight_decay}_{lr}_[{use_positional_encoder[0]},{use_positional_encoder[1]}, {use_positional_encoder[2]}]',True, True, True, True, True, True, True, multi_class=True)\n",
    "    # trainer.fit(classfier_lightning_model, train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)\n",
    "    classfier_lightning_model.model = classfier_lightning_model.model.eval()\n",
    "    classfier_lightning_model = classfier_lightning_model.eval()\n",
    "    calculate_metrics(classfier_lightning_model, test_dataloader)\n",
    "    model_manager.evaluate_best_models(CnnGnnClassifierLightningModel, test_dataloader,True, True, True, True, True, True, True, multi_class=True, model=classifier_torch_model, num_classes=len(class_id))\n",
    "    return model_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "# print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_use_positional_encoder=[\n",
    "    [False, False, False],\n",
    "    [True, False, False],\n",
    "    [False, True, False],\n",
    "    [False, False, True],\n",
    "    [True, True, False],\n",
    "    [True, False, True],\n",
    "    [False, True, True],\n",
    "    [True, True, True]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for upe in all_use_positional_encoder:\n",
    "    for i in range(5):\n",
    "        model_mgr = train_model(30, 0.2, 0.000012, 0.0032, upe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Use regular expressions to extract numbers\n",
    "\n",
    "\n",
    "# Convert the list of strings to integers\n",
    "\n",
    "\n",
    "# Reshape the list into a 2D array\n",
    "\n",
    "\n",
    "print(array[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "def calculate_accuracy(confusion_matrix):\n",
    "    diagonal_sum = confusion_matrix.trace()\n",
    "    total_sum = confusion_matrix.sum()\n",
    "    accuracy = diagonal_sum / total_sum\n",
    "    return accuracy\n",
    "\n",
    "def calculate_f1_score(confusion_matrix):\n",
    "    # Calculate precision\n",
    "    precision = np.diag(confusion_matrix) / np.sum(confusion_matrix, axis=0)\n",
    "    # Calculate recall\n",
    "    recall = np.diag(confusion_matrix) / np.sum(confusion_matrix, axis=1)\n",
    "    \n",
    "    accuracy = np.sum(np.diag(confusion_matrix)) / np.sum(confusion_matrix)\n",
    "    \n",
    "    # Ignore division by zero errors\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        # Calculate F1-score\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    # Replace NaNs with zeros\n",
    "    f1_score = np.nan_to_num(f1_score)\n",
    "    \n",
    "    # Average F1-scores for all classes (macro F1-score)\n",
    "    macro_f1_score = np.mean(f1_score)\n",
    "    \n",
    "    return macro_f1_score, np.mean(precision), np.mean(recall), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False]: 0.8866956114768982\n",
      "[True, False, False]: 0.8717352151870728\n",
      "[False, True, False]: 0.865287446975708\n",
      "[False, False, True]: 0.8872195720672608\n",
      "[True, True, False]: 0.9016591668128967\n",
      "[True, False, True]: 0.881246280670166\n",
      "[False, True, True]: 0.929870891571045\n",
      "[True, True, True]: 0.8552929997444153\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "all_use_positional_encoder=[\n",
    "    [False, False, False],\n",
    "    [True, False, False],\n",
    "    [False, True, False],\n",
    "    [False, False, True],\n",
    "    [True, True, False],\n",
    "    [True, False, True],\n",
    "    [False, True, True],\n",
    "    [True, True, True]\n",
    "]\n",
    "\n",
    "for i in range(len(all_use_positional_encoder)):\n",
    "    list_of_metric = np.zeros((5, ))\n",
    "    \n",
    "    for j in range(5):\n",
    "        k = i*5 + j\n",
    "        file_path = fr'logs\\CNN-GNN9_positional_encoding\\version_{k}\\best_model_test_metrics.txt'\n",
    "        with open(file_path, 'rt') as f:\n",
    "            all_lines = f.readlines()\n",
    "            all_lines = '\\n'.join([l for l in all_lines[1:13]])\n",
    "        numbers = re.findall(r'\\d+', all_lines)\n",
    "        numbers = [int(num) for num in numbers]\n",
    "        array = np.array(numbers).reshape(12, 12)\n",
    "        \n",
    "        _, _, _, _  = calculate_f1_score(array)\n",
    "        \n",
    "        with open(file_path, 'rt') as f:\n",
    "            all_lines = f.readlines()\n",
    "            list_of_metric[j] = float(re.findall(r\"[-+]?(?:\\d*\\.*\\d+)\", all_lines[39])[0])\n",
    "        # all_numbers = re.findall(r\"[-+]?(?:\\d*\\.*\\d+)\", all_lines[28])\n",
    "    print(f'{all_use_positional_encoder[i]}: {np.mean(list_of_metric)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False]: 0.8058819345729418\n",
      "[True, False, False]: 0.8183530710496264\n",
      "[False, True, False]: 0.8118894756486521\n",
      "[False, False, True]: 0.8143845496448414\n",
      "[True, True, False]: 0.8153780260387069\n",
      "[True, False, True]: 0.8165760205534346\n",
      "[False, True, True]: 0.8076799462849106\n",
      "[True, True, True]: 0.8162460730553475\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_use_positional_encoder)):\n",
    "    list_of_metric = np.zeros((5, ))\n",
    "    for j in range(5):\n",
    "        k = i*5 + j\n",
    "        file_path = fr'logs\\CNN-GNN9_positional_encoding\\version_{k}\\best_model_test_metrics.txt'\n",
    "        with open(file_path, 'rt') as f:\n",
    "            all_lines = f.readlines()\n",
    "            all_lines = '\\n'.join([l for l in all_lines[1:13]])\n",
    "        numbers = re.findall(r'\\d+', all_lines)\n",
    "        numbers = [int(num) for num in numbers]\n",
    "        array = np.array(numbers).reshape(12, 12)\n",
    "        # all_numbers = re.findall(r\"[-+]?(?:\\d*\\.*\\d+)\", all_lines[28])\n",
    "        list_of_metric[j] = calculate_f1_score(array)\n",
    "    print(f'{all_use_positional_encoder[i]}: {np.mean(list_of_metric)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     print(lightning_model.model.embedding.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=len(class_id))\n",
    "# lightning_model = CnnGnnClassifierLightningModel.load_from_checkpoint(r'logs\\CNN-GNN9_positional_encoding\\version_30\\checkpoints\\epoch=1-step=466.ckpt', num_classes=len(class_id), map_location=None, hparams_file=None, strict=True).eval().to(device)\n",
    "# optimizer = torch.optim.Adam(lightning_model.model.parameters(), lr=0.0032, weight_decay=0.000012)\n",
    "# loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# for i in range(50):\n",
    "#     lightning_model.model.train()\n",
    "#     j = 0\n",
    "#     batch_accuracies = []\n",
    "#     for X, y in tqdm(train_dataloader):\n",
    "#         X = X.to(device)\n",
    "#         y = y.to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         y_out = lightning_model.model(X)\n",
    "#         if True in torch.isnan(y_out):\n",
    "#             print(f'Has nan for batch number {j}')\n",
    "#             print(f'X: {X.cpu()}')\n",
    "#             print(f'y: {y.cpu()}')\n",
    "#             print(f'y_out: {y_out.cpu()}')\n",
    "#             raise Exception(f'Has nan for batch number {j}')\n",
    "#         j += 1\n",
    "#         loss = loss_func(y_out.view(y.shape), y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_acc(torch.argmax(y_out.cpu(), dim=1), torch.argmax(y.cpu(), dim=1))\n",
    "        \n",
    "#         batch_accuracies.append(train_acc.compute().item())\n",
    "#     print(np.mean(np.array(batch_accuracies)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/CNN-GNN8_HTML_Spacy\\\\version_60\\\\checkpoints\\\\epoch=15-step=1872.ckpt'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mgr.trainer.checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropouts = [0.2]\n",
    "# start_lrs = [0.0035, 0.0037, 0.0039]\n",
    "# wds = [0.000012]#, 0.000013, 0.0000125]\n",
    "# for dp in dropouts:\n",
    "#     for lr in start_lrs:\n",
    "#         for wd in wds:\n",
    "#             model_mgr = train_model(100, dp, wd, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier_torch_model = CNN_for_Text(num_embedding=len(vocab_dict), batch_size=batch_size, hidden_dim=hidden_dim, embedding_dim=embedding_dim, max_char_count=256, dropout=0.176, num_out_features=len(class_id), seed=911, g_node_degree=25, virtual_nodes=1).to(device)\n",
    "# optimizer = torch.optim.Adam(classifier_torch_model.parameters(), lr=lr, weight_decay=0.00001)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 100, 150, 200, 250, 300, 350],gamma=0.5, verbose=False)\n",
    "# loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "# classfier_lightning_model = ClassifierLightningModel(classifier_torch_model, \n",
    "#                                                     num_classes=len(class_id),\n",
    "#                                             learning_rate=lr,\n",
    "#                                             batch_size=batch_size,\n",
    "#                                             optimizer=optimizer,\n",
    "#                                             loss_func=loss_func,\n",
    "#                                             lr_scheduler=lr_scheduler,\n",
    "#                                             user_lr_scheduler=True\n",
    "#                                             ).to(device)\n",
    "\n",
    "# trainer = L.Trainer(\n",
    "#             max_epochs=50,\n",
    "#             accelerator='gpu',\n",
    "#         #     logger=self.logger,\n",
    "#             num_sanity_val_steps=0,\n",
    "#         #     default_root_dir=self.model_save_dir\n",
    "#         )\n",
    "# # model_manager = ClassifierModelManager(classifier_torch_model, classfier_lightning_model, log_name='CNN-GNN',device=device, num_train_epoch=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightning.pytorch.tuner import Tuner\n",
    "# tuner = Tuner(trainer)\n",
    "# tuning_result = tuner.lr_find(classfier_lightning_model, train_dataloaders=train_dataloader, val_dataloaders=test_dataloader, min_lr=0.00001,max_lr=0.1, num_training=100)\n",
    "\n",
    "# fig = tuning_result.plot(suggest=True)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = model_mgr.trainer.model.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.5859e-03,  4.0568e-04,  1.5457e-03],\n",
      "         [-4.6530e-03, -3.5093e-03, -7.9185e-03],\n",
      "         [-6.7445e-06, -1.0661e-03, -4.8041e-04],\n",
      "         ...,\n",
      "         [-1.8301e-03, -3.0299e-03, -1.6987e-03],\n",
      "         [-1.5803e-03, -3.0021e-03, -1.6618e-03],\n",
      "         [-1.5129e-03, -3.0795e-03, -1.6778e-03]],\n",
      "\n",
      "        [[-1.8611e-02, -2.4336e-02, -1.7940e-02],\n",
      "         [-1.2113e-02, -1.3636e-02, -1.0038e-02],\n",
      "         [-3.3944e-02, -2.5293e-02, -2.8411e-02],\n",
      "         ...,\n",
      "         [-1.0011e-02, -1.0661e-02, -1.0458e-02],\n",
      "         [-9.7561e-03, -1.0256e-02, -9.9880e-03],\n",
      "         [-1.0762e-02, -1.0156e-02, -1.0146e-02]],\n",
      "\n",
      "        [[ 2.3748e-04, -7.9370e-03, -4.8272e-03],\n",
      "         [-1.7340e-02, -1.4321e-02, -7.0624e-03],\n",
      "         [-3.0633e-02, -1.8230e-02, -2.6760e-02],\n",
      "         ...,\n",
      "         [-1.5387e-02, -1.3403e-02, -1.2101e-02],\n",
      "         [-1.5907e-02, -1.4880e-02, -1.3240e-02],\n",
      "         [-1.4117e-02, -1.2760e-02, -1.6694e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.1362e-02, -3.8102e-02, -3.8871e-02],\n",
      "         [-8.5064e-03, -1.3271e-02, -2.3503e-02],\n",
      "         [-5.2185e-02, -3.8843e-02, -4.7424e-02],\n",
      "         ...,\n",
      "         [-5.0054e-02, -5.6935e-02, -1.1502e-02],\n",
      "         [-2.5772e-02, -4.3679e-02, -4.9731e-02],\n",
      "         [-3.5175e-02, -1.1335e-02, -2.6141e-02]],\n",
      "\n",
      "        [[-1.5194e-10,  6.2239e-11, -3.7658e-09],\n",
      "         [-3.3713e-09, -3.1818e-09, -1.2055e-08],\n",
      "         [-4.4756e-09, -4.4946e-09, -2.1800e-09],\n",
      "         ...,\n",
      "         [-2.1188e-09, -2.0067e-09, -2.0556e-09],\n",
      "         [-2.0290e-09, -2.0017e-09, -1.9847e-09],\n",
      "         [-2.1222e-09, -2.0549e-09, -2.0813e-09]],\n",
      "\n",
      "        [[-6.0264e-07, -4.5557e-09, -4.1110e-06],\n",
      "         [-5.2245e-07, -1.1073e-08, -1.3686e-06],\n",
      "         [-8.1524e-28, -7.4413e-28,  4.9565e-25],\n",
      "         ...,\n",
      "         [-1.8118e-22, -1.0839e-22, -1.2305e-21],\n",
      "         [ 5.8531e-23, -4.9612e-22, -2.4743e-23],\n",
      "         [-5.0322e-22, -7.0546e-22, -1.9542e-22]]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(torch_model.gcnn2.conv.weight[torch.topk(torch.amax(torch_model.gcnn2.conv.weight, dim=[1, 2]), 16).indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([-1.2447e-09, -3.1937e-08, -4.8926e-08, -1.3328e-06, -1.4419e-06],\n",
       "       device='cuda:0', grad_fn=<TopkBackward0>),\n",
       "indices=tensor([45, 35, 15, 44,  8], device='cuda:0'))"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(torch.mean(torch_model.gcnn2.conv.weight, dim=[1,2]), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0219,  0.0631,  0.0708, -0.1806, -0.0510, -0.0279,  0.0320,  0.0383,\n",
       "         0.0140,  0.0665, -0.0600, -0.2229, -0.4666,  0.1677, -0.0167, -0.2121,\n",
       "        -0.3531, -0.0469, -0.0419,  0.0017, -0.0114, -0.0997, -0.0214, -0.0817,\n",
       "        -0.0107, -0.1290,  0.0237, -0.2218,  0.0676, -0.1563,  0.0965,  0.0842,\n",
       "         0.0813, -0.3827,  0.0661, -0.0840,  0.0560, -0.0665,  0.0518, -0.5597,\n",
       "         0.0371, -0.0294,  0.0286, -0.0230, -0.0536, -0.0712, -0.1645,  0.0377,\n",
       "         0.0138, -0.0564,  0.0515, -0.0242,  0.1301, -0.0617,  0.0122, -0.0658,\n",
       "         0.0048, -0.0688,  0.0078, -0.0628,  0.0633, -0.1881,  0.0978,  0.3663],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model.gcnn1.gnn.lin_l.weight[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.2702e-07,  2.7896e-07, -2.5949e-06],\n",
       "        [ 1.1807e-05,  9.0392e-06,  1.0132e-05],\n",
       "        [-2.1984e-05,  1.4411e-05, -4.2878e-05],\n",
       "        [-1.0756e-05,  2.1367e-05, -2.3552e-05],\n",
       "        [-6.6925e-05, -2.3981e-05, -7.5533e-05],\n",
       "        [ 5.1804e-06,  7.2347e-06,  1.7205e-06],\n",
       "        [ 3.9877e-05,  1.0202e-05,  1.8564e-05],\n",
       "        [-8.1309e-05, -8.7911e-05, -6.4120e-05],\n",
       "        [-5.7794e-05, -4.4700e-05, -9.3490e-05],\n",
       "        [ 6.7567e-06, -1.8901e-06, -5.8325e-07],\n",
       "        [-2.0610e-06, -2.5011e-06, -4.8132e-06],\n",
       "        [ 6.9594e-07,  1.1810e-06,  6.4741e-07],\n",
       "        [ 3.5025e-05, -1.0883e-05,  3.5249e-05],\n",
       "        [ 6.8693e-06,  4.4552e-06,  3.1595e-06],\n",
       "        [-2.8348e-05, -3.8623e-06, -1.7717e-05],\n",
       "        [-5.9693e-05, -9.1237e-05, -5.0126e-05],\n",
       "        [-3.5822e-05, -3.0066e-05, -3.7751e-06],\n",
       "        [-9.9738e-05, -1.3992e-04, -9.1665e-05],\n",
       "        [-1.0202e-06,  5.7738e-05, -8.6722e-06],\n",
       "        [ 1.3780e-04,  1.3465e-04,  6.7533e-05],\n",
       "        [ 1.0222e-05,  3.8819e-07,  1.8926e-05],\n",
       "        [ 3.3648e-06, -3.2111e-06,  4.3801e-06],\n",
       "        [-7.0083e-06,  2.3532e-06, -4.4270e-06],\n",
       "        [-3.6780e-05, -2.6963e-05,  1.3400e-06],\n",
       "        [ 4.1433e-05,  5.8982e-05,  1.7125e-05],\n",
       "        [-6.8199e-06, -3.5100e-05, -2.3131e-05],\n",
       "        [-1.9565e-05, -3.0115e-05, -1.1942e-05],\n",
       "        [-4.4977e-05, -5.4827e-06, -2.7378e-05],\n",
       "        [-1.4235e-05,  5.1231e-06, -8.6039e-06],\n",
       "        [-1.1545e-07, -1.0412e-05, -2.1609e-05],\n",
       "        [ 2.5685e-05, -8.5861e-06,  2.1908e-05],\n",
       "        [ 1.5646e-05,  1.4218e-05,  1.3060e-05],\n",
       "        [-4.8052e-40, -4.8215e-40, -4.4767e-40],\n",
       "        [ 4.1908e-40,  7.9456e-40, -4.0919e-40],\n",
       "        [-6.2599e-40, -7.8188e-40, -1.3730e-39],\n",
       "        [-4.1112e-40, -4.1307e-40, -1.0086e-20],\n",
       "        [-5.9389e-40,  4.7570e-40, -4.1031e-40],\n",
       "        [-6.5840e-02, -7.5632e-02, -7.6874e-02],\n",
       "        [-2.9258e-40,  1.0271e-39,  6.9612e-41],\n",
       "        [ 6.0637e-40,  4.1364e-40,  4.0978e-40],\n",
       "        [ 2.2754e-05,  2.6293e-05,  1.6894e-05],\n",
       "        [-4.0950e-40, -4.3788e-40,  4.4479e-40],\n",
       "        [ 4.3661e-40, -4.8769e-40, -4.4212e-40],\n",
       "        [ 6.3676e-40, -5.7714e-40, -4.4492e-40],\n",
       "        [-4.7857e-40, -1.4658e-40, -8.4415e-40],\n",
       "        [ 6.9643e-40,  5.1962e-40,  4.3741e-40],\n",
       "        [-4.2881e-40,  4.0939e-40, -8.1705e-40],\n",
       "        [-7.1238e-22, -4.9605e-20, -3.5086e-20],\n",
       "        [ 1.1512e-39, -7.4231e-40, -5.8537e-40],\n",
       "        [-4.8717e-40, -4.0957e-40, -5.2448e-40],\n",
       "        [-2.4365e-40, -1.1856e-39, -6.0729e-40],\n",
       "        [-4.3256e-40, -2.5772e-40,  4.9299e-40],\n",
       "        [-4.9757e-40,  4.6254e-40, -4.2097e-40],\n",
       "        [-4.9985e-40, -4.3159e-40, -5.0121e-40],\n",
       "        [-6.0499e-02, -6.3477e-02, -5.8355e-02],\n",
       "        [-4.4273e-40,  4.1606e-40,  6.4583e-41],\n",
       "        [-5.0054e-40, -4.3899e-40,  5.1484e-40],\n",
       "        [ 5.1335e-40, -5.2431e-40,  8.7283e-40],\n",
       "        [ 1.1145e-39,  4.0972e-40,  6.8564e-40],\n",
       "        [ 3.7699e-40,  3.4504e-39,  6.5172e-40],\n",
       "        [ 4.7379e-40, -4.6202e-40, -4.4791e-40],\n",
       "        [ 4.8985e-40, -4.8044e-40,  4.2450e-40],\n",
       "        [ 4.5104e-40, -5.2487e-40, -4.3571e-40],\n",
       "        [ 4.9890e-40, -4.9039e-40, -4.5935e-40],\n",
       "        [-2.9050e-05, -2.8226e-05, -2.1920e-05],\n",
       "        [-2.2903e-05, -2.1028e-05, -2.0321e-05],\n",
       "        [-2.0126e-05, -1.9671e-05, -2.3297e-05],\n",
       "        [-2.2079e-05, -2.3708e-05, -1.9168e-05],\n",
       "        [-2.0900e-05, -1.6661e-05, -2.0401e-05],\n",
       "        [-2.3955e-05, -2.1425e-05, -1.3048e-05],\n",
       "        [-1.9615e-05, -1.9442e-05, -2.3333e-05],\n",
       "        [-3.0899e-05, -2.0651e-05,  4.5978e-06],\n",
       "        [-1.9578e-05, -2.0607e-05, -2.2279e-05],\n",
       "        [-2.2442e-05, -2.2418e-05, -2.0905e-05],\n",
       "        [-1.6827e-05, -2.1580e-05, -2.0546e-05],\n",
       "        [-2.3018e-05, -2.1884e-05, -2.1312e-05],\n",
       "        [-2.2111e-05, -2.1154e-05, -2.1532e-05],\n",
       "        [-2.2281e-05, -2.0485e-05, -2.1747e-05],\n",
       "        [-2.3107e-05, -1.7629e-05, -2.4030e-05],\n",
       "        [-2.2521e-05, -2.2116e-05, -2.2357e-05],\n",
       "        [-1.9222e-05, -2.2339e-05, -2.2306e-05],\n",
       "        [-2.0429e-05, -2.1109e-05, -2.1382e-05],\n",
       "        [-2.0219e-05, -2.0676e-05, -2.3209e-05],\n",
       "        [-2.0860e-05, -2.1412e-05, -2.1971e-05],\n",
       "        [-1.9768e-05, -2.1762e-05, -2.0871e-05],\n",
       "        [-2.2347e-05, -2.0864e-05, -2.1920e-05],\n",
       "        [-2.1661e-05, -2.2055e-05, -2.1368e-05],\n",
       "        [-2.0582e-05, -2.3293e-05, -2.0854e-05],\n",
       "        [-2.2763e-05, -2.1485e-05, -2.0956e-05],\n",
       "        [-2.1500e-05, -2.1903e-05, -2.0946e-05],\n",
       "        [-2.2567e-05, -2.0038e-05, -2.2294e-05],\n",
       "        [-2.1747e-05, -2.2917e-05, -2.0830e-05],\n",
       "        [-2.5087e-05, -2.3127e-05, -1.9995e-05],\n",
       "        [-2.0186e-05, -2.2119e-05, -2.1262e-05],\n",
       "        [-1.9342e-05, -2.2519e-05, -2.0964e-05],\n",
       "        [-1.9936e-05, -2.1272e-05, -2.1276e-05],\n",
       "        [-2.2144e-05, -2.1657e-05, -2.3003e-05],\n",
       "        [-2.1159e-05, -2.0159e-05, -2.3178e-05],\n",
       "        [-2.0614e-05, -2.1985e-05, -1.1859e-05],\n",
       "        [-2.5095e-05, -2.2235e-05, -1.9504e-05],\n",
       "        [-2.1536e-05, -2.2380e-05, -2.0517e-05],\n",
       "        [-3.0848e-05, -2.0589e-05, -2.1478e-05],\n",
       "        [-2.1061e-05, -2.1979e-05, -2.0664e-05],\n",
       "        [-2.1570e-05, -2.2097e-05, -1.9886e-05],\n",
       "        [-2.2905e-05, -2.1137e-05, -2.1098e-05],\n",
       "        [-2.2150e-05, -2.0943e-05, -2.1568e-05],\n",
       "        [-2.6553e-05, -2.0359e-05, -2.1113e-05],\n",
       "        [-2.2496e-05, -2.1705e-05, -2.1542e-05],\n",
       "        [-2.0606e-05, -1.9761e-05, -2.3295e-05],\n",
       "        [-2.1623e-05, -2.1166e-05, -2.1932e-05],\n",
       "        [-1.9872e-05, -2.0692e-05, -2.2033e-05],\n",
       "        [-2.1520e-05, -2.2939e-05, -2.1680e-05],\n",
       "        [-2.2386e-05, -2.1122e-05, -2.2627e-05],\n",
       "        [-2.0480e-05, -2.0287e-05, -2.1227e-05],\n",
       "        [-1.9809e-05, -2.2346e-05, -2.1349e-05],\n",
       "        [-2.0565e-05, -1.0091e-05, -2.1360e-05],\n",
       "        [-1.9132e-05, -2.0726e-05, -2.1314e-05],\n",
       "        [-2.2136e-05, -2.2087e-05, -1.8755e-05],\n",
       "        [-2.0834e-05, -2.7117e-05, -2.2549e-05],\n",
       "        [-2.4627e-05, -1.8307e-05, -1.9770e-05],\n",
       "        [-1.8403e-05, -2.0651e-05, -2.0935e-05],\n",
       "        [-2.0154e-05, -2.0820e-05, -2.2423e-05],\n",
       "        [-2.3472e-05, -2.2898e-05, -2.0113e-05],\n",
       "        [-2.0477e-05, -1.9639e-05, -2.1734e-05],\n",
       "        [-2.4353e-05, -2.1576e-05, -2.0113e-05],\n",
       "        [-2.7241e-05, -2.4991e-05, -1.9430e-05],\n",
       "        [-2.0170e-05, -2.0934e-05, -2.1503e-05],\n",
       "        [-2.7724e-05, -2.2267e-05, -2.0276e-05]], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model.gcnn2.conv.weight[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "14\n",
      "16\n",
      "34\n",
      "47\n",
      "53\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "for i in range(64):\n",
    "    if torch.max(torch_model.gcnn2.conv.weight[i])> 0.0001:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_for_Text(\n",
       "  (embedding): Embedding(96, 64)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (positional_encoding): Embedding(1024, 64)\n",
       "  (conv1): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (conv3): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (conv4): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (gcnn1): GCNN(\n",
       "    (gnn): GATv2Conv(64, 8, heads=4)\n",
       "    (conv): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (fc): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (gcnn2): GCNN(\n",
       "    (gnn): GATv2Conv(128, 16, heads=4)\n",
       "    (conv): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (fc): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (graph_generator): GenGraph(\n",
       "    (virtual_node_embeddings): Embedding(0, 64)\n",
       "  )\n",
       "  (fc0): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (fc1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=32, bias=True)\n",
       "  (fc_out): Linear(in_features=32, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for d in dropouts:\n",
    "#     for wd in weight_decays:\n",
    "#         for lr in lrs:\n",
    "#             train_model(50, d, wd, lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
