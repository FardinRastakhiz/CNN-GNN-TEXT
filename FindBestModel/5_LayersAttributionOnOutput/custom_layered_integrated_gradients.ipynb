{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (20) must match the size of tensor b (10) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\CGNet\\FindBestModel\\5_LayersAttributionOnOutput\\custom_layered_integrated_gradients.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/custom_layered_integrated_gradients.ipynb#X11sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m target_class_idx \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m  \u001b[39m# Target class index\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/custom_layered_integrated_gradients.ipynb#X11sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m \u001b[39m# Compute Layered Integrated Gradients\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/custom_layered_integrated_gradients.ipynb#X11sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m lig_results \u001b[39m=\u001b[39m lig\u001b[39m.\u001b[39;49mcompute_integrated_gradients(input_tensor, target_class_idx)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/custom_layered_integrated_gradients.ipynb#X11sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mIntegrated Gradients:\u001b[39m\u001b[39m\"\u001b[39m, lig_results)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/custom_layered_integrated_gradients.ipynb#X11sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m lig\u001b[39m.\u001b[39mremove_hooks()\n",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\CGNet\\FindBestModel\\5_LayersAttributionOnOutput\\custom_layered_integrated_gradients.ipynb Cell 1\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/custom_layered_integrated_gradients.ipynb#X11sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39m# Average over the steps and scale by input difference\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/custom_layered_integrated_gradients.ipynb#X11sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m integrated_grads \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m steps\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/custom_layered_integrated_gradients.ipynb#X11sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39mreturn\u001b[39;00m integrated_grads \u001b[39m*\u001b[39;49m (input_tensor \u001b[39m-\u001b[39;49m baseline)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (20) must match the size of tensor b (10) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Module\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class LayerIntegratedGradients:\n",
    "    \n",
    "    def __init__(self, model: Module, layer: Module):\n",
    "        \"\"\"\n",
    "        Initialize the Layer Integrated Gradients instance.\n",
    "\n",
    "        Parameters:\n",
    "        model: PyTorch model\n",
    "            The model to analyze.\n",
    "        layer: PyTorch layer\n",
    "            The layer to analyze gradients for.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.layer = layer\n",
    "        self.hook = None\n",
    "        self.activations = None\n",
    "        self.gradients = None\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        \"\"\"Register forward and backward hooks to capture activations and gradients.\"\"\"\n",
    "        def forward_hook(module, inp, out):\n",
    "            self.activations = out\n",
    "\n",
    "        def backward_hook(module, grad_inp, grad_out):\n",
    "            self.gradients = grad_out[0]\n",
    "\n",
    "        self.hook = self.layer.register_forward_hook(forward_hook)\n",
    "        self.layer.register_full_backward_hook(backward_hook)\n",
    "\n",
    "    def compute_integrated_gradients(self, input_tensor, target_class_idx, baseline=None, steps=50):\n",
    "        \"\"\"\n",
    "        Compute integrated gradients for a specific input and target class.\n",
    "\n",
    "        Parameters:\n",
    "        input_tensor: torch.Tensor\n",
    "            The input tensor to analyze.\n",
    "        target_class_idx: int\n",
    "            Index of the target class for which gradients are calculated.\n",
    "        baseline: torch.Tensor, optional\n",
    "            Baseline tensor for IG. If None, a zero tensor of the same shape as input is used.\n",
    "        steps: int\n",
    "            Number of steps for the Riemann approximation.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Integrated gradients for the layer's activations.\n",
    "        \"\"\"\n",
    "        if baseline is None:\n",
    "            baseline = torch.zeros_like(input_tensor)\n",
    "\n",
    "        # Scale inputs from baseline to the actual input\n",
    "        scaled_inputs = torch.stack([\n",
    "            baseline + (float(i) / steps) * (input_tensor - baseline) for i in range(steps + 1)\n",
    "        ])\n",
    "\n",
    "        integrated_grads = None\n",
    "        for scaled_input in scaled_inputs:\n",
    "            self.model.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = self.model(scaled_input)\n",
    "\n",
    "            # Select the target class\n",
    "            target = output[:, target_class_idx].sum()\n",
    "\n",
    "            # Backward pass\n",
    "            target.backward(retain_graph=True)\n",
    "\n",
    "            # Accumulate gradients wrt activations\n",
    "            if integrated_grads is None:\n",
    "                integrated_grads = self.gradients.clone()\n",
    "            else:\n",
    "                integrated_grads += self.gradients\n",
    "\n",
    "        # Average over the steps and scale by input difference\n",
    "        integrated_grads /= steps\n",
    "        return integrated_grads * (input_tensor - baseline)\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Remove the registered hooks.\"\"\"\n",
    "        if self.hook:\n",
    "            self.hook.remove()\n",
    "\n",
    "# Example usage with a custom PyTorch or PyTorch-Geometric model\n",
    "class CustomModel(Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(10, 20)\n",
    "        self.fc2 = torch.nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Create a toy model and dummy input\n",
    "model = CustomModel()\n",
    "layer = model.fc1\n",
    "lig = LayerIntegratedGradients(model, layer)\n",
    "\n",
    "input_tensor = torch.randn(1, 10, requires_grad=True)\n",
    "target_class_idx = 1  # Target class index\n",
    "\n",
    "# Compute Layered Integrated Gradients\n",
    "lig_results = lig.compute_integrated_gradients(input_tensor, target_class_idx)\n",
    "print(\"Integrated Gradients:\", lig_results)\n",
    "\n",
    "lig.remove_hooks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
