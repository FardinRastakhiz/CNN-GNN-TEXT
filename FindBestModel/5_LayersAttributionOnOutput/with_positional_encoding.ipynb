{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import time\n",
    "from torch_scatter import scatter_max, scatter_mean, scatter_std\n",
    "from sklearn.model_selection import KFold\n",
    "import torchmetrics\n",
    "import lightning as L\n",
    "from torch_geometric.data import Batch, Data\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "from transformers import AutoModel, DebertaV2Tokenizer, AutoTokenizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_vocab = {v:k for k,v in tokenizer.vocab.items()}\n",
    "all_vocab_indices = list(id_vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# all_vocab_str = []\n",
    "# vector_keys = list(nlp.vocab.vectors.keys())\n",
    "# for i in range(len(vector_keys)):\n",
    "#     try:\n",
    "#         t = nlp.vocab.strings[vector_keys[i]]\n",
    "#         all_vocab_str.append(t)\n",
    "#     except:\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# Create a set of all English characters, numbers, and punctuation\n",
    "allowed_chars = string.ascii_letters + string.digits + string.punctuation\n",
    "all_chars = set(allowed_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'Data\\ReducedEmbeddings\\deberta_larg_reduced_embeddings_64.npy', 'rb') as f:\n",
    "    embeddings = np.load(f)\n",
    "embeddings = torch.from_numpy(embeddings)\n",
    "all_vocab_str = []\n",
    "for i in range(len(id_vocab)):\n",
    "    all_vocab_str.append(id_vocab[i])\n",
    "    \n",
    "# embeddings = (embeddings - torch.min(embeddings)) / (torch.max(embeddings)-torch.min(embeddings))\n",
    "token_vocab_dict = dict(zip(all_vocab_str, embeddings))\n",
    "# del all_vocab_str\n",
    "# del all_vocab_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(r'Data\\ReducedEmbeddings\\polarity_debertav3_tokens_gpt_mini_emb.npy', 'rb') as f:\n",
    "    polarities_subjectivities= np.load(f)\n",
    "polarities_subjectivities = torch.from_numpy(polarities_subjectivities)\n",
    "polarity_vocab_dict = dict(zip(all_vocab_str, polarities_subjectivities))\n",
    "polarity_vocab_dict['<n>'] = torch.tensor([0.0, 0.0])\n",
    "len(token_vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128001, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarities_subjectivities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1085 tensor([0.7000, 0.6000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_vocab_str)):\n",
    "    if 'nice' in all_vocab_str[i]:\n",
    "        print(i, polarities_subjectivities[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# Create a set of all English characters, numbers, and punctuation\n",
    "allowed_chars = string.ascii_letters + string.digits + string.punctuation + ' \\t'\n",
    "all_chars = set(allowed_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 224\n",
    "folder_path = r'C:\\Users\\fardin\\Projects\\CGNet\\Data\\TextClassification\\IMDB'\n",
    "# t_tokenizer = TweetTokenizer()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_ratio = 0.05\n",
    "test_df = pd.read_csv(r'data\\TextClassification\\IMDB\\test.csv')\n",
    "test_df['Topic'] = test_df['label']\n",
    "# test_df['Content'] = [test_df.text[i].values[0] + \" \\n \" + test_df.text[i].values[1] for i in range(len(test_df))]\n",
    "test_df['Content'] = test_df['text']\n",
    "# test_df['Content'] = test_df['Content']\n",
    "test_df.drop(['label', 'text'], axis=1, inplace=True)\n",
    "test_df.dropna(inplace=True)\n",
    "test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "test_df = test_df.iloc[:int(keep_ratio*test_df.shape[0])]\n",
    "train_df = pd.read_csv(r'data\\TextClassification\\IMDB\\train.csv')\n",
    "train_df['Topic'] = train_df['label']\n",
    "# train_df['Content'] = [train_df.text[i].values[0] + \" \\n \" + train_df.text[i].values[1] for i in range(len(train_df))]\n",
    "train_df['Content'] = train_df['text']\n",
    "train_df.drop(['label', 'text'], axis=1, inplace=True)\n",
    "train_df.dropna(inplace=True)\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "train_df = train_df.iloc[:int(keep_ratio*train_df.shape[0])]\n",
    "sst_classes = [\"Negative\", \"Positive\"]\n",
    "df = pd.DataFrame(np.concatenate([train_df.values, test_df.values]), columns=train_df.columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12988\n",
      "CPU times: total: 78.1 ms\n",
      "Wall time: 69 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_lengths = np.array([len(df.Content[i]) for i in df.index])\n",
    "print(np.max(doc_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_list = df.Topic.unique()\n",
    "class_id = {c:i for i, c in enumerate(sst_classes)}\n",
    "id_class = {i:c for i, c in enumerate(sst_classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_dict = {c:i for i, c in enumerate(allowed_chars)}\n",
    "# if '\\x01' not in vocab_dict:\n",
    "#     vocab_dict['\\x01'] = len(vocab_dict)\n",
    "# char_Set = set(vocab_dict.keys())\n",
    "# len(char_Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12988\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a histogram\n",
    "plt.hist(doc_lengths, bins='auto', edgecolor='black')\n",
    "plt.xlim([0,14000])\n",
    "# Add labels and title\n",
    "plt.xlabel('String Lengths in Character')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of String Lengths')\n",
    "print(doc_lengths.max())\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5.62 s\n",
      "Wall time: 6.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_token_lengths = np.array([len(tokenizer.tokenize(df.Content[i])) for i in df.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3088\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a histogram\n",
    "plt.hist(doc_token_lengths, bins='auto', edgecolor='black')\n",
    "plt.xlim([0,2000])\n",
    "# Add labels and title\n",
    "plt.xlabel('String Lengths in Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of String Lengths')\n",
    "print(doc_token_lengths.max())\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5.92 s\n",
      "Wall time: 6.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "char_set = set()\n",
    "for doc in df.Content.values:\n",
    "    char_set.update(set(' '.join(tokenizer.tokenize(doc))))\n",
    "    char_set.update(set(doc))\n",
    "len(char_set)\n",
    "\n",
    "allowed_chars = string.ascii_letters + string.digits + string.punctuation + ' '\n",
    "all_chars = set(''.join(char_set).join(allowed_chars))\n",
    "vocab_dict = {c:i for i, c in enumerate(all_chars)}\n",
    "if '\\x01' not in vocab_dict:\n",
    "    vocab_dict['\\x01'] = len(vocab_dict)\n",
    "char_Set = set(vocab_dict.keys())\n",
    "num_embedding = len(vocab_dict)\n",
    "vocab_dict_rev = dict(zip(list(vocab_dict.values()), list(vocab_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterandTokenLevelCustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, num_classes, char_dict, token_dict, sentiment_dict, tokenizer, shuffle=True, batch_size=128) -> None:\n",
    "        super().__init__()\n",
    "        # y = y[indices].values\n",
    "        # y = torch.from_numpy(np.array([class_id[c] for c in y], dtype=np.longlong))\n",
    "        \n",
    "        # print(f'self.num_sections1: {len(y) // batch_size}')\n",
    "        \n",
    "        if len(y) % batch_size != 0:\n",
    "            self.shortage = ((len(y) // batch_size)+1)*batch_size - len(y)\n",
    "            empty_labels = [i%2 for i in range(self.shortage)]\n",
    "            empty_strings = [id_class[l] for l in empty_labels]\n",
    "            \n",
    "            # print(f'y1 - {y.shape}: {y}')\n",
    "            y = np.concatenate([y, empty_labels])\n",
    "            # print(f'y2 - {y.shape}: {y}')\n",
    "            # print(f'X1 - {X.shape}: {X}')\n",
    "            X = np.concatenate([X, empty_strings])\n",
    "        #     print(f'X2 - {X.shape}: {X}')\n",
    "        \n",
    "        # print(f'self.num_sections2: {len(y) // batch_size}')\n",
    "        \n",
    "        y = torch.from_numpy(y)\n",
    "        self.shuffle = shuffle\n",
    "        self.y = torch.nn.functional.one_hot(y, num_classes=num_classes).float()\n",
    "        self.X = X\n",
    "        self.char_dict = char_dict\n",
    "        self.char_Set = set(char_dict.keys())\n",
    "        self.vocab_size = len(self.char_dict)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_dict = token_dict\n",
    "        self.sentiment_dict = sentiment_dict\n",
    "        self.max_token_count = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.all_data = []\n",
    "        self.token_lengths = []\n",
    "        self.token_embeddign_ids = []\n",
    "        \n",
    "        self.sum_a = 0\n",
    "        \n",
    "        for doc in tqdm(self.X):\n",
    "            g_data = self.content_to_graph(doc)\n",
    "            self.all_data.append(g_data)\n",
    "        \n",
    "        \n",
    "        self.num_sections = len(y) // batch_size\n",
    "        self.x_lengths = np.array([self.all_data[i].character_length for i in range(len(self.all_data))])\n",
    "        self.x_len_args = np.argsort(self.x_lengths)[::-1]\n",
    "        \n",
    "        self.section_ranges = np.linspace(0, len(self.x_len_args), self.num_sections+1)\n",
    "        self.section_ranges = [(int(self.section_ranges[i-1]), int(self.section_ranges[i])) for i in range(1, len(self.section_ranges))]\n",
    "\n",
    "        self.position_j = 0\n",
    "        self.section_i = 0\n",
    "        self.epoch = 0\n",
    "        self.each_section_i = np.zeros((self.num_sections, ), dtype=int)\n",
    "        \n",
    "        self.sections, self.section_size = self.split_into_k_groups(self.x_len_args, self.x_lengths, self.num_sections)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        index = self.get_section_index()\n",
    "        return self.all_data[index], self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def get_section_index(self):\n",
    "        # if self.shuffle:\n",
    "            \n",
    "        #     t_range = self.section_ranges[self.section_i]\n",
    "        #     target_index = np.random.randint(t_range[0], t_range[1])\n",
    "        # else:\n",
    "        #     t_range = self.section_ranges[self.section_i]\n",
    "        #     target_index = t_range[0] + self.each_section_i[self.section_i]\n",
    "        #     self.each_section_i[self.section_i] = (self.each_section_i[self.section_i] + 1) % (t_range[1] - t_range[0])\n",
    "        # print()\n",
    "        # print(f'self.section_i: {self.section_i},   self.position_j: {self.position_j}')\n",
    "        target_index = self.sections[self.section_i, self.position_j]\n",
    "        \n",
    "        self.position_j = (self.position_j + 1) % self.section_size\n",
    "        if self.position_j == 0:\n",
    "            self.section_i = (self.section_i + 1) % self.num_sections\n",
    "            if self.shuffle and self.section_i == 0:\n",
    "                self.sections, self.section_size = self.split_into_k_groups(self.x_len_args, self.x_lengths, self.num_sections)\n",
    "                # random_positions = np.random.choice(np.arange(0, self.section_size), size=self.section_size, replace=False)\n",
    "        # return self.x_len_args[target_index]\n",
    "        return target_index\n",
    "\n",
    "    def reset_params(self):\n",
    "        self.section_i = 0\n",
    "        self.position_j = 0\n",
    "        self.each_section_i = np.zeros((self.num_sections, ), dtype=int)\n",
    "        \n",
    "    def split_into_k_groups(self, len_sorted_args, lengths:np.array, k):\n",
    "        if self.shuffle and self.epoch > 0:\n",
    "            randomize_sections = np.concatenate([np.random.choice(np.arange(r[0], r[1]), size=r[1]-r[0], replace=False) for r in self.section_ranges])\n",
    "            len_sorted_args = len_sorted_args[randomize_sections]\n",
    "        \n",
    "        nums = lengths[len_sorted_args]\n",
    "        groups_size = len(len_sorted_args) // k\n",
    "        \n",
    "        \n",
    "        groups = [[] for _ in range(k)]\n",
    "        group_sums = np.zeros(k, dtype=int)\n",
    "        group_sizes = np.zeros(k, dtype=int)\n",
    "        \n",
    "        # print(f'groups_size: {groups_size}')\n",
    "        # print(f'len(len_sorted_args): {len(len_sorted_args)}')\n",
    "        # print(f'k: {k}')\n",
    "        for i, num in enumerate(nums):\n",
    "            candidate_indices = np.where(group_sizes<groups_size)[0]\n",
    "            # print(f'candidate_indices: {candidate_indices}')\n",
    "            min_group_idx = candidate_indices[np.argmin(group_sums[candidate_indices])]\n",
    "            groups[min_group_idx].append(len_sorted_args[i])\n",
    "            group_sums[min_group_idx] += num\n",
    "            group_sizes[min_group_idx] += 1\n",
    "        self.epoch += 1\n",
    "        \n",
    "        groups = np.array(groups)\n",
    "        group_sums_argsort = np.argsort(group_sums)[::-1]\n",
    "        groups = groups[group_sums_argsort]\n",
    "        \n",
    "        # check_x = self.X[groups]\n",
    "        # check_x_lens = [np.sum(np.array([len(sx) for sx in rx])) for rx in check_x]\n",
    "        # print(f'check_x: {check_x}')\n",
    "        \n",
    "        \n",
    "        return np.array(groups), groups_size\n",
    "        \n",
    "    def content_to_graph(self, doc):\n",
    "        # tokens = self.tokenizer(''.join(c for c in doc if c in self.char_Set))\n",
    "        # tokens = [t.text for t in tokens]\n",
    "        tokens = self.tokenizer(doc)\n",
    "        if len(tokens) == 0:\n",
    "            tokens = ['empty']\n",
    "                        \n",
    "        token_lengths = [len(t) for t in tokens]\n",
    "        tokens.append('\\x01')\n",
    "        \n",
    "        token_lengths.append(len(tokens[-1])-1)\n",
    "        token_lengths = torch.from_numpy(np.array(token_lengths, dtype=np.longlong))+1\n",
    "        token_embs = [self.token_dict[t] if t in self.token_dict else torch.zeros((64, ), dtype=torch.float32) for t in tokens]\n",
    "        token_sentiments = [self.sentiment_dict[t] if t in self.sentiment_dict else (0.0, 0.0) for t in tokens]\n",
    "        token_embs = torch.from_numpy(np.array(token_embs, dtype=np.float32))\n",
    "        token_sentiments = torch.from_numpy(np.array(token_sentiments, dtype=np.float32))\n",
    "        doc = ' '.join(tokens)\n",
    "        characters = torch.from_numpy(np.array([self.char_dict[t] for t in doc], dtype=np.longlong))\n",
    "        token_positions = torch.arange(len(token_lengths), dtype=torch.long)\n",
    "        token_indices = torch.repeat_interleave(token_positions, token_lengths)\n",
    "        num_tokens = len(token_lengths)\n",
    "        if num_tokens > self.max_token_count:\n",
    "            self.max_token_count = num_tokens\n",
    "        g_data = Data(x=characters,\n",
    "                        token_positions=token_positions,\n",
    "                        character_length = len(characters),\n",
    "                        num_tokens = num_tokens,\n",
    "                        token_indices=token_indices,\n",
    "                        token_lengths=token_lengths,\n",
    "                        token_embeddings=token_embs,\n",
    "                        token_sentiments=token_sentiments)\n",
    "        return g_data\n",
    " \n",
    "    def caluculate_batch_token_positions(self, num_tokens, character_length, token_indices):\n",
    "        cumsum_vals = torch.cumsum(num_tokens, dim=0).roll(1)\n",
    "        cumsum_vals[0] = 0\n",
    "        additions = torch.repeat_interleave(cumsum_vals, character_length)\n",
    "        cumulative_token_indices = token_indices + additions\n",
    "        return cumulative_token_indices       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "class CharacterandTokenLevelDataLoader(DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size: int = 1,\n",
    "        shuffle: bool = False,\n",
    "        follow_batch: List[str] | None = None,\n",
    "        exclude_keys: List[str] | None = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(CharacterandTokenLevelDataLoader, self).__init__(\n",
    "            dataset, batch_size, shuffle, follow_batch, exclude_keys, **kwargs\n",
    "        )\n",
    "\n",
    "    def __iter__(self):\n",
    "        base_iterator = super(CharacterandTokenLevelDataLoader, self).__iter__()\n",
    "        for batch in base_iterator:\n",
    "            cumsum_vals = torch.cumsum(batch[0].num_tokens, dim=0).roll(1)\n",
    "            cumsum_vals[0] = 0\n",
    "            additions = torch.repeat_interleave(cumsum_vals, batch[0].character_length)\n",
    "            batch[0].cumulative_token_indices = batch[0].token_indices + additions\n",
    "            yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(df.Content.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 16/1344 [00:00<00:27, 48.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1344/1344 [00:30<00:00, 43.99it/s] \n",
      "100%|██████████| 1344/1344 [00:29<00:00, 45.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 2s\n",
      "Wall time: 1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_dataset = CharacterandTokenLevelCustomDataset(train_df.Content.values, train_df.Topic.values, len(class_id), vocab_dict, token_vocab_dict, polarity_vocab_dict, tokenizer.tokenize, batch_size=batch_size)\n",
    "test_dataset = CharacterandTokenLevelCustomDataset(test_df.Content.values, test_df.Topic.values, len(class_id), vocab_dict, token_vocab_dict, polarity_vocab_dict, tokenizer.tokenize, batch_size=batch_size)\n",
    "max_token_count = max(train_dataset.max_token_count, test_dataset.max_token_count)\n",
    "train_dataloader = CharacterandTokenLevelDataLoader(train_dataset, batch_size=batch_size, drop_last=False, shuffle=True)\n",
    "test_dataloader = CharacterandTokenLevelDataLoader(test_dataset, batch_size=batch_size, drop_last=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Data(x=[6708], token_positions=[1179], character_length=6708, num_tokens=1179, token_indices=[6708], token_lengths=[1179], token_embeddings=[1179, 64], token_sentiments=[1179, 2]),\n",
       " tensor([1., 0.]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1453\n",
      "3089\n"
     ]
    }
   ],
   "source": [
    "train_lengths = np.array([train_dataset[i][0].num_tokens for i in range(len(train_dataset))])\n",
    "test_lengths = np.array([test_dataset[i][0].num_tokens for i in range(len(test_dataset))])\n",
    "print(np.max(train_lengths))\n",
    "print(np.max(test_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[341319], token_positions=[63640], character_length=[224], num_tokens=[224], token_indices=[341319], token_lengths=[63640], token_embeddings=[63640, 64], token_sentiments=[63640, 2], batch=[341319], ptr=[225], cumulative_token_indices=[341319])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Topic, Content]\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[np.argwhere(['hardy' in c and 'obscure' in c for c in  df.Content.values]).squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ▁Before ▁I ▁begin , ▁let ▁me ▁get ▁something ▁off ▁my ▁chest : ▁I ' m ▁a ▁huge ▁fan ▁of ▁John ▁Eyre s ' ▁first ▁film ▁PROJECT : ▁SHADOW CHA SER . ▁The ▁film , ▁a ▁B - grade ▁cross ▁of ▁both ▁THE ▁TERM IN ATOR ▁& ▁DIE ▁HARD , ▁may ▁not ▁be ▁the ▁work ▁of ▁a ▁cinematic ▁genius , ▁but ▁is ▁a ▁hugely ▁entertaining ▁action ▁film ▁that ▁became ▁a ▁cult ▁hit ▁( & ▁spawned ▁two ▁sequels ▁& ▁a ▁spin ▁off ) . < br ▁/ > < br ▁/ > Judge ▁and ▁Jury ▁begins ▁with ▁Joseph ▁Meeker , ▁a ▁convicted ▁killer ▁who ▁was ▁sent ▁to ▁Death ▁Row ▁following ▁his ▁capture ▁after ▁the ▁so - called ▁\" Bloody ▁Shootout \" ▁( which ▁seems ▁like ▁a ▁poor ▁name ▁for ▁a ▁killing ▁spree ▁  ▁Meeker ▁kills ▁three ▁people ▁while ▁trying ▁to ▁rob ▁a ▁convenience ▁store ) , ▁being ▁led ▁to ▁the ▁electric ▁chair . ▁There ▁is ▁an ▁amusing ▁scene ▁where ▁Meeker ▁talks ▁to ▁the ▁priest ▁about ▁living ▁for ▁sex ▁but ▁meeting ▁his ▁one ▁true ▁love ▁( who ▁was ▁killed ▁during ▁the ▁shootout ) , ▁expressing ▁his ▁revenge ▁for ▁the ▁person ▁who ▁killed ▁her ▁  ▁Michael ▁Silva no , ▁a ▁washed - up ▁football ▁star ▁who ▁spends ▁his ▁days ▁watching ▁his ▁son ▁Alex ▁practicing ▁football ▁with ▁his ▁high ▁school ▁team ▁( and ▁ends ▁up ▁harassing ▁his ▁son ' s ▁coach ) . ▁But ▁once ▁executed , ▁Meeker ▁returns ▁as ▁a ▁re ve nant ▁( or ▁as ▁Kelly ▁Per ine ▁calls ▁\" a ▁hamburger ▁without ▁the ▁fries \" ) , ▁whose ▁sole ▁aim ▁is ▁to ▁get ▁his ▁revenge , ▁which ▁basically ▁means ▁making ▁Silva no ' s ▁life ▁a ▁misery . < br ▁/ > < br ▁/ > Let ▁me ▁point ▁out ▁the ▁fact ▁that ▁Judge ▁and ▁Jury ▁is ▁not ▁a ▁true ▁horror ▁film . ▁It ▁is ▁a ▁supernatural ▁action ▁film , ▁with ▁Meeker ▁chasing ▁Silva no , ▁using ▁his ▁ability ▁to ▁change ▁form ▁( which ▁amounts ▁to ▁David ▁Keith ▁dressing ▁up ▁as ▁everything ▁from ▁an ▁Elvis ▁impersonator , ▁a ▁French ▁chef ▁( with ▁an ▁accent ▁as ▁bad ▁as ▁his ▁moustache ) , ▁a ▁drag ▁queen , ▁a ▁clown ▁& ▁a ▁stand - up ▁comedian ) , ▁a ▁shotgun ▁which ▁fires ▁explosive ▁rounds ▁& ▁an ▁in vulnerability ▁to ▁death ▁( although ▁that ▁doesn ' t ▁stop ▁Martin ▁Ko ve ▁from ▁shooting ▁Keith ▁with ▁a ▁Desert ▁Eagle ) , ▁to ▁pay ▁Silva no ▁back ▁for ▁killing ▁Meeker ' s ▁wife . < br ▁/ > < br ▁/ > Director ▁John ▁Eyre s ▁does ▁not ▁seem ▁interested ▁in ▁characterisation s , ▁instead ▁focusing ▁solely ▁on ▁action ▁scenes , ▁which ▁the ▁film ▁has ▁plenty ▁of . ▁But ▁that ▁is ▁the ▁film ' s ▁main ▁flaw , ▁since ▁there ' s ▁nothing ▁to ▁connect ▁the ▁action ▁scenes ▁together . ▁The ▁acting ▁is ▁surprisingly ▁good , ▁with ▁Keith ▁delivering ▁the ▁best ▁performance , ▁supported ▁ably ▁by ▁Ko ve , ▁as ▁well ▁as ▁Paul ▁Kos lo , ▁who ▁plays ▁the ▁washed - up ▁cop ▁quite ▁well . ▁Kelly ▁Per ine ▁is ▁annoying ▁as ▁the ▁cabbie ▁who ▁tries ▁to ▁help ▁but ▁makes ▁the ▁situation ▁worse . \u0001\n",
      "0 ▁Go ▁up ▁to ▁any ▁film ▁fan ▁and ▁ask ▁them ▁the ▁title ▁of ▁the ▁film ▁which ▁was ▁directed ▁by ▁Robert ▁Wise , ▁with ▁second - unit ▁direction ▁by ▁Yakima ▁Can ut t ▁and ▁Sergio ▁Leone , ▁was ▁designed ▁by ▁Ken ▁Adam ▁and ▁scored ▁by ▁Max ▁Steiner , ▁starred ▁Sir ▁Stanley ▁Baker , ▁Sir ▁Cedric ▁Hardwick e ▁and ▁Brigitte ▁Bar dot , ▁was ▁filmed ▁in ▁colour , ▁scope ▁and ▁stereo , ▁at ▁Cine c itta ▁in ▁1955 , ▁with ▁a ▁thousand ▁extras ▁- ▁and ▁they ' ll ▁tell ▁you ▁to ▁go ▁away ▁and ▁stop ▁being ▁silly . < br ▁/ > < br ▁/ > They ' ll ▁tell ▁you ▁that ▁no ▁such ▁film ▁EXIST S . ▁That ▁the ▁names ▁you ' ve ▁quoted ▁NEVER ▁worked ▁together ▁- ▁they ▁weren ' t ▁even ▁contemporaneous . ▁And ▁that ▁you ' ve ▁just ▁picked ▁the ▁names ▁out ▁of ▁a ▁movie ▁publication ▁at ▁random ▁and ▁are ▁attempting ▁to ▁be f uddle ▁them . < br ▁/ > < br ▁/ > At ▁which ▁point ▁you ▁can ▁direct ▁them ▁to ▁IMDb ▁and ▁show ▁them ▁the ▁cast ▁and ▁crew ▁of ▁\" Helen ▁Of ▁Troy \" . ▁They ' ll ▁be ▁amazed ! ▁This ▁lesser - known ▁sword - and - s and al ▁epic ▁has ▁ALL ▁these ▁names ▁and ▁more ▁- ▁Niall ▁Mac G inni s , ▁Janette ▁Scott ▁and ▁good ▁old ▁Harry ▁Andrews . < br ▁/ > < br ▁/ > And ▁it ▁is ▁certainly ▁an ▁oddity . ▁After ▁the ▁war , ▁1 , 000 ▁Italian ▁extras ▁cost ▁about ▁$ 25 ▁a ▁day ▁and ▁to ga ▁dramas ▁were ▁a ▁staple ▁of ▁Italian ▁cinema . ▁The ▁or gy ▁scenes ▁were ▁shot ▁twice ▁- ▁one ▁with ▁tops , ▁the ▁other ▁without ▁( you ▁can ▁guess ▁which ▁version ▁Britain ▁and ▁America ▁got ) . ▁I ▁believe ▁even ▁La ▁Sophia ▁is ▁an ▁extra ▁in ▁this ▁one . < br ▁/ > < br ▁/ > Either ▁way , ▁the ▁names ▁S TAG GER ▁the ▁mind . ▁But ▁it ' s ▁really ▁just ▁a ▁coincidence . ▁All ▁of ▁said ▁names ▁were ▁either ▁just ▁reaching ▁the ▁ends ▁of ▁their ▁careers ▁( Can ut t , ▁Steiner ) ▁or ▁beginning ▁them ▁( everybody ▁else ) . < br ▁/ > < br ▁/ > Only ▁Robert ▁Wise ▁and ▁Niall ▁Mac G inni s ▁were ▁in ▁the ▁middle ▁of ▁their ▁careers . < br ▁/ > < br ▁/ > For ▁the ▁record , ▁Leone ▁was ▁uncredited ▁and ▁learning ▁his ▁trade ▁- ▁Adam ▁still ▁had ▁to ▁invent ▁the ▁descending ▁circle ▁in ▁the ▁ceiling ▁of ▁sets ▁( a ▁trademark ▁he ' d ▁go ▁on ▁to ▁put ▁into ▁all ▁the ▁early ▁Bonds ) ▁- ▁Baker ▁had ▁yet ▁to ▁star ▁in ▁and ▁help ▁produce ▁the ▁likes ▁of ▁\" Zulu \" ▁and ▁\" Rob bery \" ▁- ▁and ▁go ▁on ▁to ▁direct ▁a ▁Welsh ▁TV ▁company ▁called ▁Harle ch ▁- ▁then ▁die ▁tragically ▁young . < br ▁/ > < br ▁/ > While ▁Harry ▁Andrews ▁would ▁go ▁on ▁to ▁become ▁one ▁of ▁Britain ' s ▁favourite ▁character ▁actors ▁- ▁Janette ▁Scott ▁( Thor a ▁Hi rd ' s ▁daughter ) ▁would ▁never ▁make ▁the ▁really ▁big ▁time , ▁but ▁who ▁can ▁forget ▁her ▁in ▁\" Day ▁Of ▁The ▁Trif fid s \" ▁( even ▁though ▁her ▁bit ▁was ▁added ▁later ▁- ▁for ▁padding ▁and ▁a ▁happy ▁ending ) ▁or ▁\" Crack ▁In ▁The ▁World \" ? < br ▁/ > < br ▁/ > Sir ▁Cedric ▁was ▁theatre , ▁but ▁knew ▁how ▁to ▁mug ▁on ▁film ▁- ▁and ▁Bar dot . . . ▁was ▁BAR DOT , ▁for ▁ga wd s ake ! < br ▁/ > < br ▁/ > But ▁what ▁were ▁these ▁stellar ▁people ▁DOING ▁in ▁this ▁camp ▁old ▁nonsense ? ▁Don ' t ▁ask ▁me . ▁The ▁two ▁main ▁stars ▁were ▁no - name ▁Italians ▁- ▁Helen ▁had ▁a ▁moustache ▁and ▁Paris ▁was ▁pretty ▁- ▁while ▁the ▁Brits ▁were ▁only ▁there ▁for ▁support . < br ▁/ > < br ▁/ > To ▁summarise , ▁I ▁think ▁you ▁can ▁just ▁mark ▁this ▁one ▁up ▁as ▁a ▁major ▁FL UKE . ▁In ▁stereo . ▁To ▁be ▁honest , ▁if ▁I ▁hadn ' t ▁seen ▁it ▁- ▁I ▁wouldn ' t ▁believe ▁it ▁EITHER ! \u0001\n",
      "0 ▁This ▁guy ▁is ▁a ▁real ▁piece ▁of ▁work . ▁An ▁angry , ▁immature ▁boy ▁in ▁a ▁grown ▁man ' s ▁body , ▁packing ▁all ▁the ▁charisma ▁of ▁a ▁rock , ▁he ▁goes ▁around ▁to ▁places ▁most ▁people ▁would ▁only ▁wish ▁to ▁visit ▁and ▁does ▁his ▁best ▁to ▁be ▁as ▁miserable ▁as ▁possible . < br ▁/ > < br ▁/ > Give ▁this ▁job ▁to ▁someone ▁else ▁who ▁actually ▁appreciates ▁it . < br ▁/ > < br ▁/ > I ▁could ▁go ▁down ▁an ▁endless ▁list ▁of ▁all ▁the ▁stupid ▁things ▁this ▁guy ▁does ▁in ▁his ▁\" episode s , \" ▁though ▁I ' ll ▁just ▁highlight ▁the ▁worst : ▁Crete . ▁While ▁the ▁locals ▁are ▁putting ▁up ▁seaside ▁picnics ▁in ▁his ▁\" honour , \" ▁this ▁clown ▁has ▁the ▁gall ▁to ▁act ▁like ▁a ▁petulant , ▁spoiled ▁child . ▁He ▁complains ▁about ▁everything , ▁including ▁the ▁fashion ▁sense ▁of ▁the ▁people ▁who ▁live ▁there . ▁What ▁an ▁imbecile . < br ▁/ > < br ▁/ > When ▁he ▁went ▁to ▁Sweden , ▁he ▁spent ▁at ▁least ▁five ▁minutes ▁feigning ▁incredulity ▁at ▁a ▁bunch ▁of ▁chefs ▁( who ▁probably ▁had ▁better ▁things ▁to ▁do ▁than ▁talk ▁with ▁some ▁dim wit ▁American , ▁like ▁work ) ▁because ▁they ▁didn ' t ▁think ▁Abba ▁was ▁horrible . ▁Everywhere ▁he ▁went , ▁he ▁brought ▁up ▁Abba . ▁This ▁is ▁the ▁kind ▁of ▁talk ▁you ' d ▁hear ▁from ▁13 - year - olds ▁who ▁watch ▁too ▁much ▁MTV . < br ▁/ > < br ▁/ > When ▁he ▁was ▁in ▁New ▁Orleans , ▁he ▁got ▁upset ▁that ▁a ▁certain ▁restaurant ▁had ▁better - tasting ▁fries ▁than ▁his , ▁so ▁he ▁\" accidentally \" ▁spilled ▁some ▁wine ▁on ▁them ▁in ▁order ▁to ▁ruin ▁them . ▁What ▁a ▁strange , ▁emotionally ▁unstable ▁person . < br ▁/ > < br ▁/ > The ▁worst ▁of ▁it ▁all ▁are ▁his ▁clumsy ▁voice - overs , ▁where ▁he ▁attempts ▁in ▁vain ▁to ▁add ▁some ▁kind ▁of ▁perspective ▁on ▁a ▁situation ▁he ▁was ▁too ▁thick ▁and ▁ignorant ▁to ▁appreciate . ▁He ▁tries ▁to ▁use ▁all ▁these ▁\" big \" ▁words ▁in ▁order ▁to ▁sound ▁like ▁an ▁author , ▁but ▁he ' s ▁really ▁just ▁a ▁pretentious ▁hack ▁whose ▁lack ▁of ▁awareness ▁has ▁convinced ▁him ▁he ▁has ▁something ▁to ▁say . ▁That , ▁by ▁the ▁way , ▁is ▁probably ▁the ▁one ▁good ▁thing ▁about ▁this ▁joker ' s ▁TV ▁show . ▁It ▁goes ▁to ▁show ▁you , ▁no ▁matter ▁how ▁inept ▁you ▁are , ▁as ▁long ▁as ▁you ▁take ▁yourself ▁seriously ▁enough , ▁the ▁world ▁will ▁as ▁well . < br ▁/ > < br ▁/ > Then ▁there ' s ▁the ▁way ▁he ▁speaks ▁with ▁local ▁guides ▁whose ▁English ▁is ▁obviously ▁only ▁rudimentary . ▁He ' ll ▁use ▁vocabulary ▁any ▁writer - - as ▁he ▁believes ▁himself ▁to ▁be - - would ▁instinctively ▁know ▁will ▁most ▁likely ▁not ▁be ▁understood ▁by ▁these ▁people . ▁Does ▁he ▁care ? ▁No . ▁Self - important ▁schmuck s ▁like ▁this ▁Bourdain ▁clown ▁do ▁not ▁use ▁language ▁to ▁communicate ; ▁they ▁use ▁it ▁to ▁make ▁themselves ▁look ▁important . < br ▁/ > < br ▁/ > Mc g 13 j th m ' s ▁review ▁on ▁this ▁same ▁page ▁is ▁a ▁perfect ▁example ▁of ▁the ▁kind ▁of ▁mind ▁Bourdain ▁attracts - - that ▁of ▁a ▁low ▁IQ ▁social ▁misfit . ▁Observe ▁how ▁the ▁reviewer ▁attempts ▁to ▁justify ▁Bourdain ' s ▁sociopath ▁nature ▁with ▁simple - minded , ▁childish ▁excuses ▁that ▁hardly ▁make ▁sense . ▁\" Bour dain ▁may ▁complain ▁but ▁he ▁goes ▁through ▁' a ▁lot ' ▁and , ▁not ▁only ▁that , ▁he ▁was ▁' forced ' ▁to ▁do ▁this ▁show ▁but ▁is ▁trying ▁to ▁redeem ▁himself . \" ▁A ▁do lt ▁attracts ▁do lt s , ▁and ▁reading ▁Mc g 13 j th m ' s ▁review ▁should ▁let ▁you ▁know ▁perfectly ▁well ▁whether ▁or ▁not ▁you ▁are ▁the ▁kind ▁of ▁person ▁who ' d ▁enjoy ▁this ▁utterly ▁useless , ▁pointless ▁show . < br ▁/ > < br ▁/ > Finally , ▁to ▁add ▁a ▁bit ▁of ▁\" fair ness \" ▁to ▁my ▁diatribe , ▁I ▁admit ▁Bourdain ▁would ▁have ▁been ▁momentarily ▁amusing ▁had ▁I ▁met ▁him ▁in ▁a ▁bar . ▁But ▁as ▁a ▁TV ▁host ▁of ▁a ▁travel ▁show ▁whose ▁purpose ▁is ▁to ▁show ▁the ▁viewer ▁the ▁beauty ▁of ▁other ▁places ▁and ▁cultures , ▁Bourdain ▁is ▁a ▁miserable , ▁abject , ▁hopeless , ▁grim ▁and ▁depressing ▁failure . < br ▁/ > < br ▁/ > A ▁failure . \u0001\n",
      "0 ▁Soul taker ▁was ▁written ▁by ▁and ▁starred ▁Vivian ▁Schilling . ▁It ▁also ▁starred ▁Joe ▁Este vez , ▁Gregg ▁Thomsen , ▁and ▁Robert ▁D ' Z ar ▁as ▁the ▁Angel ▁of ▁Death . < br ▁/ > < br ▁/ > The ▁story ▁begins ▁with ▁introduction ▁to ▁Soul taker , ▁played ▁by ▁Joe ▁Este vez . ▁We ▁quickly ▁learn ▁what ▁Soul taker ' s ▁role ▁will ▁be ▁in ▁this ▁movie . < br ▁/ > < br ▁/ > Next ▁the ▁college ▁aged ▁young ▁people ▁are ▁getting ▁ready ▁for ▁a ▁summer ▁festival , ▁aptly ▁named ▁\" Summer fest \" . ▁In ▁this ▁film , ▁the ▁battle ▁of ▁the ▁classes ▁is ▁omni - present ▁throughout ▁the ▁film . ▁The ▁girls ▁come ▁from ▁a ▁wealthy ▁class , ▁and ▁the ▁guys ▁come ▁from ▁roughly ▁middle ▁or ▁lower ▁class . ▁The ▁class ▁roles ▁seem ▁to ▁play ▁a ▁role ▁in ▁the ▁film ▁for ▁some ▁reason ▁which ▁isn ' t ▁really ▁clear ▁or ▁pertinent ▁to ▁the ▁story . < br ▁/ > < br ▁/ > At ▁Summer fest ▁we ▁learn ▁more ▁about ▁the ▁apparent ▁class ▁struggles ▁of ▁why ▁Zach ▁isn ' t ▁encouraged ▁to ▁date ▁Natalie . ▁Soul taker ▁makes ▁an ▁appearance ▁as ▁well , ▁with ▁apparently ▁his ▁boss ▁the ▁Angel ▁of ▁Death . ▁Here ▁D ' Z ar ' s ▁character ▁points ▁out ▁who ▁is ▁to ▁die ▁and ▁who ' s ▁souls ▁are ▁to ▁be ▁taken . ▁It ' s ▁revealed ▁as ▁well , ▁that ▁Soul taker ▁will ▁have ▁a ▁character ▁conflict ▁regarding ▁Natalie , ▁and ▁how ▁he ▁deals ▁with ▁her ▁because ▁of ▁someone ▁in ▁his ▁past . < br ▁/ > < br ▁/ > Meanwhile ▁Natalie ▁is ▁ditched ▁by ▁her ▁ride ▁to ▁Summer fest , ▁and ▁Zach ▁convinces ▁her ▁to ▁ride ▁home ▁with ▁them . ▁During ▁the ▁ride ▁home , ▁Soul taker ▁takes ▁an ▁active ▁role ▁causing ▁them ▁to ▁wreck ▁horribly ▁at ▁high ▁speeds . < br ▁/ > < br ▁/ > The ▁rest ▁of ▁the ▁story ▁surrounds ▁the ▁Soul taker ▁collecting ▁the ▁souls ▁of ▁the ▁dead ▁passengers , ▁and ▁Zach ▁and ▁Natalie ▁trying ▁to ▁out wit ▁him ▁to ▁return ▁to ▁their ▁bodies ▁so ▁they ▁can ▁continue ▁to ▁live . ▁The ▁class ▁and ▁character ▁conflicts ▁lay ▁in ▁the ▁story , ▁but ▁are ▁really ▁never ▁brought ▁to ▁the ▁forefront ▁or ▁resolved . < br ▁/ > < br ▁/ > There ' s ▁an ▁attempt ▁towards ▁the ▁end ▁to ▁drag ▁out ▁some ▁of ▁the ▁drama , ▁there ' s ▁a ▁lot ▁of ▁chasing ▁and ▁running ▁which ▁does ▁tend ▁to ▁be ▁really ▁boring . ▁It ' s ▁not ▁really ▁acceptable , ▁and ▁it ▁would ' ve ▁been ▁nice ▁had ▁this ▁been ▁dealt ▁with ▁differently , ▁somehow ▁to ▁maybe ▁increase ▁the ▁drama ▁but ▁not ▁bore ▁the ▁audience . < br ▁/ > < br ▁/ > The ▁story ▁and ▁acting ▁are ▁decent . ▁The ▁soundtrack ▁is ▁OK , ▁and ▁even ▁the ▁production ▁values ▁are ▁good . < br ▁/ > < br ▁/ > Robert ▁D ' Z ar ▁in ▁his ▁brief ▁on ▁screen ▁appearances ▁does ▁a ▁nice ▁job ▁as ▁the ▁Angel ▁of ▁Death . ▁Joe ▁Este vez ▁does ▁OK , ▁however ▁sometimes ▁his ▁role ▁acting ▁a ▁bit ▁flat . ▁Vivian ▁is ▁pretty ▁and ▁does ▁a ▁decent ▁job ▁as ▁Natalie , ▁although ▁perhaps ▁over ▁acting ▁a ▁bit ▁in ▁a ▁few ▁scenes . < br ▁/ > < br ▁/ > This ▁may ▁sound ▁odd , ▁but ▁this ▁movie ▁definitely ▁could ' ve ▁benefited ▁from ▁some ▁pointless ▁nudity . ▁Vivian ▁teases ▁us ▁a ▁bit ▁but ▁that ▁wasn ' t ▁enough . < br ▁/ > < br ▁/ > In ▁my ▁opinion ▁this ▁was ▁a ▁pretty ▁serious ▁attempt ▁at ▁making ▁a ▁movie . ▁The ▁results , ▁it ' s ▁worth ▁watching . ▁Just ▁don ' t ▁expect ▁a ▁perfect ▁production . < br ▁/ > < br ▁/ > 3 / 10 \u0001\n",
      "1 ▁Being ▁an ▁Israeli ▁Jew ▁of ▁naturally ▁sarcastic ▁nature ▁as ▁well ▁as ▁a ▁lover ▁of ▁different ▁and ▁independent ▁cinema , ▁it ▁always ▁gives ▁me ▁pleasure ▁to ▁see ▁a ▁film ▁that ▁takes ▁a ▁view ▁on ▁the ▁holocaust ▁that ' s ▁sensitive ▁and ▁respectful ▁while ▁also ▁being ▁original ▁and ▁unusual . ▁While ▁I ▁haven ' t ▁read ▁the ▁book ▁  ▁or , ▁for ▁that ▁matter , ▁heard ▁of ▁its ▁existence ▁prior ▁to ▁watching ▁the ▁film ▁  ▁and ▁therefore ▁cannot , ▁like ▁some ▁other ▁reviewers , ▁comment ▁on ▁how ▁they ▁stack ▁up ▁in ▁comparison , ▁Everything ▁I s ▁Illuminated ▁gave ▁me ▁great ▁pleasure , ▁and ▁I ▁can ▁certainly ▁comment ▁on ▁that . < br ▁/ > < br ▁/ > To ▁label ▁Everything ▁I s ▁Illuminated ▁a ▁holocaust ▁film ▁would ▁be ▁to ▁do ▁it ▁great ▁injustice , ▁even ▁though ▁it ▁is ▁undeniably ▁about ▁the ▁holocaust . ▁So ▁would ▁labeling ▁it ▁as ▁a ▁comedy ▁or ▁a ▁travel ▁film , ▁although ▁it ' s ▁about ▁a ▁journey ▁and ▁is ▁as ▁exceptionally ▁funny ▁as ▁it ▁is ▁moving . ▁Everything ▁I s ▁Illuminated ▁is ▁about ▁Jonathan ▁Safran ▁Fo er ▁  ▁played ▁to ▁minimalist ▁perfection ▁by ▁Elijah ▁Wood , ▁in ▁the ▁most ▁impressive ▁dramatic ▁performance ▁I ' ve ▁seen ▁him ▁in ▁yet , ▁with ▁a ▁poker ▁face ▁that ▁shows ▁nothing ▁and ▁reveals ▁all ▁  ▁a ▁young ▁American ▁Jew , ▁and ▁an ▁obsessive ▁collector ▁of ▁family ▁heirlooms ▁and ▁historical ▁artifacts , ▁who ▁travels ▁to ▁the ▁Ukraine ▁on ▁a ▁journey ▁to ▁find ▁the ▁woman ▁who ▁saved ▁his ▁grandfather ▁from ▁the ▁Nazis . ▁It ' s ▁also ▁about ▁Alex , ▁his ▁tour ▁guide ▁through ▁the ▁Ukraine , ▁and ▁Alex ' s ▁grandfather . ▁What ' s ▁fascinating ▁about ▁these ▁characters ▁is ▁that ▁in ▁the ▁beginning ▁of ▁the ▁film ▁they ▁look ▁like ▁comic ▁relief ▁to ▁balance ▁out ▁the ▁melancholy ▁nature ▁of ▁Wood ' s ▁character ; ▁but ▁both ▁Alex ▁and ▁his ▁grandfather ▁go ▁through ▁fascinating ▁changes ▁throughout ▁the ▁film , ▁and ▁turn ▁out ▁to ▁be ▁at ▁least ▁as ▁important ▁as ▁Jonathan . ▁In ▁fact , ▁Boris ▁Le skin ' s ▁as ▁the ▁grumpy , ▁self - declared ▁blind ▁grandfather ▁turns ▁out ▁to ▁be ▁the ▁finest ▁dramatic ▁performance ▁in ▁the ▁film . < br ▁/ > < br ▁/ > A side ▁from ▁the ▁surreal ▁nature ▁of ▁the ▁film ▁and ▁the ▁characters , ▁the ▁beautiful ▁mix ▁of ▁original ▁acoustic ▁music ▁and ▁Russian ▁folk ▁music , ▁the ▁sensitive ▁cinematography ▁and ▁the ▁chilling ▁contrast ▁between ▁the ▁beauty ▁of ▁the ▁landscapes ▁and ▁the ▁horrors ▁of ▁history , ▁what ▁made ▁Everything ▁I s ▁Illuminated ▁a ▁powerful ▁and ▁moving ▁experience ▁for ▁me ▁was ▁the ▁fact ▁that ▁from ▁Alex ▁and ▁his ▁grandfather ▁we ▁get ▁a ▁very ▁different ▁and ▁original ▁viewpoint ▁on ▁this ▁painful ▁subject ; ▁several ▁excellent ▁films , ▁such ▁as ▁The ▁Grey ▁Zone ▁and ▁Down fall , ▁have ▁already ▁given ▁us ▁the ▁point ▁of ▁view ▁of ▁the ▁lower - rank ▁Nazis ▁who ▁are ▁presented ▁as ▁human ▁beings ▁who ▁aren ' t ▁necessarily ▁fully ▁aware ▁of ▁the ▁moral ▁implications ▁of ▁their ▁actions ▁but ▁are ▁caught ▁up ▁in ▁the ▁reality ▁of ▁the ▁war . ▁Everything ▁I s ▁Illuminated ▁presents ▁a ▁point ▁of ▁view ▁rarely ▁treated ▁before : ▁Alex ' s ▁point ▁of ▁view ▁is ▁that ▁of ▁a ▁young ▁man ▁who ▁was ▁born ▁many ▁years ▁after ▁the ▁war , ▁who ▁sees ▁it ▁as ▁hardly ▁more ▁than ▁cold ▁historical ▁fact , ▁who ▁finds ▁himself ▁having ▁to ▁face ▁up ▁to ▁the ▁horrors ▁his ▁own ▁people ▁  ▁and ▁maybe ▁his ▁own ▁family ▁as ▁well ▁  ▁were ▁capable ▁of . ▁The ▁change ▁in ▁Alex ' s ▁attitude ▁  ▁and ▁his ▁grandfather ' s ▁  ▁towards ▁Jonathan , ▁towards ▁the ▁Holocaust , ▁and ▁towards ▁the ▁Jewish ▁people ▁in ▁general , ▁makes ▁the ▁film ▁a ▁fascinating ▁and ▁original ▁study ▁in ▁character ▁development . < br ▁/ > < br ▁/ > Everything ▁I s ▁Illuminated ▁is ▁a ▁terrific ▁directorial ▁debut ▁for ▁actor ▁Lie v ▁Schreiber , ▁and ▁one ▁of ▁the ▁most ▁original ▁and ▁unique ▁films ▁of ▁2005 . ▁It ' s ▁a ▁highly ▁recommended ▁viewing ▁experience , ▁especially ▁or ▁anyone ▁interested ▁in ▁the ▁holocaust ▁and ▁World ▁War ▁II . \u0001\n"
     ]
    }
   ],
   "source": [
    "for i in range(15,20):\n",
    "    char_ids = X[i].x\n",
    "    text_for_ids = ''.join([vocab_dict_rev[ci.item()] for ci in char_ids])\n",
    "    print(torch.argmax(y[i]).item(), text_for_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rand = torch.randn((64, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9996)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rand.var(unbiased=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[341319], token_positions=[63640], character_length=[224], num_tokens=[224], token_indices=[341319], token_lengths=[63640], token_embeddings=[63640, 64], token_sentiments=[63640, 2], batch=[341319], ptr=[225], cumulative_token_indices=[341319])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = torch.randn((len(X.token_positions), 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([63640])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.repeat_interleave(torch.arange(len(X.num_tokens)), X.num_tokens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  ..., 70, 71, 72])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.token_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3197,  1.3102,  1.3217,  ...,  1.0564,  0.8946,  2.1401],\n",
       "        [ 0.0972,  0.9615,  1.8240,  ...,  0.1614,  0.6604,  0.6865],\n",
       "        [ 1.5708, -0.6290,  0.4220,  ..., -1.0549, -0.4237, -0.3870],\n",
       "        ...,\n",
       "        [-0.0783, -0.9660, -0.5623,  ..., -1.2335, -0.6625, -1.3791],\n",
       "        [ 0.6665,  0.7280, -2.4444,  ...,  1.0533, -1.0511,  0.2131],\n",
       "        [ 0.0288,  1.4813, -2.0293,  ..., -1.1579,  0.1973,  0.3177]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATv2Conv, SimpleConv\n",
    "\n",
    "# Normalization on each feature of all tokens, for this we used batch norm class but with tokens at batch dimention\n",
    "class GCNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, *args, **kwargs):\n",
    "        super(GCNN, self).__init__(*args, **kwargs)\n",
    "        self.gnn = GATv2Conv(hidden_dim, hidden_dim//8, heads=4, add_self_loops=False)\n",
    "        self.conv = nn.Conv1d(hidden_dim, hidden_dim//2, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(hidden_dim//2, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim//2)\n",
    "        \n",
    "    def forward(self, x, edge_data, return_attention_weights = False):\n",
    "        x1, edge_weights = self.gnn(x, edge_data, return_attention_weights=return_attention_weights) \n",
    "        x2 = F.relu(self.conv(x.T).T)\n",
    "        x1 = F.leaky_relu_(self.bn1(x1))\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        return x, edge_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from copy import copy, deepcopy\n",
    "# gcnn_model = GCNN(64)\n",
    "# flopt_counter = FlopCounterMode(gcnn_model)\n",
    "# X.edge_index = torch.randint(0, len(X.token_positions), size=(2,200))\n",
    "# with flopt_counter:\n",
    "#     x_input = deepcopy(torch.randn((len(X.token_positions), 64)))\n",
    "#     edge_data = deepcopy(X.edge_index)\n",
    "#     gcnn_model(x_input, edge_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx, to_undirected\n",
    "\n",
    "class GenGraph(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, virtual_nodes, lattice_step, lattice_pattern=None, *args, **kwargs):\n",
    "        super(GenGraph, self).__init__(*args, **kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.virtual_nodes = virtual_nodes\n",
    "        self.lattice_step = lattice_step\n",
    "        self.lp = lattice_pattern if lattice_pattern is None else torch.tensor(lattice_pattern)\n",
    "        self.virtual_node_embeddings = nn.Embedding(self.virtual_nodes, hidden_dim)\n",
    "        \n",
    "    def gen_graph(self, x, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance=2, seed=-1):\n",
    "        random_links, lattice_links, tc_range = self.calculate_graph(x, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance, seed)\n",
    "        v_n_e_counts = total_token_coutns*self.virtual_nodes\n",
    "        base_numel = random_links.numel() + lattice_links.numel()*2\n",
    "        edge_index = torch.empty((2, base_numel + v_n_e_counts*2), dtype=torch.int64, device=x.device)\n",
    "        self.fill_lattice_and_random_edges(edge_index, random_links, lattice_links, tc_range)\n",
    "            \n",
    "        if self.virtual_nodes > 0:\n",
    "            virtual_nodes_range = torch.arange(self.virtual_nodes, device=x.device).view(1, -1)\n",
    "            virtual_nodes_ids = torch.repeat_interleave(virtual_nodes_range, len(token_counts), dim=0)\n",
    "            v_n_idx = (virtual_nodes_ids + torch.arange(0, len(token_counts)*self.virtual_nodes, self.virtual_nodes, device=x.device).view(-1, 1) + total_token_coutns )\n",
    "            virtual_edge_ids = torch.repeat_interleave(v_n_idx.view(-1), token_counts.view(-1, 1).expand(len(token_counts), self.virtual_nodes).reshape(-1), dim=0).view(1, -1)\n",
    "            \n",
    "            embs = self.virtual_node_embeddings(virtual_nodes_ids.T).view(-1, self.hidden_dim)\n",
    "            x_extended = torch.cat([x, embs], dim=0)\n",
    "            x_index = torch.arange(total_token_coutns, device=x.device).repeat(self.virtual_nodes).view(1, -1)\n",
    "            edge_index[:, base_numel:base_numel+v_n_e_counts] = torch.cat([x_index, virtual_edge_ids], dim=0)\n",
    "            edge_index[:, base_numel+v_n_e_counts:] = torch.cat([virtual_edge_ids, x_index], dim=0)\n",
    "            x = x_extended\n",
    "        \n",
    "        return Batch.from_data_list([Data(x=x, edge_index=edge_index)])\n",
    "        \n",
    "    def re_gen_graph(self, x, edge_index, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance=2, seed=-1):\n",
    "        random_links, lattice_links, tc_range = self.calculate_graph(x, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance, seed)\n",
    "        base_numel = random_links.numel() + lattice_links.numel()*2\n",
    "        \n",
    "        self.fill_lattice_and_random_edges(edge_index, random_links, lattice_links, tc_range)\n",
    "        # for i in range(base.shape[1]):\n",
    "        #     edge_index[:, i*base.shape[0]:(i+1)*base.shape[0]] = torch.cat([tc_range, base[:,i].view(1,-1)], dim=0)\n",
    "            \n",
    "        return Batch.from_data_list([Data(x=x, edge_index=edge_index)])\n",
    "    \n",
    "    def replace_unimportant_edges(self, edge_weights, x, edge_index, total_token_coutns, token_counts, random_edges, lattice_edges, p_keep=1, lattice_start_distance=2, seed=-1):\n",
    "        v_n_e_counts = total_token_coutns*self.virtual_nodes\n",
    "        if v_n_e_counts>0:\n",
    "            important_indices = torch.topk(edge_weights[:-2*v_n_e_counts].view(-1, total_token_coutns), p_keep, dim=0).indices\n",
    "        else:\n",
    "            important_indices = torch.topk(edge_weights.view(-1, total_token_coutns), p_keep, dim=0).indices\n",
    "        # important_indices = torch.topk(edge_weights[:-1*total_token_coutns].view(-1, total_token_coutns), 1, dim=0).indices.squeeze()\n",
    "\n",
    "        important_indices = torch.arange(total_token_coutns, dtype=torch.int64, device=x.device) + important_indices*total_token_coutns\n",
    "        important_indices = important_indices.view(-1)\n",
    "        random_links, lattice_links, tc_range = self.calculate_graph(x, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance, seed)\n",
    "        base_numel = random_links.numel() + lattice_links.numel()*2\n",
    "        \n",
    "        new_edge_index = torch.empty((2, base_numel + important_indices.shape[0] + 2*v_n_e_counts), dtype=torch.int64, device=x.device)\n",
    "        self.fill_lattice_and_random_edges(new_edge_index, random_links, lattice_links, tc_range)\n",
    "        new_edge_index[:, base_numel:base_numel+important_indices.shape[0]] = edge_index[:, important_indices]\n",
    "        if(self.virtual_nodes>0):\n",
    "            new_edge_index[:, -2*v_n_e_counts:] = edge_index[:, -2*v_n_e_counts:]\n",
    "            \n",
    "        # for i in range(base.shape[1]):\n",
    "        #     new_edge_index[:, i*base.shape[0]:(i+1)*base.shape[0]] = torch.cat([tc_range, base[:,i].view(1,-1)], dim=0)\n",
    "        \n",
    "        return Batch.from_data_list([Data(x=x, edge_index=new_edge_index)])\n",
    "         \n",
    "    def calculate_graph(self, x, total_token_counts, token_counts, random_edges, lattice_edges, lattice_start_distance, seed=-1):\n",
    "        if seed>-1:\n",
    "            torch.manual_seed(seed)\n",
    "        tc_extended = torch.repeat_interleave(token_counts, token_counts, dim=0).view(-1,1)\n",
    "        tc_lower_bound = torch.empty((len(token_counts)+1), dtype=torch.long, device=x.device) #torch.cuda.IntTensor(len(token_counts)+1) #\n",
    "        tc_lower_bound[0] = 0\n",
    "        tc_lower_bound[1:] = torch.cumsum(token_counts, dim=0)\n",
    "        tc_lower_bound_extended = torch.repeat_interleave(tc_lower_bound[:-1], token_counts, dim=0).view(-1,1)\n",
    "        tc_range = torch.arange(tc_lower_bound[-1], device=x.device).view(-1,1)\n",
    "        # torch.arange(tc_lower_bound[-1], dtype=torch.int32, device=x.device).view(-1,1)\n",
    "        random_ints = torch.randint(0, 2*total_token_counts, (total_token_counts, random_edges), device=x.device) # torch.cuda.IntTensor(len(token_lengths), random_edges).random_()\n",
    "        lattice = self.lp.to(x.device) if self.lp is not None else torch.arange(lattice_start_distance, max(lattice_start_distance, self.lattice_step*lattice_edges+1), self.lattice_step, device=x.device).view(1, -1)\n",
    "        \n",
    "\n",
    "        # exponentials = torch.pow(2, torch.arange(1, self.exp_edges+1, device=x.device)).view(1, -1)\n",
    "        tc_local_range = tc_range - tc_lower_bound_extended\n",
    "        random_links = (((random_ints % (tc_extended - 1))+1 + tc_local_range) % tc_extended)+tc_lower_bound_extended\n",
    "        lattice_links = ((lattice + tc_local_range) % tc_extended)+tc_lower_bound_extended\n",
    "        # base = torch.cat([base1, base2], dim=1)\n",
    "        tc_range = tc_range.view(1,-1)\n",
    "        return random_links, lattice_links, tc_range\n",
    "    \n",
    "    def fill_lattice_and_random_edges(self, edge_index, random_links, lattice_links, tc_range):\n",
    "        for i in range(0, lattice_links.shape[1]*2, 2):\n",
    "            edge_index[:, i*lattice_links.shape[0]:(i+1)*lattice_links.shape[0]] = torch.cat([lattice_links[:,i//2].view(1,-1), tc_range], dim=0)\n",
    "            edge_index[:, (i+1)*lattice_links.shape[0]:(i+2)*lattice_links.shape[0]] = edge_index[:, i*lattice_links.shape[0]:(i+1)*lattice_links.shape[0]][[1, 0]]\n",
    "            \n",
    "        for i in range(random_links.shape[1]):\n",
    "            j = i + lattice_links.shape[1]*2\n",
    "            edge_index[:, j*random_links.shape[0]:(j+1)*random_links.shape[0]] = torch.cat([random_links[:,i].view(1,-1), tc_range], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment_Injection(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)        \n",
    "        self.conv1 = nn.Conv1d(2, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim*2, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "    def forward(self, x, token_sentiments):\n",
    "        print(f'1 x: {x.shape}')\n",
    "        print(f'1 token_sentiments: {token_sentiments.shape}')\n",
    "        x1 = F.relu_(self.bn1(self.conv1(token_sentiments.T).T))\n",
    "        x = F.relu_(self.conv2(torch.cat([x, x1], dim=1).T))\n",
    "        # x = x + x1\n",
    "        return x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment1  = Sentiment_Injection(64)\n",
    "# sentiment1(torch.ones((41047, 64)), torch.ones((41047, 2))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.nn import GATv2Conv, SimpleConv, summary\n",
    "\n",
    "# class CNN_for_Text(nn.Module):\n",
    "    \n",
    "#     def __init__(self, num_embedding, pos_emb_size=8192, embedding_dim=64, hidden_dim=64, dropout=0.3, num_out_features=4, seed=-1, random_edges=4, lattice_edges=10, virtual_nodes=1, lattice_step=2, lattice_start_distance=2, inject_embedding_dim=64, use_positional_encoder=[False, False, False], *args, **kwargs) -> None:\n",
    "#         super(CNN_for_Text, self).__init__(*args, **kwargs)\n",
    "#         self.pos_emb_size = pos_emb_size\n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.virtual_nodes = virtual_nodes\n",
    "#         self.base_random_edges = random_edges\n",
    "#         self.base_lattice_edges = lattice_edges\n",
    "#         self.lattice_start_distance = lattice_start_distance\n",
    "#         # self.use_token_polarity = use_token_polarity\n",
    "#         self.use_positional_encoder = use_positional_encoder\n",
    "#         if seed>-1:\n",
    "#             torch.manual_seed(seed)\n",
    " \n",
    "#         self.embedding = nn.Embedding(num_embedding, embedding_dim)\n",
    "        \n",
    "#         self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "#         self.positional_encoding = nn.Embedding(pos_emb_size, embedding_dim)\n",
    "#         self.positional_encoding.weight = self.create_positional_encoding()\n",
    "\n",
    "#         self.conv1 = nn.Conv1d(embedding_dim, hidden_dim, kernel_size=5, padding=2)\n",
    "#         self.pool1 = nn.MaxPool1d(2)\n",
    "#         self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, padding=2)\n",
    "        \n",
    "#         # if self.use_token_polarity[0]:\n",
    "#         #     self.conv3 = nn.Conv1d(2*hidden_dim + 2, hidden_dim, kernel_size=3, padding=1)\n",
    "#         # else:\n",
    "#         self.conv3 = nn.Conv1d(2*hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        \n",
    "#         self.conv4 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        \n",
    "#         # if self.use_token_polarity[1]:\n",
    "#         self.sentiment1  = Sentiment_Injection(hidden_dim)\n",
    "#         # if self.use_token_polarity[2]:\n",
    "#         self.sentiment2  = Sentiment_Injection(hidden_dim)\n",
    "            \n",
    "#         self.gcnn1 = GCNN(hidden_dim)\n",
    "#         self.gcnn2 = GCNN(hidden_dim+inject_embedding_dim)\n",
    "#         self.graph_generator = GenGraph(hidden_dim, virtual_nodes, lattice_step)\n",
    "        \n",
    "#         k = 4\n",
    "#         self.fc0 = nn.Linear(hidden_dim , hidden_dim+inject_embedding_dim)\n",
    "#         self.fc1 = nn.Linear(hidden_dim+inject_embedding_dim , hidden_dim * k)\n",
    "#         self.fc2 = nn.Linear(hidden_dim * (2+virtual_nodes) * k , 32)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.fc_out = nn.Linear(32, num_out_features)\n",
    "#         self.max_length = 0\n",
    "    \n",
    "#     def forward(self, g_data):\n",
    "            \n",
    "#         x = self.embedding(g_data.x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = x.T\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x1 = scatter_max(x, g_data.cumulative_token_indices, dim=1)[0]\n",
    "#         x2 = scatter_mean(x, g_data.cumulative_token_indices, dim=1)\n",
    "\n",
    "#         # if self.use_token_polarity[0]:\n",
    "#         #     x = torch.cat([x1, x2, g_data.token_sentiments.T], dim=0)\n",
    "#         # else:\n",
    "#         x = torch.cat([x1, x2], dim=0)\n",
    "            \n",
    "#         x = F.relu(self.conv3(x)).T\n",
    "        \n",
    "#         # if self.use_token_polarity[1]:\n",
    "#         x = self.sentiment1(x, g_data.token_sentiments)\n",
    "            \n",
    "#         if self.use_positional_encoder[0]:\n",
    "#             x = x + self.positional_encoding(g_data.token_positions)\n",
    "#         # x = x + self.positional_encoding(g_data.token_positions)\n",
    "\n",
    "#         rand_edges, lattice_edges = self.base_random_edges, self.base_lattice_edges\n",
    "            \n",
    "#         graph = self.graph_generator.gen_graph(x, len(g_data.token_lengths), g_data.num_tokens, rand_edges, lattice_edges, lattice_start_distance=self.lattice_start_distance)\n",
    "#         rand_edges = rand_edges-1\n",
    "#         lattice_edges = lattice_edges-1\n",
    "        \n",
    "        \n",
    "#         doc_token_index = torch.repeat_interleave(torch.arange(len(g_data.num_tokens), device=x.device), g_data.num_tokens)\n",
    "#         x, edge_weights = self.gcnn1(graph.x, graph.edge_index, return_attention_weights = True)\n",
    "#         edge_weights = edge_weights[1][:graph.edge_index.shape[1], 0]\n",
    "        \n",
    "#         graph = self.graph_generator.replace_unimportant_edges(edge_weights, x, graph.edge_index, len(g_data.token_lengths), g_data.num_tokens, rand_edges, lattice_edges, p_keep=2, lattice_start_distance=self.lattice_start_distance+1)\n",
    "        \n",
    "        \n",
    "#         # if self.use_token_polarity[2]:\n",
    "#         x = self.sentiment2(x, g_data.token_sentiments)\n",
    "            \n",
    "        \n",
    "#         if self.use_positional_encoder[1]:\n",
    "#             xa = graph.x[:g_data.token_embeddings.shape[0]] + self.positional_encoding(g_data.token_positions)\n",
    "#         else:\n",
    "#             xa = graph.x[:g_data.token_embeddings.shape[0]]\n",
    "#         if self.use_positional_encoder[2]:\n",
    "#             xb = g_data.token_embeddings + self.positional_encoding(g_data.token_positions)\n",
    "#         else:\n",
    "#             xb = g_data.token_embeddings\n",
    "#         x = torch.cat([xa, xb], dim=1)\n",
    "#         # x = torch.cat([graph.x[:g_data.token_embeddings.shape[0]], g_data.token_embeddings], dim=1)\n",
    "        \n",
    "#         x1 = F.relu(self.fc0(graph.x[g_data.token_embeddings.shape[0]:]))\n",
    "#         x = torch.cat([x, x1], dim=0)\n",
    "        \n",
    "#         x, edge_weights = self.gcnn2(x, graph.edge_index)\n",
    "\n",
    "#         x = F.elu_(self.fc1(x))\n",
    "#         x1 = scatter_max(x[:len(g_data.token_lengths)], doc_token_index, dim=0)[0]\n",
    "#         x2 = scatter_mean(x[:len(g_data.token_lengths)], doc_token_index, dim=0)\n",
    "#         vn_embs = x[len(g_data.token_lengths):]\n",
    "#         x_for_cat = [x1, x2]\n",
    "#         x_for_cat.extend([vn_embs[i*x1.shape[0]:(i+1)*x1.shape[0]] for i in range(self.virtual_nodes)])\n",
    "#         x = torch.cat(x_for_cat, dim=1)\n",
    "        \n",
    "#         x = F.elu_(self.fc2(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc_out(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "#     def create_positional_encoding(self):\n",
    "#         position = torch.arange(self.pos_emb_size).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, self.hidden_dim, 2) * (-math.log(10000.0) / self.hidden_dim))\n",
    "#         pe = torch.zeros(self.pos_emb_size, self.hidden_dim)\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         return torch.nn.Parameter(pe, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  ..., 70, 71, 72])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.token_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 6350])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xxx = torch.randn(3200, 127)\n",
    "\n",
    "xxx = torch.chunk(xxx, xxx.shape[0] // 64, dim=0)\n",
    "xxx = torch.cat(xxx, dim=1)\n",
    "xxx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5 ** False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATv2Conv, SimpleConv, summary\n",
    "\n",
    "class CNN_for_Text_No_Positional_Encoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embedding, pos_emb_size=8192, embedding_dim=64, hidden_dim=64, dropout=0.3, num_out_features=4, seed=-1, random_edges=4, lattice_edges=10, virtual_nodes=1, lattice_step=2, lattice_start_distance=2, inject_embedding_dim=64, isXaiTests=False, step_of_test = 0, num_tests=50, *args, **kwargs) -> None:\n",
    "        super(CNN_for_Text_No_Positional_Encoding, self).__init__(*args, **kwargs)\n",
    "        self.pos_emb_size = pos_emb_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.virtual_nodes = virtual_nodes\n",
    "        self.base_random_edges = random_edges\n",
    "        self.base_lattice_edges = lattice_edges\n",
    "        self.lattice_start_distance = lattice_start_distance\n",
    "        self.isXaiTests = int(isXaiTests)\n",
    "        self.num_tests = num_tests\n",
    "        self.step_of_test = step_of_test\n",
    "        # self.use_token_polarity = use_token_polarity\n",
    "        if seed>-1:\n",
    "            torch.manual_seed(seed)\n",
    " \n",
    "        self.embedding = nn.Embedding(num_embedding, embedding_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(embedding_dim, hidden_dim, kernel_size=5, padding=2)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, padding=2)\n",
    "        \n",
    "        # if self.use_token_polarity[0]:\n",
    "        #     self.conv3 = nn.Conv1d(2*hidden_dim + 2, hidden_dim, kernel_size=3, padding=1)\n",
    "        # else:\n",
    "        self.conv3 = nn.Conv1d(2*hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv4 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        \n",
    "        # if self.use_token_polarity[1]:\n",
    "        self.sentiment1  = Sentiment_Injection(hidden_dim)\n",
    "        # if self.use_token_polarity[2]:\n",
    "        self.sentiment2  = Sentiment_Injection(hidden_dim)\n",
    "            \n",
    "        self.gcnn1 = GCNN(hidden_dim)\n",
    "        self.gcnn2 = GCNN(hidden_dim+inject_embedding_dim)\n",
    "        self.graph_generator = GenGraph(hidden_dim, virtual_nodes, lattice_step)\n",
    "        \n",
    "        k = 4\n",
    "        self.fc0 = nn.Linear(hidden_dim , hidden_dim+inject_embedding_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim+inject_embedding_dim , hidden_dim * k)\n",
    "        self.fc2 = nn.Linear(hidden_dim * (2+virtual_nodes) * k , 32)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(32, num_out_features)\n",
    "        self.max_length = 0\n",
    "    \n",
    "    def forward(self, x, edge_index, token_indices, token_sentiments, token_lengths, num_tokens, character_length, token_embeddings):\n",
    "        # cumulative_token_indices = token_indices if not self.isXaiTests else self.caluculate_batch_token_positions(num_tokens, character_length, token_indices)\n",
    "        cumulative_token_indices = self.caluculate_batch_token_positions(num_tokens, character_length, token_indices)\n",
    "        \n",
    "        print(f'2: {x.shape}')\n",
    "        x = self.embedding(x)\n",
    "        print(f'2.5: {x.shape}')\n",
    "        x = self.dropout(x)\n",
    "        print(f'2.6: {x.shape}')\n",
    "        x = x.T\n",
    "        print(f'2.7: {x.shape}')\n",
    "        # x = self.refine_shape(1, x, 0)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        print(f'2.8: {x.shape}')\n",
    "        x = self.refine_shape(1, x, 0)\n",
    "        print(f'2.8 refined: {x.shape}')\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.refine_shape(2, x, 0)\n",
    "        print(f'2.9: {x.shape}')\n",
    "        x = self.dropout(x)\n",
    "        # x = self.refine_shape(4, x, 0)\n",
    "        print(f'3: {x.shape}')\n",
    "        x1 = scatter_max(x, cumulative_token_indices, dim=1)[0]\n",
    "        x2 = scatter_mean(x, cumulative_token_indices, dim=1)\n",
    "\n",
    "        # if self.use_token_polarity[0]:\n",
    "        #     x = torch.cat([x1, x2, g_data.token_sentiments.T], dim=0)\n",
    "        # else:\n",
    "        x = torch.cat([x1, x2], dim=0)\n",
    "            \n",
    "        print(f'4: {x.shape}')\n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        print(f'4.5: {x.shape}, self.hidden_dim: {self.hidden_dim}, self.is_tests_token_level: {self.step_of_test}')\n",
    "        # if self.isXaiTests and x.shape[0] != self.hidden_dim:\n",
    "        # x = torch.chunk(x, self.num_tests ** (1 - self.isXaiTests), dim=0)\n",
    "        x = self.refine_shape(3, x, 0)\n",
    "        \n",
    "        # x = torch.chunk(x, (x.shape[0] // self.hidden_dim)**self.is_tests_token_level, dim=0)\n",
    "        # x = torch.cat(x, dim=1)\n",
    "        \n",
    "        # if self.isXaiTests and x.shape[0] != self.hidden_dim:\n",
    "        #     x = torch.chunk(x, self.num_tests, dim=0)\n",
    "        #     x = torch.cat(x, dim=1)\n",
    "        # x = x.reshape(self.hidden_dim, -1)\n",
    "        print(\"abababdadasd\")\n",
    "        print(f'5: {x.shape}, {edge_index.shape}, {cumulative_token_indices.shape}, {token_sentiments.shape}, {token_lengths.shape}, {num_tokens.shape}, {character_length.shape}, {token_embeddings.shape}')\n",
    "        # if self.use_token_polarity[1]:\n",
    "        x = self.sentiment1(x.T, token_sentiments)\n",
    "\n",
    "        print(f'6: {x.shape}')\n",
    "        x = self.refine_shape(4, x, 1)\n",
    "        print(f'6 refined: {x.shape}')\n",
    "        rand_edges, lattice_edges = self.base_random_edges, self.base_lattice_edges\n",
    "            \n",
    "        graph = self.graph_generator.gen_graph(x, len(token_lengths), num_tokens, rand_edges, lattice_edges, lattice_start_distance=self.lattice_start_distance)\n",
    "        rand_edges = rand_edges-1\n",
    "        lattice_edges = lattice_edges-1\n",
    "        \n",
    "        print(f'7: {graph.x.shape}')\n",
    "        \n",
    "        doc_token_index = torch.repeat_interleave(torch.arange(len(num_tokens), device=x.device), num_tokens)\n",
    "        x, edge_weights = self.gcnn1(graph.x, graph.edge_index, return_attention_weights = True)\n",
    "        \n",
    "        print(f'7.1: {x.shape}')\n",
    "        x = self.refine_shape(5, x, 1)\n",
    "        print(f'7.2 refined: {x.shape}')\n",
    "        print(f'7.3 edge_weights: {len(edge_weights)}')\n",
    "        print(f'7.3 edge_weights: {edge_weights[0].shape}')\n",
    "        print(f'7.3 edge_weights: {edge_weights[1].shape}')\n",
    "        print(f'7.4 graph.edge_index.shape: {graph.edge_index.shape}')\n",
    "        edge_weights = edge_weights[1].unsqueeze(-1)\n",
    "        edge_weights = edge_weights[1][:min(graph.edge_index.shape[1], edge_weights[1].shape[0]), 0]\n",
    "        edge_weights = edge_weights.squeeze()\n",
    "        \n",
    "        print(f'7.1: {x.shape}')\n",
    "        x = self.refine_shape(5, x, 1)\n",
    "        print(f'7.2 refined: {x.shape}')\n",
    "        \n",
    "        graph = self.graph_generator.replace_unimportant_edges(edge_weights, x, graph.edge_index, len(token_lengths), num_tokens, rand_edges, lattice_edges, p_keep=2, lattice_start_distance=self.lattice_start_distance+1)\n",
    "        \n",
    "        print(f'8: {graph.x.shape}')\n",
    "        \n",
    "        # if self.use_token_polarity[2]:\n",
    "        x = self.sentiment2(x, token_sentiments)\n",
    "          \n",
    "        print(f'8.1: {x.shape}')\n",
    "        x = self.refine_shape(6, x, 1)\n",
    "        print(f'8.2 refined: {x.shape}')\n",
    "        \n",
    "        print(f'9: {x.shape}')  \n",
    "        xa = graph.x[:token_embeddings.shape[0]]\n",
    "        xb = token_embeddings\n",
    "        x = torch.cat([xa, xb], dim=1)\n",
    "        # x = torch.cat([graph.x[:g_data.token_embeddings.shape[0]], g_data.token_embeddings], dim=1)\n",
    "        \n",
    "        print(f'10: {x.shape}')  \n",
    "        x1 = F.relu(self.fc0(graph.x[token_embeddings.shape[0]:]))\n",
    "        x = torch.cat([x, x1], dim=0)\n",
    "        \n",
    "        print(f'11: {x.shape}')  \n",
    "        x, edge_weights = self.gcnn2(x, graph.edge_index)\n",
    "\n",
    "        print(f'12: {x.shape}')  \n",
    "        x = F.elu_(self.fc1(x))\n",
    "        x1 = scatter_max(x[:len(token_lengths)], doc_token_index, dim=0)[0]\n",
    "        x2 = scatter_mean(x[:len(token_lengths)], doc_token_index, dim=0)\n",
    "        vn_embs = x[len(token_lengths):]\n",
    "        x_for_cat = [x1, x2]\n",
    "        x_for_cat.extend([vn_embs[i*x1.shape[0]:(i+1)*x1.shape[0]] for i in range(self.virtual_nodes)])\n",
    "        x = torch.cat(x_for_cat, dim=1)\n",
    "        \n",
    "        print(f'13: {x.shape}')  \n",
    "        x = F.elu_(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc_out(x)\n",
    "        print(f'14: {x.shape}')  \n",
    "        return x\n",
    "    \n",
    "    def caluculate_batch_token_positions(self, num_tokens, character_length, token_indices):\n",
    "        cumsum_vals = torch.cumsum(num_tokens, dim=0).roll(1)\n",
    "        cumsum_vals[0] = 0\n",
    "        additions = torch.repeat_interleave(cumsum_vals, character_length)\n",
    "        cumulative_token_indices = token_indices + additions\n",
    "        return cumulative_token_indices\n",
    "    \n",
    "    def refine_shape(self, test_step, x, section=0):\n",
    "        x = torch.chunk(x, (x.shape[section] // self.hidden_dim)**(self.step_of_test==test_step), dim=0)\n",
    "        x = torch.cat(x, dim=1-section)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X.num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([63640, 2])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.token_sentiments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: torch.Size([341319])\n",
      "2.5: torch.Size([341319, 64])\n",
      "2.6: torch.Size([341319, 64])\n",
      "2.7: torch.Size([64, 341319])\n",
      "2.8: torch.Size([64, 341319])\n",
      "2.8 refined: torch.Size([64, 341319])\n",
      "2.9: torch.Size([64, 341319])\n",
      "3: torch.Size([64, 341319])\n",
      "4: torch.Size([128, 63640])\n",
      "4.5: torch.Size([64, 63640]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 63640]), torch.Size([2, 0]), torch.Size([341319]), torch.Size([63640, 2]), torch.Size([63640]), torch.Size([224]), torch.Size([224]), torch.Size([63640, 64])\n",
      "1 x: torch.Size([63640, 64])\n",
      "1 token_sentiments: torch.Size([63640, 2])\n",
      "6: torch.Size([63640, 64])\n",
      "6 refined: torch.Size([63640, 64])\n",
      "7: torch.Size([63640, 64])\n",
      "7.1: torch.Size([63640, 64])\n",
      "7.2 refined: torch.Size([63640, 64])\n",
      "7.3 edge_weights: 2\n",
      "7.3 edge_weights: torch.Size([2, 763680])\n",
      "7.3 edge_weights: torch.Size([763680, 4])\n",
      "7.4 graph.edge_index.shape: torch.Size([2, 763680])\n",
      "7.1: torch.Size([63640, 64])\n",
      "7.2 refined: torch.Size([63640, 64])\n",
      "Module                                              FLOP    % Total\n",
      "-----------------------------------------------  -------  ---------\n",
      " CNN_for_Text_No_Positional_Encoding.conv1       13.980B     39.30%\n",
      "  - aten.convolution                             13.980B     39.30%\n",
      "CNN_for_Text_No_Positional_Encoding              35.569B    100.00%\n",
      " - aten.convolution                              35.048B     98.53%\n",
      " - aten.addmm                                     0.521B      1.47%\n",
      " CNN_for_Text_No_Positional_Encoding.conv2       13.980B     39.30%\n",
      "  - aten.convolution                             13.980B     39.30%\n",
      " CNN_for_Text_No_Positional_Encoding.conv3        3.128B      8.79%\n",
      "  - aten.convolution                              3.128B      8.79%\n",
      " CNN_for_Text_No_Positional_Encoding.sentiment1   3.177B      8.93%\n",
      "  - aten.convolution                              3.177B      8.93%\n",
      " CNN_for_Text_No_Positional_Encoding.gcnn1        1.303B      3.66%\n",
      "  - aten.addmm                                    0.521B      1.47%\n",
      "  - aten.convolution                              0.782B      2.20%\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 63640]' is invalid for input of size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\CGNet\\FindBestModel\\5_LayersAttributionOnOutput\\with_positional_encoding.ipynb Cell 49\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#X63sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m flopt_counter \u001b[39m=\u001b[39m FlopCounterMode(classifier_torch_model)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#X63sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mwith\u001b[39;00m flopt_counter:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#X63sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     classifier_torch_model(X\u001b[39m.\u001b[39;49mx, torch\u001b[39m.\u001b[39;49mzeros((\u001b[39m2\u001b[39;49m, \u001b[39m0\u001b[39;49m)), X\u001b[39m.\u001b[39;49mtoken_indices, X\u001b[39m.\u001b[39;49mtoken_sentiments, X\u001b[39m.\u001b[39;49mtoken_lengths, X\u001b[39m.\u001b[39;49mnum_tokens, X\u001b[39m.\u001b[39;49mcharacter_length, X\u001b[39m.\u001b[39;49mtoken_embeddings)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1582\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1579\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1580\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1582\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1583\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1584\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[0;32m   1585\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[0;32m   1586\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[0;32m   1587\u001b[0m     ):\n\u001b[0;32m   1588\u001b[0m         \u001b[39m# mark that always called hook is run\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\CGNet\\FindBestModel\\5_LayersAttributionOnOutput\\with_positional_encoding.ipynb Cell 49\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#X63sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrefine_shape(\u001b[39m5\u001b[39m, x, \u001b[39m1\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#X63sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m7.2 refined: \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#X63sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m graph \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgraph_generator\u001b[39m.\u001b[39;49mreplace_unimportant_edges(edge_weights, x, graph\u001b[39m.\u001b[39;49medge_index, \u001b[39mlen\u001b[39;49m(token_lengths), num_tokens, rand_edges, lattice_edges, p_keep\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, lattice_start_distance\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlattice_start_distance\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#X63sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m8: \u001b[39m\u001b[39m{\u001b[39;00mgraph\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#X63sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m \u001b[39m# if self.use_token_polarity[2]:\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\CGNet\\FindBestModel\\5_LayersAttributionOnOutput\\with_positional_encoding.ipynb Cell 49\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#X63sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     important_indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtopk(edge_weights[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mv_n_e_counts]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, total_token_coutns), p_keep, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mindices\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#X63sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#X63sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     important_indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtopk(edge_weights\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, total_token_coutns), p_keep, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mindices\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#X63sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# important_indices = torch.topk(edge_weights[:-1*total_token_coutns].view(-1, total_token_coutns), 1, dim=0).indices.squeeze()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#X63sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m important_indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(total_token_coutns, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint64, device\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdevice) \u001b[39m+\u001b[39m important_indices\u001b[39m*\u001b[39mtotal_token_coutns\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\flop_counter.py:541\u001b[0m, in \u001b[0;36mFlopCounterMode.__torch_dispatch__\u001b[1;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__torch_dispatch__\u001b[39m(\u001b[39mself\u001b[39m, func, types, args\u001b[39m=\u001b[39m(), kwargs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    540\u001b[0m     kwargs \u001b[39m=\u001b[39m kwargs \u001b[39mif\u001b[39;00m kwargs \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m--> 541\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    542\u001b[0m     func_packet \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m_overloadpacket\n\u001b[0;32m    543\u001b[0m     \u001b[39mif\u001b[39;00m func_packet \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflop_registry:\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_ops.py:594\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[1;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(self_, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[0;32m    592\u001b[0m     \u001b[39m# use `self_` to avoid naming collide with aten ops arguments that\u001b[39;00m\n\u001b[0;32m    593\u001b[0m     \u001b[39m# are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[39;00m\n\u001b[1;32m--> 594\u001b[0m     \u001b[39mreturn\u001b[39;00m self_\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 63640]' is invalid for input of size 4"
     ]
    }
   ],
   "source": [
    "# for p1 in [False, True]:\n",
    "#     for p2 in [False, True]:\n",
    "#         for p3 in [False, True]:\n",
    "# print(f'\\n{p1}, {p2}, {p3}: \\n')\n",
    "classifier_torch_model = CNN_for_Text_No_Positional_Encoding(num_embedding=num_embedding, hidden_dim=64, embedding_dim=64, pos_emb_size=3080, dropout=0.2, num_out_features=len(class_id), seed=911, random_edges=4, lattice_edges=4, lattice_step=2, virtual_nodes=0, lattice_start_distance=2, isXaiTests=True, num_tests=len(X.num_tokens)).eval()\n",
    "flopt_counter = FlopCounterMode(classifier_torch_model)\n",
    "with flopt_counter:\n",
    "    classifier_torch_model(X.x, torch.zeros((2, 0)), X.token_indices, X.token_sentiments, X.token_lengths, X.num_tokens, X.character_length, X.token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(classifier_torch_model.parameters())).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CnnGnnClassifierLightningModel(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        num_classes,\n",
    "        optimizer=None,\n",
    "        loss_func=None,\n",
    "        learning_rate=0.01,\n",
    "        batch_size=64,\n",
    "        lr_scheduler=None,\n",
    "        user_lr_scheduler=False,\n",
    "        min_lr=0.0,\n",
    "    ):\n",
    "        super(CnnGnnClassifierLightningModel, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "        self.min_lr = min_lr\n",
    "        # self.save_hyperparameters(ignore=[\"model\"])\n",
    "        self.save_hyperparameters(logger=False)\n",
    "        self.optimizer = self._get_optimizer(optimizer)\n",
    "        self.lr_scheduler = (\n",
    "            self._get_lr_scheduler(lr_scheduler) if user_lr_scheduler else None\n",
    "        )\n",
    "        self.loss_func = loss_func\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.model(x.x, torch.zeros((2, 0)), x.token_indices, x.token_sentiments, x.token_lengths, x.num_tokens, x.character_length, x.token_embeddings)\n",
    "\n",
    "    def on_train_epoch_start(self) -> None:\n",
    "        param_groups = next(iter(self.optimizer.param_groups))\n",
    "        if \"lr\" in param_groups and param_groups[\"lr\"] is not None:\n",
    "            current_learning_rate = float(param_groups[\"lr\"])\n",
    "            self.log(\n",
    "                \"lr\",\n",
    "                current_learning_rate,\n",
    "                batch_size=self.batch_size,\n",
    "                on_epoch=True,\n",
    "                on_step=False,\n",
    "            )\n",
    "\n",
    "    def training_step(self, batch, *args, **kwargs):\n",
    "        X, y = batch\n",
    "        X.to(self.device)\n",
    "        y.to(self.device)\n",
    "        \n",
    "        self.model.train()\n",
    "        y_out = self(X)\n",
    "\n",
    "        loss = self.loss_func(y_out.view(y.shape), y )\n",
    "        self.train_losses.append(loss.detach().item())\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            batch_size=self.batch_size,\n",
    "            on_epoch=True,\n",
    "            on_step=True,\n",
    "        )\n",
    "        \n",
    "        self.train_acc(torch.argmax(y_out, dim=1), torch.argmax(y, dim=1))\n",
    "        self.log('train_acc', self.train_acc, prog_bar=True, on_epoch=True, on_step=True, batch_size=self.batch_size)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, *args, **kwargs):\n",
    "        X, y = batch\n",
    "        X.to(self.device)\n",
    "        y.to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        y_out = self(X)\n",
    "        loss = self.loss_func(y_out.view(y.shape), y )\n",
    "        self.val_losses.append(loss.detach().item())\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            batch_size=self.batch_size,\n",
    "            on_epoch=True,\n",
    "            on_step=True,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.val_acc(torch.argmax(y_out, dim=1), torch.argmax(y, dim=1))\n",
    "        self.log('val_acc', self.val_acc, prog_bar=True, on_epoch=True, on_step=True, batch_size=self.batch_size)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.lr_scheduler is None:\n",
    "            return self.optimizer\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": self.optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": self.lr_scheduler,\n",
    "                \"monitor\": \"train_loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def update_learning_rate(self, learning_rate: float):\n",
    "        self.learning_rate = learning_rate\n",
    "        for g in self.optimizer.param_groups:\n",
    "            g[\"lr\"] = learning_rate\n",
    "\n",
    "    def _get_optimizer(self, optimizer):\n",
    "        return (\n",
    "            optimizer\n",
    "            if optimizer is not None\n",
    "            else torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        )\n",
    "\n",
    "    def _get_lr_scheduler(self, lr_scheduler):\n",
    "        return (\n",
    "            lr_scheduler\n",
    "            if lr_scheduler is not None\n",
    "            else torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer, patience=5, factor=0.5, mode=\"min\", min_lr=self.min_lr\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from torchmetrics import ConfusionMatrix\n",
    "\n",
    "def calculate_metrics(cl_model, dataloader):\n",
    "    cm = ConfusionMatrix(task=\"multiclass\", num_classes=len(class_id))\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    cl_model = cl_model.eval()\n",
    "    cl_model.to(device)\n",
    "    for X, y in tqdm(dataloader):\n",
    "        X = X.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_p = cl_model(X)\n",
    "            y_p = y_p.cpu()\n",
    "        y_pred.append(y_p)\n",
    "        y_true.append(y)\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "    y_true = torch.cat(y_true, dim=0)\n",
    "    y_pred2 = torch.argmax(y_pred, dim=1)\n",
    "    y_true2 = torch.argmax(y_true, dim=1)\n",
    "    print(f'classification report: \\n {classification_report(y_true2, y_pred2, digits=4)}')\n",
    "    print(f'confusion matrix:\\n {cm(y_pred2, y_true2)}')\n",
    "    print('================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fardin Rastakhiz @ 2023\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from lightning.pytorch.loggers import Logger, CSVLogger\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from typing import List\n",
    "from pytorch_lightning.core.saving import save_hparams_to_yaml\n",
    "\n",
    "class ModelManager(ABC):\n",
    "\n",
    "    def __init__(self,\n",
    "                 torch_model: torch.nn.Module,\n",
    "                 lightning_model,\n",
    "                 model_save_dir: str = '~/Desktop',\n",
    "                 log_dir: str = 'logs/',\n",
    "                 log_name: str = 'model_logs',\n",
    "                 device='cpu',\n",
    "                 max_epochs = 100,\n",
    "                 ckpt_path: str|None=None,\n",
    "                 accumulate_grad_batches=1):\n",
    "        self.torch_model = torch_model\n",
    "        self.lightning_model = lightning_model\n",
    "        self.log_dir = log_dir\n",
    "        self.log_name = log_name\n",
    "        self.model_save_dir = model_save_dir\n",
    "        self.device = device\n",
    "        self.accelerator = 'cpu' if self.device=='cpu' else 'gpu'\n",
    "        self.max_epochs = max_epochs\n",
    "        self.ckpt_path = ckpt_path\n",
    "\n",
    "        self.logger = self._create_logger()\n",
    "        self.callbacks = self._create_callbacks()\n",
    "        self.trainer: L.Trainer = self._create_trainer(accumulate_grad_batches)\n",
    "        self.tuner = Tuner(self.trainer)\n",
    "        self.tuning_result = None\n",
    "\n",
    "    def tune(self, data_manager=None, train_dataloaders=None, val_dataloaders=None, datamodule=None, draw_result=True, min_lr=0.0000001, max_lr=0.1):\n",
    "        self.tuning_result = self.tuner.lr_find(self.lightning_model, datamodule=data_manager, train_dataloaders=train_dataloaders,\n",
    "                         val_dataloaders=val_dataloaders, min_lr=min_lr,max_lr=max_lr, num_training=150)\n",
    "        if draw_result:\n",
    "            fig = self.tuning_result.plot(suggest=True)\n",
    "            fig.show()\n",
    "        self.update_learning_rate(self.tuning_result.suggestion())\n",
    "        return self.tuning_result.suggestion()\n",
    "    \n",
    "    def update_learning_rate(self, lr):\n",
    "        self.lightning_model.update_learning_rate(lr)\n",
    "\n",
    "    def fit(self, train_dataloaders=None, val_dataloaders=None, datamodule=None, max_epochs = -1, ckpt_path=None):\n",
    "        if ckpt_path is not None and ckpt_path != '':\n",
    "            self.ckpt_path = ckpt_path\n",
    "        if max_epochs>0:\n",
    "            self.trainer.fit_loop.max_epochs = max_epochs\n",
    "            # self.max_epochs = max_epochs\n",
    "            # self.trainer = self._create_trainer()\n",
    "        self.trainer.fit(self.lightning_model,\n",
    "                         datamodule=datamodule,\n",
    "                         train_dataloaders=train_dataloaders,\n",
    "                         val_dataloaders=val_dataloaders,\n",
    "                         ckpt_path = self.ckpt_path\n",
    "                         )\n",
    "\n",
    "    def validate(self, dataloaders=None, datamodule=None):\n",
    "        return self.trainer.validate(self.lightning_model,\n",
    "                             datamodule=datamodule,\n",
    "                             dataloaders=dataloaders)\n",
    "\n",
    "    def predict(self, dataloaders=None, datamodule=None):\n",
    "        return self.trainer.predict(self.lightning_model,\n",
    "                             datamodule=datamodule,\n",
    "                             dataloaders=dataloaders)\n",
    "\n",
    "    def _create_trainer(self, accumulate_grad_batches) -> L.Trainer:\n",
    "        return L.Trainer(\n",
    "            callbacks=self.callbacks,\n",
    "            max_epochs=self.max_epochs,\n",
    "            accelerator=self.accelerator,\n",
    "            logger=self.logger,\n",
    "            num_sanity_val_steps=0,\n",
    "            default_root_dir=self.model_save_dir,\n",
    "            accumulate_grad_batches=accumulate_grad_batches\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def _create_callbacks(self) -> List[Callback]:\n",
    "        pass\n",
    "\n",
    "    def _create_logger(self) -> Logger:\n",
    "        return CSVLogger(save_dir=self.log_dir, name=self.log_name)\n",
    "\n",
    "    @abstractmethod\n",
    "    def draw_summary(self, dataloader):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def plot_csv_logger(self, loss_names, eval_names):\n",
    "        pass\n",
    "    \n",
    "    def save_hyper_parameters(self):\n",
    "        mhparams = {\n",
    "            'start_lr': 0.045,\n",
    "            'ckpt_lrs' :  {51: 0.002, 65: 0.00058},\n",
    "            'last_lr' : 0.0003,\n",
    "            'ac_loss_factor': 0.0002,\n",
    "            'weight_decay': 0.0012\n",
    "        }\n",
    "        save_hparams_to_yaml(config_yaml=r'logs\\hetero_model_17_AG\\version_12\\hparams.yaml',\n",
    "                     hparams=mhparams)\n",
    "        \n",
    "    # def find_best_settings(data_manager,\n",
    "    #                        lrs: List[float]=[0.001], dropouts: List[float]=[0.2], \n",
    "    #                        weight_decays: List[float]=[0.00055], emb_factors: List[float]=[0.1], \n",
    "    #                        batch_sizes: List[int]=[128], log_name='find_best_settings'):\n",
    "    #     for lr in lrs:\n",
    "    #         for dropout in dropouts:\n",
    "    #             for wd in weight_decays:\n",
    "    #                 for emb_factor in emb_factors:\n",
    "    #                     for bs in batch_sizes:\n",
    "    #                         data_manager.update_batch_size(bs)\n",
    "    #                         torch_model = HeteroGcnGatModel1(300, 1, X1.metadata(), 128, dropout=dropout)\n",
    "    #                         lightning_model = HeteroBinaryLightningModel(torch_model,\n",
    "    #                                         torch.optim.Adam(torch_model.parameters(), lr=lr, weight_decay=wd),\n",
    "    #                                             loss_func=HeteroLoss1(exception_keys='word', enc_factor=emb_factor),\n",
    "    #                                             learning_rate=lr,\n",
    "    #                                             batch_size=bs,\n",
    "    #                                             user_lr_scheduler=True\n",
    "    #                                             ).to(device)\n",
    "    #                         model_manager = ClassifierModelManager(torch_model, lightning_model, log_name=log_name, device=device, num_train_epoch=10)\n",
    "    #                         model_manager.fit(datamodule=data_manager)\n",
    "    #                         model_manager.save_plot_csv_logger(name_prepend=f'{lr}_{dropout}_{wd}_{emb_factor}_{bs}', loss_names=['train_loss', 'val_loss'], eval_names=['train_acc_epoch', 'val_acc_epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fardin Rastakhiz @ 2023\n",
    "import torch\n",
    "# from scripts.managers.ModelManager import ModelManager\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from torch_geometric.nn import summary\n",
    "from lightning.pytorch.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from os import path\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, hinge_loss\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "class ClassifierModelManager(ModelManager):\n",
    "\n",
    "    def __init__(self,\n",
    "                 torch_model: torch.nn.Module,\n",
    "                 lightning_model,\n",
    "                 model_save_dir: str = '~/Desktop',\n",
    "                 log_dir: str = 'logs/',\n",
    "                 log_name: str = 'model_logs',\n",
    "                 device='cpu',\n",
    "                 num_train_epoch = 100,\n",
    "                 accumulate_grad_batches=1):\n",
    "        super(ClassifierModelManager, self).__init__(torch_model, lightning_model, model_save_dir, log_dir, log_name, device, num_train_epoch, accumulate_grad_batches=accumulate_grad_batches)\n",
    "\n",
    "    def _create_callbacks(self) -> List[Callback]:\n",
    "        return [\n",
    "            ModelCheckpoint(save_top_k=2, mode='max', monitor='val_acc', save_last=True),\n",
    "            # EarlyStopping(patience=50, mode='max', monitor='val_acc')\n",
    "        ]\n",
    "\n",
    "    def draw_summary(self, dataloader):\n",
    "        X, y = next(iter(dataloader))\n",
    "        print(summary(self.torch_model, X.to(self.device)))\n",
    "\n",
    "    def plot_csv_logger(self, loss_names=['train_loss', 'val_loss'], eval_names=['train_acc', 'val_acc']):\n",
    "        csv_path = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', 'metrics.csv')\n",
    "        metrics = pd.read_csv(csv_path)\n",
    "\n",
    "        aggregation_metrics = []\n",
    "        agg_col = 'epoch'\n",
    "        for i, dfg in metrics.groupby(agg_col):\n",
    "            agg = dict(dfg.mean())\n",
    "            agg[agg_col] = i\n",
    "            aggregation_metrics.append(agg)\n",
    "\n",
    "        df_metrics = pd.DataFrame(aggregation_metrics)\n",
    "        df_metrics[loss_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='loss')\n",
    "        df_metrics[eval_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='accuracy')\n",
    "        plt.show()\n",
    "\n",
    "    def save_plot_csv_logger(self, loss_names=['train_loss', 'val_loss'], eval_names=['train_acc', 'val_acc'], name_prepend: str=\"\"):\n",
    "        csv_path = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', 'metrics.csv')\n",
    "        metrics = pd.read_csv(csv_path)\n",
    "\n",
    "        aggregation_metrics = []\n",
    "        agg_col = 'epoch'\n",
    "        for i, dfg in metrics.groupby(agg_col):\n",
    "            agg = dict(dfg.mean())\n",
    "            agg[agg_col] = i\n",
    "            aggregation_metrics.append(agg)\n",
    "\n",
    "        df_metrics = pd.DataFrame(aggregation_metrics)\n",
    "        df_metrics[loss_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='loss')\n",
    "        \n",
    "        loss_png = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', f'{name_prepend}_loss_metric.png')\n",
    "        plt.savefig(loss_png)\n",
    "        \n",
    "        df_metrics[eval_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='accuracy')\n",
    "        \n",
    "        acc_png = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', f'{name_prepend}_acc_metric.png')\n",
    "        plt.savefig(acc_png)\n",
    "        \n",
    "        plt.close()\n",
    "    \n",
    "    def evaluate(self, eval_dataloader,\n",
    "                 give_confusion_matrix: bool=True, \n",
    "                 give_report: bool=True, \n",
    "                 give_f1_score: bool=False, \n",
    "                 give_accuracy_score: bool=False, \n",
    "                 give_precision_score: bool=False, \n",
    "                 give_recall_score: bool=False, \n",
    "                 give_hinge_loss: bool=False):\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        self.lightning_model.eval()\n",
    "        for X, y in eval_dataloader:\n",
    "            y_p = self.lightning_model(X.to(self.device))\n",
    "            if type(y_p) is tuple:\n",
    "                y_p = y_p[0]\n",
    "            y_pred.append((y_p>0).to(torch.int32).detach().to(y.device))\n",
    "            y_true.append(y.to(torch.int32))\n",
    "        y_true = torch.concat(y_true)\n",
    "        y_pred = torch.concat(y_pred)\n",
    "        if(give_confusion_matrix):\n",
    "            print(f'confusion_matrix: \\n{confusion_matrix(y_true, y_pred)}')\n",
    "        if(give_report):\n",
    "            print(classification_report(y_true, y_pred))\n",
    "        if(give_f1_score):\n",
    "            print(f'f1_score: {f1_score(y_true, y_pred)}')\n",
    "        if(give_accuracy_score):\n",
    "            print(f'accuracy_score: {accuracy_score(y_true, y_pred)}')\n",
    "        if(give_precision_score):\n",
    "            print(f'precision_score: {precision_score(y_true, y_pred)}')\n",
    "        if(give_recall_score):\n",
    "            print(f'recall_score: {recall_score(y_true, y_pred)}')\n",
    "        # if(give_hinge_loss):\n",
    "        #     print(f'hinge_loss: {hinge_loss(y_true, y_pred)}')\n",
    "                \n",
    "    def evaluate_best_models(self, lightning_type: L.LightningModule, eval_dataloader,\n",
    "                             give_confusion_matrix: bool=True, \n",
    "                             give_report: bool=True, \n",
    "                             give_f1_score: bool=False, \n",
    "                             give_accuracy_score: bool=False, \n",
    "                             give_precision_score: bool=False, \n",
    "                             give_recall_score: bool=False, \n",
    "                             give_hinge_loss: bool=False,\n",
    "                             multi_class: bool=False, **kwargs):\n",
    "        self.lightning_model = lightning_type.load_from_checkpoint(rf'{self.trainer.checkpoint_callback.best_model_path}', map_location=None, hparams_file=None, strict=True, **kwargs).eval()\n",
    "        self.save_evaluation(eval_dataloader, 'best_model', give_confusion_matrix, give_report,\n",
    "                             give_f1_score, give_accuracy_score, give_precision_score, give_recall_score, give_hinge_loss, multi_class)\n",
    "            \n",
    "    def save_evaluation(self, eval_dataloader, name_prepend: str='',\n",
    "                    give_confusion_matrix: bool=True, \n",
    "                    give_report: bool=True, \n",
    "                    give_f1_score: bool=False, \n",
    "                    give_accuracy_score: bool=False, \n",
    "                    give_precision_score: bool=False, \n",
    "                    give_recall_score: bool=False, \n",
    "                    give_hinge_loss: bool=False,\n",
    "                    multi_class: bool=False\n",
    "                    ):\n",
    "            \n",
    "            test_metrics_path = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', f'{name_prepend}_test_metrics.txt')\n",
    "            \n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            self.lightning_model.eval()\n",
    "            self.lightning_model.model.eval()\n",
    "            self.torch_model.eval()\n",
    "            self.trainer.model.eval()\n",
    "            for X, y in eval_dataloader:\n",
    "                with torch.no_grad():\n",
    "                    y_p = self.lightning_model(X.to(self.device))\n",
    "                if type(y_p) is tuple:\n",
    "                    y_p = y_p[0]\n",
    "                \n",
    "                if multi_class:\n",
    "                    y_pred.append(y_p.detach().to(y.device))\n",
    "                    y_true.append(y)\n",
    "                else:\n",
    "                    y_pred.append((y_p>0).to(torch.int32).detach().to(y.device))\n",
    "                    y_true.append(y.to(torch.int32))\n",
    "                    \n",
    "            y_true = torch.concat(y_true)\n",
    "            y_pred = torch.concat(y_pred)\n",
    "            print(y_true.shape)\n",
    "            print(y_pred.shape)\n",
    "            if multi_class:\n",
    "                y_true_num = torch.argmax(y_true, dim=1)\n",
    "                y_pred_num = torch.argmax(y_pred, dim=1)\n",
    "            else:\n",
    "                y_true_num = y_true\n",
    "                y_pred_num = y_pred\n",
    "                \n",
    "            print(y_true_num.shape)\n",
    "            print(y_pred_num.shape)\n",
    "            with open(test_metrics_path, 'at+') as f:\n",
    "                if(give_confusion_matrix):\n",
    "                    print(f'confusion_matrix: \\n{confusion_matrix(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_report):\n",
    "                    print(classification_report(y_true_num, y_pred_num), file=f)\n",
    "                if(give_f1_score):\n",
    "                    if multi_class:\n",
    "                        print(f'f1_score: {f1_score(y_true_num, y_pred_num, average=None)}', file=f)\n",
    "                    else:\n",
    "                        print(f'f1_score: {f1_score(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_accuracy_score):\n",
    "                    print(f'accuracy_score: {accuracy_score(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_precision_score):\n",
    "                    if multi_class:\n",
    "                        print(f'precision: {precision_score(y_true_num, y_pred_num, average=None)}', file=f)\n",
    "                    else:\n",
    "                        print(f'precision: {precision_score(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_recall_score):\n",
    "                    if multi_class:\n",
    "                        print(f'recall: {recall_score(y_true_num, y_pred_num, average=None)}', file=f)\n",
    "                    else:\n",
    "                        print(f'recall: {recall_score(y_true_num, y_pred_num)}', file=f)\n",
    "                # if(give_hinge_loss):\n",
    "                #     print(f'hinge_loss: {hinge_loss(y_true_num, y_pred)}', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 128\n",
    "hidden_dim = 64\n",
    "embedding_dim = 64\n",
    "label_size = 1\n",
    "seed = 911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_size = 32\n",
    "# hidden_dim = 16\n",
    "# embedding_dim = 16\n",
    "# label_size = 1\n",
    "# seed = 911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "def train_model(epochs=30, dropout=0.25, weight_decay=0.000012, lr=0.0002, amsgrad=False, fused=True, use_positional_encoder=[False, False, False]):\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    classifier_torch_model = CNN_for_Text_No_Positional_Encoding(num_embedding=num_embedding, hidden_dim=hidden_dim, embedding_dim=embedding_dim, pos_emb_size=4096, dropout=dropout, num_out_features=len(class_id), seed=seed, random_edges=4, lattice_edges=8, lattice_step=2, virtual_nodes=0, lattice_start_distance=2).to(device)\n",
    "    # optimizer = torch.optim.Adam(classifier_torch_model.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=amsgrad, fused=fused)\n",
    "    optimizer = torch.optim.AdamW(classifier_torch_model.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=amsgrad, fused=fused)\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 100, 150, 200, 250, 300, 350],gamma=0.5, verbose=False)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15, 20, 30, 40, 45,50,55],gamma=0.5, verbose=False)\n",
    "    loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "    classfier_lightning_model = CnnGnnClassifierLightningModel(classifier_torch_model, \n",
    "                                                        num_classes=len(class_id),\n",
    "                                                learning_rate=lr,\n",
    "                                                batch_size=batch_size,\n",
    "                                                optimizer=optimizer,\n",
    "                                                loss_func=loss_func,\n",
    "                                                lr_scheduler=lr_scheduler,\n",
    "                                                user_lr_scheduler=True\n",
    "                                                ).to(device)\n",
    "\n",
    "\n",
    "    model_manager = ClassifierModelManager(classifier_torch_model, classfier_lightning_model, log_name=f'CNN-GNN_{use_positional_encoder[0]}_{use_positional_encoder[1]}_{use_positional_encoder[2]}',device=device, num_train_epoch=epochs, accumulate_grad_batches=1)\n",
    "    # trainer = L.Trainer(\n",
    "    #             # callbacks=callbacks,\n",
    "    #             max_epochs=epochs,\n",
    "    #             accelerator= 'gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    #             logger=CSVLogger(save_dir='logs/', name='log2'), \n",
    "    #             num_sanity_val_steps=0,\n",
    "    #         #     default_root_dir='models\\model2_word_embedding-256-2'\n",
    "    #         )\n",
    "\n",
    "    train_dataset.reset_params()\n",
    "    train_dataset.position_j = 0\n",
    "    test_dataset.reset_params()\n",
    "    test_dataset.position_j = 0\n",
    "    \n",
    "    # train_dataset.section_i = 0\n",
    "    # train_dataset.each_section_i = np.zeros((train_dataset.num_sections, ), dtype=int)\n",
    "    # test_dataset.section_i = 0\n",
    "    # test_dataset.each_section_i = np.zeros((test_dataset.num_sections, ), dtype=int)\n",
    "    \n",
    "    model_manager.fit(train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)\n",
    "    model_manager.save_plot_csv_logger(loss_names=['train_loss_epoch', 'val_loss_epoch'], eval_names=['train_acc_epoch', 'val_acc_epoch'], name_prepend=f'tests_{dropout}_{weight_decay}_{lr}_{amsgrad}_{fused}')\n",
    "    model_manager.torch_model = model_manager.torch_model.to(device)\n",
    "    model_manager.save_evaluation(test_dataloader, f'{dropout}_{weight_decay}_{lr}]',True, True, True, True, True, True, True, multi_class=True)\n",
    "    # trainer.fit(classfier_lightning_model, train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)\n",
    "    classfier_lightning_model.model = classfier_lightning_model.model.eval()\n",
    "    classfier_lightning_model = classfier_lightning_model.eval()\n",
    "    calculate_metrics(classfier_lightning_model, test_dataloader)\n",
    "    model_manager.evaluate_best_models(CnnGnnClassifierLightningModel, test_dataloader,True, True, True, True, True, True, True, multi_class=True, model=classifier_torch_model, num_classes=len(class_id))\n",
    "    return model_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16557"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_manager = train_model(70, 0.2, 0.000012, 0.0032, use_positional_encoder=[False, False, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculatge_metrics(chpt_path, target_data_loader, num_embedding):\n",
    "        classifier_torch_model = CNN_for_Text_No_Positional_Encoding(num_embedding=num_embedding, hidden_dim=hidden_dim, embedding_dim=embedding_dim, pos_emb_size=4096, dropout=0.2, num_out_features=len(class_id), seed=seed, random_edges=4, lattice_edges=8, lattice_step=2, virtual_nodes=0, lattice_start_distance=2)\n",
    "        classfier_lightning_model = CnnGnnClassifierLightningModel.load_from_checkpoint(chpt_path, model=classifier_torch_model, num_classes=len(class_id)).eval()\n",
    "        mean_infer_acc = []\n",
    "        mean_infer_f1 = []\n",
    "        mean_infer_prec = []\n",
    "        mean_infer_rec = []\n",
    "        for i in range(5):\n",
    "            all_ys = []\n",
    "            all_y_preds = []\n",
    "            for X, y in target_data_loader:\n",
    "                with torch.no_grad():\n",
    "                    y_pred = classfier_lightning_model(X.to(device))\n",
    "                all_ys.append(torch.argmax(y,dim=1))\n",
    "                all_y_preds.append(torch.argmax(y_pred.cpu(), dim=1))\n",
    "            all_ys = torch.concat(all_ys)\n",
    "            all_y_preds = torch.concat(all_y_preds)\n",
    "            \n",
    "            cm = confusion_matrix(all_ys, all_y_preds)\n",
    "            \n",
    "            accuracy = np.sum(np.diag(cm))/ np.sum(cm)\n",
    "            precision = np.mean(np.diag(cm) / np.sum(cm, axis=0))\n",
    "            recall = np.mean(np.diag(cm) / np.sum(cm, axis=1))\n",
    "            f1_score = (2*precision*recall)/(precision + recall)\n",
    "            \n",
    "            mean_infer_acc.append(accuracy)\n",
    "            mean_infer_f1.append(f1_score)\n",
    "            mean_infer_prec.append(precision)\n",
    "            mean_infer_rec.append(recall)\n",
    "        mean_infer_acc = torch.mean(torch.tensor(mean_infer_acc))\n",
    "        mean_infer_f1 = torch.mean(torch.tensor(mean_infer_f1))\n",
    "        mean_infer_prec = torch.mean(torch.tensor(mean_infer_prec))\n",
    "        mean_infer_rec = torch.mean(torch.tensor(mean_infer_rec))\n",
    "        return mean_infer_acc, mean_infer_f1, mean_infer_prec, mean_infer_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "def get_best_chpt(metrics_path, epoch_numbers):\n",
    "    epoch_data = pd.read_csv(metrics_path)\n",
    "    if 'val_acc_epoch' in epoch_data.columns and epoch_data['val_acc_epoch'].notna().any():\n",
    "        best_chpt = epoch_data.loc[epoch_data['val_acc_epoch'].idxmax()]\n",
    "    elif 'val_loss_epoch' in epoch_data.columns and epoch_data['val_loss_epoch'].notna().any():\n",
    "        best_chpt = epoch_data.loc[epoch_data['val_loss_epoch'].idxmin()]\n",
    "    else:\n",
    "        raise ValueError(f\"No valid validation metrics available for epoch {epoch_numbers}.\")\n",
    "    return np.argwhere(np.array(epoch_numbers)==best_chpt['epoch']).item(), best_chpt['val_loss_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_metrics_mean(num_embedding, base_path = 'logs\\CNN-GNN18_mr2k_seeds', start=0, interval=1):\n",
    "    total_accuracy = []\n",
    "    total_f1 = []\n",
    "    total_prec = []\n",
    "    total_rec = []\n",
    "    total_loss = []\n",
    "    \n",
    "    for i in range(start, start + interval):\n",
    "        version_path = join(base_path, f'version_{i}')\n",
    "        checkpoint_path = join(version_path, f'checkpoints')\n",
    "        onlyfiles  = [f for f in listdir(checkpoint_path) if (isfile(join(checkpoint_path, f)) and 'epoch' in f) ]\n",
    "        epoch_numbers = [int(re.search(r'\\d+', f).group()) for f in onlyfiles]\n",
    "        best_chpt_id, loss = get_best_chpt(join(version_path, 'metrics.csv'), epoch_numbers)\n",
    "        print(onlyfiles[best_chpt_id])\n",
    "        mean_infer_acc, mean_infer_f1, mean_infer_prec, mean_infer_rec = calculatge_metrics(join(checkpoint_path, f'{onlyfiles[best_chpt_id]}'), test_dataloader, num_embedding)\n",
    "            \n",
    "        total_accuracy.append(mean_infer_acc)\n",
    "        total_f1.append(mean_infer_f1)\n",
    "        total_prec.append(mean_infer_prec)\n",
    "        total_rec.append(mean_infer_rec)\n",
    "        total_loss.append(loss)\n",
    "\n",
    "    total_accuracy = torch.mean(torch.tensor(total_accuracy))\n",
    "    total_f1 = torch.mean(torch.tensor(total_f1))\n",
    "    total_prec = torch.mean(torch.tensor(total_prec))\n",
    "    total_rec = torch.mean(torch.tensor(total_rec))\n",
    "    total_loss = torch.mean(torch.tensor(total_loss))\n",
    "    print(f'total_accuracy: {total_accuracy}')\n",
    "    print(f'total_f1: {total_f1}')\n",
    "    print(f'total_prec: {total_prec}')\n",
    "    print(f'total_rec: {total_rec}')\n",
    "    print(f'total_loss: {total_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=36-step=222.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'loss_func' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_func'])`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: torch.Size([341136])\n",
      "2.5: torch.Size([341136, 64])\n",
      "2.6: torch.Size([341136, 64])\n",
      "2.7: torch.Size([64, 341136])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8: torch.Size([64, 341136])\n",
      "2.8 refined: torch.Size([64, 341136])\n",
      "2.9: torch.Size([64, 341136])\n",
      "3: torch.Size([64, 341136])\n",
      "4: torch.Size([128, 64108])\n",
      "4.5: torch.Size([64, 64108]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 64108]), torch.Size([2, 0]), torch.Size([341136]), torch.Size([64108, 2]), torch.Size([64108]), torch.Size([224]), torch.Size([224]), torch.Size([64108, 64])\n",
      "1 x: torch.Size([64108, 64])\n",
      "1 token_sentiments: torch.Size([64108, 2])\n",
      "6: torch.Size([64108, 64])\n",
      "6 refined: torch.Size([64108, 64])\n",
      "7: torch.Size([64108, 64])\n",
      "7.1: torch.Size([64108, 64])\n",
      "7.2 refined: torch.Size([64108, 64])\n",
      "8: torch.Size([64108, 64])\n",
      "1 x: torch.Size([64108, 64])\n",
      "1 token_sentiments: torch.Size([64108, 2])\n",
      "8.1: torch.Size([64108, 64])\n",
      "8.2 refined: torch.Size([64108, 64])\n",
      "9: torch.Size([64108, 64])\n",
      "10: torch.Size([64108, 128])\n",
      "11: torch.Size([64108, 128])\n",
      "12: torch.Size([64108, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([340331])\n",
      "2.5: torch.Size([340331, 64])\n",
      "2.6: torch.Size([340331, 64])\n",
      "2.7: torch.Size([64, 340331])\n",
      "2.8: torch.Size([64, 340331])\n",
      "2.8 refined: torch.Size([64, 340331])\n",
      "2.9: torch.Size([64, 340331])\n",
      "3: torch.Size([64, 340331])\n",
      "4: torch.Size([128, 62817])\n",
      "4.5: torch.Size([64, 62817]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 62817]), torch.Size([2, 0]), torch.Size([340331]), torch.Size([62817, 2]), torch.Size([62817]), torch.Size([224]), torch.Size([224]), torch.Size([62817, 64])\n",
      "1 x: torch.Size([62817, 64])\n",
      "1 token_sentiments: torch.Size([62817, 2])\n",
      "6: torch.Size([62817, 64])\n",
      "6 refined: torch.Size([62817, 64])\n",
      "7: torch.Size([62817, 64])\n",
      "7.1: torch.Size([62817, 64])\n",
      "7.2 refined: torch.Size([62817, 64])\n",
      "8: torch.Size([62817, 64])\n",
      "1 x: torch.Size([62817, 64])\n",
      "1 token_sentiments: torch.Size([62817, 2])\n",
      "8.1: torch.Size([62817, 64])\n",
      "8.2 refined: torch.Size([62817, 64])\n",
      "9: torch.Size([62817, 64])\n",
      "10: torch.Size([62817, 128])\n",
      "11: torch.Size([62817, 128])\n",
      "12: torch.Size([62817, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([339731])\n",
      "2.5: torch.Size([339731, 64])\n",
      "2.6: torch.Size([339731, 64])\n",
      "2.7: torch.Size([64, 339731])\n",
      "2.8: torch.Size([64, 339731])\n",
      "2.8 refined: torch.Size([64, 339731])\n",
      "2.9: torch.Size([64, 339731])\n",
      "3: torch.Size([64, 339731])\n",
      "4: torch.Size([128, 62866])\n",
      "4.5: torch.Size([64, 62866]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 62866]), torch.Size([2, 0]), torch.Size([339731]), torch.Size([62866, 2]), torch.Size([62866]), torch.Size([224]), torch.Size([224]), torch.Size([62866, 64])\n",
      "1 x: torch.Size([62866, 64])\n",
      "1 token_sentiments: torch.Size([62866, 2])\n",
      "6: torch.Size([62866, 64])\n",
      "6 refined: torch.Size([62866, 64])\n",
      "7: torch.Size([62866, 64])\n",
      "7.1: torch.Size([62866, 64])\n",
      "7.2 refined: torch.Size([62866, 64])\n",
      "8: torch.Size([62866, 64])\n",
      "1 x: torch.Size([62866, 64])\n",
      "1 token_sentiments: torch.Size([62866, 2])\n",
      "8.1: torch.Size([62866, 64])\n",
      "8.2 refined: torch.Size([62866, 64])\n",
      "9: torch.Size([62866, 64])\n",
      "10: torch.Size([62866, 128])\n",
      "11: torch.Size([62866, 128])\n",
      "12: torch.Size([62866, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([338319])\n",
      "2.5: torch.Size([338319, 64])\n",
      "2.6: torch.Size([338319, 64])\n",
      "2.7: torch.Size([64, 338319])\n",
      "2.8: torch.Size([64, 338319])\n",
      "2.8 refined: torch.Size([64, 338319])\n",
      "2.9: torch.Size([64, 338319])\n",
      "3: torch.Size([64, 338319])\n",
      "4: torch.Size([128, 62544])\n",
      "4.5: torch.Size([64, 62544]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 62544]), torch.Size([2, 0]), torch.Size([338319]), torch.Size([62544, 2]), torch.Size([62544]), torch.Size([224]), torch.Size([224]), torch.Size([62544, 64])\n",
      "1 x: torch.Size([62544, 64])\n",
      "1 token_sentiments: torch.Size([62544, 2])\n",
      "6: torch.Size([62544, 64])\n",
      "6 refined: torch.Size([62544, 64])\n",
      "7: torch.Size([62544, 64])\n",
      "7.1: torch.Size([62544, 64])\n",
      "7.2 refined: torch.Size([62544, 64])\n",
      "8: torch.Size([62544, 64])\n",
      "1 x: torch.Size([62544, 64])\n",
      "1 token_sentiments: torch.Size([62544, 2])\n",
      "8.1: torch.Size([62544, 64])\n",
      "8.2 refined: torch.Size([62544, 64])\n",
      "9: torch.Size([62544, 64])\n",
      "10: torch.Size([62544, 128])\n",
      "11: torch.Size([62544, 128])\n",
      "12: torch.Size([62544, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([337866])\n",
      "2.5: torch.Size([337866, 64])\n",
      "2.6: torch.Size([337866, 64])\n",
      "2.7: torch.Size([64, 337866])\n",
      "2.8: torch.Size([64, 337866])\n",
      "2.8 refined: torch.Size([64, 337866])\n",
      "2.9: torch.Size([64, 337866])\n",
      "3: torch.Size([64, 337866])\n",
      "4: torch.Size([128, 62930])\n",
      "4.5: torch.Size([64, 62930]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 62930]), torch.Size([2, 0]), torch.Size([337866]), torch.Size([62930, 2]), torch.Size([62930]), torch.Size([224]), torch.Size([224]), torch.Size([62930, 64])\n",
      "1 x: torch.Size([62930, 64])\n",
      "1 token_sentiments: torch.Size([62930, 2])\n",
      "6: torch.Size([62930, 64])\n",
      "6 refined: torch.Size([62930, 64])\n",
      "7: torch.Size([62930, 64])\n",
      "7.1: torch.Size([62930, 64])\n",
      "7.2 refined: torch.Size([62930, 64])\n",
      "8: torch.Size([62930, 64])\n",
      "1 x: torch.Size([62930, 64])\n",
      "1 token_sentiments: torch.Size([62930, 2])\n",
      "8.1: torch.Size([62930, 64])\n",
      "8.2 refined: torch.Size([62930, 64])\n",
      "9: torch.Size([62930, 64])\n",
      "10: torch.Size([62930, 128])\n",
      "11: torch.Size([62930, 128])\n",
      "12: torch.Size([62930, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([341177])\n",
      "2.5: torch.Size([341177, 64])\n",
      "2.6: torch.Size([341177, 64])\n",
      "2.7: torch.Size([64, 341177])\n",
      "2.8: torch.Size([64, 341177])\n",
      "2.8 refined: torch.Size([64, 341177])\n",
      "2.9: torch.Size([64, 341177])\n",
      "3: torch.Size([64, 341177])\n",
      "4: torch.Size([128, 63121])\n",
      "4.5: torch.Size([64, 63121]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 63121]), torch.Size([2, 0]), torch.Size([341177]), torch.Size([63121, 2]), torch.Size([63121]), torch.Size([224]), torch.Size([224]), torch.Size([63121, 64])\n",
      "1 x: torch.Size([63121, 64])\n",
      "1 token_sentiments: torch.Size([63121, 2])\n",
      "6: torch.Size([63121, 64])\n",
      "6 refined: torch.Size([63121, 64])\n",
      "7: torch.Size([63121, 64])\n",
      "7.1: torch.Size([63121, 64])\n",
      "7.2 refined: torch.Size([63121, 64])\n",
      "8: torch.Size([63121, 64])\n",
      "1 x: torch.Size([63121, 64])\n",
      "1 token_sentiments: torch.Size([63121, 2])\n",
      "8.1: torch.Size([63121, 64])\n",
      "8.2 refined: torch.Size([63121, 64])\n",
      "9: torch.Size([63121, 64])\n",
      "10: torch.Size([63121, 128])\n",
      "11: torch.Size([63121, 128])\n",
      "12: torch.Size([63121, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([340437])\n",
      "2.5: torch.Size([340437, 64])\n",
      "2.6: torch.Size([340437, 64])\n",
      "2.7: torch.Size([64, 340437])\n",
      "2.8: torch.Size([64, 340437])\n",
      "2.8 refined: torch.Size([64, 340437])\n",
      "2.9: torch.Size([64, 340437])\n",
      "3: torch.Size([64, 340437])\n",
      "4: torch.Size([128, 63060])\n",
      "4.5: torch.Size([64, 63060]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 63060]), torch.Size([2, 0]), torch.Size([340437]), torch.Size([63060, 2]), torch.Size([63060]), torch.Size([224]), torch.Size([224]), torch.Size([63060, 64])\n",
      "1 x: torch.Size([63060, 64])\n",
      "1 token_sentiments: torch.Size([63060, 2])\n",
      "6: torch.Size([63060, 64])\n",
      "6 refined: torch.Size([63060, 64])\n",
      "7: torch.Size([63060, 64])\n",
      "7.1: torch.Size([63060, 64])\n",
      "7.2 refined: torch.Size([63060, 64])\n",
      "8: torch.Size([63060, 64])\n",
      "1 x: torch.Size([63060, 64])\n",
      "1 token_sentiments: torch.Size([63060, 2])\n",
      "8.1: torch.Size([63060, 64])\n",
      "8.2 refined: torch.Size([63060, 64])\n",
      "9: torch.Size([63060, 64])\n",
      "10: torch.Size([63060, 128])\n",
      "11: torch.Size([63060, 128])\n",
      "12: torch.Size([63060, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([340183])\n",
      "2.5: torch.Size([340183, 64])\n",
      "2.6: torch.Size([340183, 64])\n",
      "2.7: torch.Size([64, 340183])\n",
      "2.8: torch.Size([64, 340183])\n",
      "2.8 refined: torch.Size([64, 340183])\n",
      "2.9: torch.Size([64, 340183])\n",
      "3: torch.Size([64, 340183])\n",
      "4: torch.Size([128, 63039])\n",
      "4.5: torch.Size([64, 63039]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 63039]), torch.Size([2, 0]), torch.Size([340183]), torch.Size([63039, 2]), torch.Size([63039]), torch.Size([224]), torch.Size([224]), torch.Size([63039, 64])\n",
      "1 x: torch.Size([63039, 64])\n",
      "1 token_sentiments: torch.Size([63039, 2])\n",
      "6: torch.Size([63039, 64])\n",
      "6 refined: torch.Size([63039, 64])\n",
      "7: torch.Size([63039, 64])\n",
      "7.1: torch.Size([63039, 64])\n",
      "7.2 refined: torch.Size([63039, 64])\n",
      "8: torch.Size([63039, 64])\n",
      "1 x: torch.Size([63039, 64])\n",
      "1 token_sentiments: torch.Size([63039, 2])\n",
      "8.1: torch.Size([63039, 64])\n",
      "8.2 refined: torch.Size([63039, 64])\n",
      "9: torch.Size([63039, 64])\n",
      "10: torch.Size([63039, 128])\n",
      "11: torch.Size([63039, 128])\n",
      "12: torch.Size([63039, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([339754])\n",
      "2.5: torch.Size([339754, 64])\n",
      "2.6: torch.Size([339754, 64])\n",
      "2.7: torch.Size([64, 339754])\n",
      "2.8: torch.Size([64, 339754])\n",
      "2.8 refined: torch.Size([64, 339754])\n",
      "2.9: torch.Size([64, 339754])\n",
      "3: torch.Size([64, 339754])\n",
      "4: torch.Size([128, 63575])\n",
      "4.5: torch.Size([64, 63575]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 63575]), torch.Size([2, 0]), torch.Size([339754]), torch.Size([63575, 2]), torch.Size([63575]), torch.Size([224]), torch.Size([224]), torch.Size([63575, 64])\n",
      "1 x: torch.Size([63575, 64])\n",
      "1 token_sentiments: torch.Size([63575, 2])\n",
      "6: torch.Size([63575, 64])\n",
      "6 refined: torch.Size([63575, 64])\n",
      "7: torch.Size([63575, 64])\n",
      "7.1: torch.Size([63575, 64])\n",
      "7.2 refined: torch.Size([63575, 64])\n",
      "8: torch.Size([63575, 64])\n",
      "1 x: torch.Size([63575, 64])\n",
      "1 token_sentiments: torch.Size([63575, 2])\n",
      "8.1: torch.Size([63575, 64])\n",
      "8.2 refined: torch.Size([63575, 64])\n",
      "9: torch.Size([63575, 64])\n",
      "10: torch.Size([63575, 128])\n",
      "11: torch.Size([63575, 128])\n",
      "12: torch.Size([63575, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([339011])\n",
      "2.5: torch.Size([339011, 64])\n",
      "2.6: torch.Size([339011, 64])\n",
      "2.7: torch.Size([64, 339011])\n",
      "2.8: torch.Size([64, 339011])\n",
      "2.8 refined: torch.Size([64, 339011])\n",
      "2.9: torch.Size([64, 339011])\n",
      "3: torch.Size([64, 339011])\n",
      "4: torch.Size([128, 63677])\n",
      "4.5: torch.Size([64, 63677]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 63677]), torch.Size([2, 0]), torch.Size([339011]), torch.Size([63677, 2]), torch.Size([63677]), torch.Size([224]), torch.Size([224]), torch.Size([63677, 64])\n",
      "1 x: torch.Size([63677, 64])\n",
      "1 token_sentiments: torch.Size([63677, 2])\n",
      "6: torch.Size([63677, 64])\n",
      "6 refined: torch.Size([63677, 64])\n",
      "7: torch.Size([63677, 64])\n",
      "7.1: torch.Size([63677, 64])\n",
      "7.2 refined: torch.Size([63677, 64])\n",
      "8: torch.Size([63677, 64])\n",
      "1 x: torch.Size([63677, 64])\n",
      "1 token_sentiments: torch.Size([63677, 2])\n",
      "8.1: torch.Size([63677, 64])\n",
      "8.2 refined: torch.Size([63677, 64])\n",
      "9: torch.Size([63677, 64])\n",
      "10: torch.Size([63677, 128])\n",
      "11: torch.Size([63677, 128])\n",
      "12: torch.Size([63677, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([338140])\n",
      "2.5: torch.Size([338140, 64])\n",
      "2.6: torch.Size([338140, 64])\n",
      "2.7: torch.Size([64, 338140])\n",
      "2.8: torch.Size([64, 338140])\n",
      "2.8 refined: torch.Size([64, 338140])\n",
      "2.9: torch.Size([64, 338140])\n",
      "3: torch.Size([64, 338140])\n",
      "4: torch.Size([128, 62433])\n",
      "4.5: torch.Size([64, 62433]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 62433]), torch.Size([2, 0]), torch.Size([338140]), torch.Size([62433, 2]), torch.Size([62433]), torch.Size([224]), torch.Size([224]), torch.Size([62433, 64])\n",
      "1 x: torch.Size([62433, 64])\n",
      "1 token_sentiments: torch.Size([62433, 2])\n",
      "6: torch.Size([62433, 64])\n",
      "6 refined: torch.Size([62433, 64])\n",
      "7: torch.Size([62433, 64])\n",
      "7.1: torch.Size([62433, 64])\n",
      "7.2 refined: torch.Size([62433, 64])\n",
      "8: torch.Size([62433, 64])\n",
      "1 x: torch.Size([62433, 64])\n",
      "1 token_sentiments: torch.Size([62433, 2])\n",
      "8.1: torch.Size([62433, 64])\n",
      "8.2 refined: torch.Size([62433, 64])\n",
      "9: torch.Size([62433, 64])\n",
      "10: torch.Size([62433, 128])\n",
      "11: torch.Size([62433, 128])\n",
      "12: torch.Size([62433, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([341023])\n",
      "2.5: torch.Size([341023, 64])\n",
      "2.6: torch.Size([341023, 64])\n",
      "2.7: torch.Size([64, 341023])\n",
      "2.8: torch.Size([64, 341023])\n",
      "2.8 refined: torch.Size([64, 341023])\n",
      "2.9: torch.Size([64, 341023])\n",
      "3: torch.Size([64, 341023])\n",
      "4: torch.Size([128, 63614])\n",
      "4.5: torch.Size([64, 63614]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 63614]), torch.Size([2, 0]), torch.Size([341023]), torch.Size([63614, 2]), torch.Size([63614]), torch.Size([224]), torch.Size([224]), torch.Size([63614, 64])\n",
      "1 x: torch.Size([63614, 64])\n",
      "1 token_sentiments: torch.Size([63614, 2])\n",
      "6: torch.Size([63614, 64])\n",
      "6 refined: torch.Size([63614, 64])\n",
      "7: torch.Size([63614, 64])\n",
      "7.1: torch.Size([63614, 64])\n",
      "7.2 refined: torch.Size([63614, 64])\n",
      "8: torch.Size([63614, 64])\n",
      "1 x: torch.Size([63614, 64])\n",
      "1 token_sentiments: torch.Size([63614, 2])\n",
      "8.1: torch.Size([63614, 64])\n",
      "8.2 refined: torch.Size([63614, 64])\n",
      "9: torch.Size([63614, 64])\n",
      "10: torch.Size([63614, 128])\n",
      "11: torch.Size([63614, 128])\n",
      "12: torch.Size([63614, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([340430])\n",
      "2.5: torch.Size([340430, 64])\n",
      "2.6: torch.Size([340430, 64])\n",
      "2.7: torch.Size([64, 340430])\n",
      "2.8: torch.Size([64, 340430])\n",
      "2.8 refined: torch.Size([64, 340430])\n",
      "2.9: torch.Size([64, 340430])\n",
      "3: torch.Size([64, 340430])\n",
      "4: torch.Size([128, 63406])\n",
      "4.5: torch.Size([64, 63406]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 63406]), torch.Size([2, 0]), torch.Size([340430]), torch.Size([63406, 2]), torch.Size([63406]), torch.Size([224]), torch.Size([224]), torch.Size([63406, 64])\n",
      "1 x: torch.Size([63406, 64])\n",
      "1 token_sentiments: torch.Size([63406, 2])\n",
      "6: torch.Size([63406, 64])\n",
      "6 refined: torch.Size([63406, 64])\n",
      "7: torch.Size([63406, 64])\n",
      "7.1: torch.Size([63406, 64])\n",
      "7.2 refined: torch.Size([63406, 64])\n",
      "8: torch.Size([63406, 64])\n",
      "1 x: torch.Size([63406, 64])\n",
      "1 token_sentiments: torch.Size([63406, 2])\n",
      "8.1: torch.Size([63406, 64])\n",
      "8.2 refined: torch.Size([63406, 64])\n",
      "9: torch.Size([63406, 64])\n",
      "10: torch.Size([63406, 128])\n",
      "11: torch.Size([63406, 128])\n",
      "12: torch.Size([63406, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([339900])\n",
      "2.5: torch.Size([339900, 64])\n",
      "2.6: torch.Size([339900, 64])\n",
      "2.7: torch.Size([64, 339900])\n",
      "2.8: torch.Size([64, 339900])\n",
      "2.8 refined: torch.Size([64, 339900])\n",
      "2.9: torch.Size([64, 339900])\n",
      "3: torch.Size([64, 339900])\n",
      "4: torch.Size([128, 62646])\n",
      "4.5: torch.Size([64, 62646]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 62646]), torch.Size([2, 0]), torch.Size([339900]), torch.Size([62646, 2]), torch.Size([62646]), torch.Size([224]), torch.Size([224]), torch.Size([62646, 64])\n",
      "1 x: torch.Size([62646, 64])\n",
      "1 token_sentiments: torch.Size([62646, 2])\n",
      "6: torch.Size([62646, 64])\n",
      "6 refined: torch.Size([62646, 64])\n",
      "7: torch.Size([62646, 64])\n",
      "7.1: torch.Size([62646, 64])\n",
      "7.2 refined: torch.Size([62646, 64])\n",
      "8: torch.Size([62646, 64])\n",
      "1 x: torch.Size([62646, 64])\n",
      "1 token_sentiments: torch.Size([62646, 2])\n",
      "8.1: torch.Size([62646, 64])\n",
      "8.2 refined: torch.Size([62646, 64])\n",
      "9: torch.Size([62646, 64])\n",
      "10: torch.Size([62646, 128])\n",
      "11: torch.Size([62646, 128])\n",
      "12: torch.Size([62646, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([339880])\n",
      "2.5: torch.Size([339880, 64])\n",
      "2.6: torch.Size([339880, 64])\n",
      "2.7: torch.Size([64, 339880])\n",
      "2.8: torch.Size([64, 339880])\n",
      "2.8 refined: torch.Size([64, 339880])\n",
      "2.9: torch.Size([64, 339880])\n",
      "3: torch.Size([64, 339880])\n",
      "4: torch.Size([128, 63252])\n",
      "4.5: torch.Size([64, 63252]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 63252]), torch.Size([2, 0]), torch.Size([339880]), torch.Size([63252, 2]), torch.Size([63252]), torch.Size([224]), torch.Size([224]), torch.Size([63252, 64])\n",
      "1 x: torch.Size([63252, 64])\n",
      "1 token_sentiments: torch.Size([63252, 2])\n",
      "6: torch.Size([63252, 64])\n",
      "6 refined: torch.Size([63252, 64])\n",
      "7: torch.Size([63252, 64])\n",
      "7.1: torch.Size([63252, 64])\n",
      "7.2 refined: torch.Size([63252, 64])\n",
      "8: torch.Size([63252, 64])\n",
      "1 x: torch.Size([63252, 64])\n",
      "1 token_sentiments: torch.Size([63252, 2])\n",
      "8.1: torch.Size([63252, 64])\n",
      "8.2 refined: torch.Size([63252, 64])\n",
      "9: torch.Size([63252, 64])\n",
      "10: torch.Size([63252, 128])\n",
      "11: torch.Size([63252, 128])\n",
      "12: torch.Size([63252, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([339845])\n",
      "2.5: torch.Size([339845, 64])\n",
      "2.6: torch.Size([339845, 64])\n",
      "2.7: torch.Size([64, 339845])\n",
      "2.8: torch.Size([64, 339845])\n",
      "2.8 refined: torch.Size([64, 339845])\n",
      "2.9: torch.Size([64, 339845])\n",
      "3: torch.Size([64, 339845])\n",
      "4: torch.Size([128, 62749])\n",
      "4.5: torch.Size([64, 62749]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 62749]), torch.Size([2, 0]), torch.Size([339845]), torch.Size([62749, 2]), torch.Size([62749]), torch.Size([224]), torch.Size([224]), torch.Size([62749, 64])\n",
      "1 x: torch.Size([62749, 64])\n",
      "1 token_sentiments: torch.Size([62749, 2])\n",
      "6: torch.Size([62749, 64])\n",
      "6 refined: torch.Size([62749, 64])\n",
      "7: torch.Size([62749, 64])\n",
      "7.1: torch.Size([62749, 64])\n",
      "7.2 refined: torch.Size([62749, 64])\n",
      "8: torch.Size([62749, 64])\n",
      "1 x: torch.Size([62749, 64])\n",
      "1 token_sentiments: torch.Size([62749, 2])\n",
      "8.1: torch.Size([62749, 64])\n",
      "8.2 refined: torch.Size([62749, 64])\n",
      "9: torch.Size([62749, 64])\n",
      "10: torch.Size([62749, 128])\n",
      "11: torch.Size([62749, 128])\n",
      "12: torch.Size([62749, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([337624])\n",
      "2.5: torch.Size([337624, 64])\n",
      "2.6: torch.Size([337624, 64])\n",
      "2.7: torch.Size([64, 337624])\n",
      "2.8: torch.Size([64, 337624])\n",
      "2.8 refined: torch.Size([64, 337624])\n",
      "2.9: torch.Size([64, 337624])\n",
      "3: torch.Size([64, 337624])\n",
      "4: torch.Size([128, 63238])\n",
      "4.5: torch.Size([64, 63238]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 63238]), torch.Size([2, 0]), torch.Size([337624]), torch.Size([63238, 2]), torch.Size([63238]), torch.Size([224]), torch.Size([224]), torch.Size([63238, 64])\n",
      "1 x: torch.Size([63238, 64])\n",
      "1 token_sentiments: torch.Size([63238, 2])\n",
      "6: torch.Size([63238, 64])\n",
      "6 refined: torch.Size([63238, 64])\n",
      "7: torch.Size([63238, 64])\n",
      "7.1: torch.Size([63238, 64])\n",
      "7.2 refined: torch.Size([63238, 64])\n",
      "8: torch.Size([63238, 64])\n",
      "1 x: torch.Size([63238, 64])\n",
      "1 token_sentiments: torch.Size([63238, 2])\n",
      "8.1: torch.Size([63238, 64])\n",
      "8.2 refined: torch.Size([63238, 64])\n",
      "9: torch.Size([63238, 64])\n",
      "10: torch.Size([63238, 128])\n",
      "11: torch.Size([63238, 128])\n",
      "12: torch.Size([63238, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([341683])\n",
      "2.5: torch.Size([341683, 64])\n",
      "2.6: torch.Size([341683, 64])\n",
      "2.7: torch.Size([64, 341683])\n",
      "2.8: torch.Size([64, 341683])\n",
      "2.8 refined: torch.Size([64, 341683])\n",
      "2.9: torch.Size([64, 341683])\n",
      "3: torch.Size([64, 341683])\n",
      "4: torch.Size([128, 63763])\n",
      "4.5: torch.Size([64, 63763]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 63763]), torch.Size([2, 0]), torch.Size([341683]), torch.Size([63763, 2]), torch.Size([63763]), torch.Size([224]), torch.Size([224]), torch.Size([63763, 64])\n",
      "1 x: torch.Size([63763, 64])\n",
      "1 token_sentiments: torch.Size([63763, 2])\n",
      "6: torch.Size([63763, 64])\n",
      "6 refined: torch.Size([63763, 64])\n",
      "7: torch.Size([63763, 64])\n",
      "7.1: torch.Size([63763, 64])\n",
      "7.2 refined: torch.Size([63763, 64])\n",
      "8: torch.Size([63763, 64])\n",
      "1 x: torch.Size([63763, 64])\n",
      "1 token_sentiments: torch.Size([63763, 2])\n",
      "8.1: torch.Size([63763, 64])\n",
      "8.2 refined: torch.Size([63763, 64])\n",
      "9: torch.Size([63763, 64])\n",
      "10: torch.Size([63763, 128])\n",
      "11: torch.Size([63763, 128])\n",
      "12: torch.Size([63763, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([341029])\n",
      "2.5: torch.Size([341029, 64])\n",
      "2.6: torch.Size([341029, 64])\n",
      "2.7: torch.Size([64, 341029])\n",
      "2.8: torch.Size([64, 341029])\n",
      "2.8 refined: torch.Size([64, 341029])\n",
      "2.9: torch.Size([64, 341029])\n",
      "3: torch.Size([64, 341029])\n",
      "4: torch.Size([128, 63584])\n",
      "4.5: torch.Size([64, 63584]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 63584]), torch.Size([2, 0]), torch.Size([341029]), torch.Size([63584, 2]), torch.Size([63584]), torch.Size([224]), torch.Size([224]), torch.Size([63584, 64])\n",
      "1 x: torch.Size([63584, 64])\n",
      "1 token_sentiments: torch.Size([63584, 2])\n",
      "6: torch.Size([63584, 64])\n",
      "6 refined: torch.Size([63584, 64])\n",
      "7: torch.Size([63584, 64])\n",
      "7.1: torch.Size([63584, 64])\n",
      "7.2 refined: torch.Size([63584, 64])\n",
      "8: torch.Size([63584, 64])\n",
      "1 x: torch.Size([63584, 64])\n",
      "1 token_sentiments: torch.Size([63584, 2])\n",
      "8.1: torch.Size([63584, 64])\n",
      "8.2 refined: torch.Size([63584, 64])\n",
      "9: torch.Size([63584, 64])\n",
      "10: torch.Size([63584, 128])\n",
      "11: torch.Size([63584, 128])\n",
      "12: torch.Size([63584, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([340197])\n",
      "2.5: torch.Size([340197, 64])\n",
      "2.6: torch.Size([340197, 64])\n",
      "2.7: torch.Size([64, 340197])\n",
      "2.8: torch.Size([64, 340197])\n",
      "2.8 refined: torch.Size([64, 340197])\n",
      "2.9: torch.Size([64, 340197])\n",
      "3: torch.Size([64, 340197])\n",
      "4: torch.Size([128, 62857])\n",
      "4.5: torch.Size([64, 62857]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 62857]), torch.Size([2, 0]), torch.Size([340197]), torch.Size([62857, 2]), torch.Size([62857]), torch.Size([224]), torch.Size([224]), torch.Size([62857, 64])\n",
      "1 x: torch.Size([62857, 64])\n",
      "1 token_sentiments: torch.Size([62857, 2])\n",
      "6: torch.Size([62857, 64])\n",
      "6 refined: torch.Size([62857, 64])\n",
      "7: torch.Size([62857, 64])\n",
      "7.1: torch.Size([62857, 64])\n",
      "7.2 refined: torch.Size([62857, 64])\n",
      "8: torch.Size([62857, 64])\n",
      "1 x: torch.Size([62857, 64])\n",
      "1 token_sentiments: torch.Size([62857, 2])\n",
      "8.1: torch.Size([62857, 64])\n",
      "8.2 refined: torch.Size([62857, 64])\n",
      "9: torch.Size([62857, 64])\n",
      "10: torch.Size([62857, 128])\n",
      "11: torch.Size([62857, 128])\n",
      "12: torch.Size([62857, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([338741])\n",
      "2.5: torch.Size([338741, 64])\n",
      "2.6: torch.Size([338741, 64])\n",
      "2.7: torch.Size([64, 338741])\n",
      "2.8: torch.Size([64, 338741])\n",
      "2.8 refined: torch.Size([64, 338741])\n",
      "2.9: torch.Size([64, 338741])\n",
      "3: torch.Size([64, 338741])\n",
      "4: torch.Size([128, 63088])\n",
      "4.5: torch.Size([64, 63088]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 63088]), torch.Size([2, 0]), torch.Size([338741]), torch.Size([63088, 2]), torch.Size([63088]), torch.Size([224]), torch.Size([224]), torch.Size([63088, 64])\n",
      "1 x: torch.Size([63088, 64])\n",
      "1 token_sentiments: torch.Size([63088, 2])\n",
      "6: torch.Size([63088, 64])\n",
      "6 refined: torch.Size([63088, 64])\n",
      "7: torch.Size([63088, 64])\n",
      "7.1: torch.Size([63088, 64])\n",
      "7.2 refined: torch.Size([63088, 64])\n",
      "8: torch.Size([63088, 64])\n",
      "1 x: torch.Size([63088, 64])\n",
      "1 token_sentiments: torch.Size([63088, 2])\n",
      "8.1: torch.Size([63088, 64])\n",
      "8.2 refined: torch.Size([63088, 64])\n",
      "9: torch.Size([63088, 64])\n",
      "10: torch.Size([63088, 128])\n",
      "11: torch.Size([63088, 128])\n",
      "12: torch.Size([63088, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([338727])\n",
      "2.5: torch.Size([338727, 64])\n",
      "2.6: torch.Size([338727, 64])\n",
      "2.7: torch.Size([64, 338727])\n",
      "2.8: torch.Size([64, 338727])\n",
      "2.8 refined: torch.Size([64, 338727])\n",
      "2.9: torch.Size([64, 338727])\n",
      "3: torch.Size([64, 338727])\n",
      "4: torch.Size([128, 62949])\n",
      "4.5: torch.Size([64, 62949]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 62949]), torch.Size([2, 0]), torch.Size([338727]), torch.Size([62949, 2]), torch.Size([62949]), torch.Size([224]), torch.Size([224]), torch.Size([62949, 64])\n",
      "1 x: torch.Size([62949, 64])\n",
      "1 token_sentiments: torch.Size([62949, 2])\n",
      "6: torch.Size([62949, 64])\n",
      "6 refined: torch.Size([62949, 64])\n",
      "7: torch.Size([62949, 64])\n",
      "7.1: torch.Size([62949, 64])\n",
      "7.2 refined: torch.Size([62949, 64])\n",
      "8: torch.Size([62949, 64])\n",
      "1 x: torch.Size([62949, 64])\n",
      "1 token_sentiments: torch.Size([62949, 2])\n",
      "8.1: torch.Size([62949, 64])\n",
      "8.2 refined: torch.Size([62949, 64])\n",
      "9: torch.Size([62949, 64])\n",
      "10: torch.Size([62949, 128])\n",
      "11: torch.Size([62949, 128])\n",
      "12: torch.Size([62949, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([338325])\n",
      "2.5: torch.Size([338325, 64])\n",
      "2.6: torch.Size([338325, 64])\n",
      "2.7: torch.Size([64, 338325])\n",
      "2.8: torch.Size([64, 338325])\n",
      "2.8 refined: torch.Size([64, 338325])\n",
      "2.9: torch.Size([64, 338325])\n",
      "3: torch.Size([64, 338325])\n",
      "4: torch.Size([128, 62664])\n",
      "4.5: torch.Size([64, 62664]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 62664]), torch.Size([2, 0]), torch.Size([338325]), torch.Size([62664, 2]), torch.Size([62664]), torch.Size([224]), torch.Size([224]), torch.Size([62664, 64])\n",
      "1 x: torch.Size([62664, 64])\n",
      "1 token_sentiments: torch.Size([62664, 2])\n",
      "6: torch.Size([62664, 64])\n",
      "6 refined: torch.Size([62664, 64])\n",
      "7: torch.Size([62664, 64])\n",
      "7.1: torch.Size([62664, 64])\n",
      "7.2 refined: torch.Size([62664, 64])\n",
      "8: torch.Size([62664, 64])\n",
      "1 x: torch.Size([62664, 64])\n",
      "1 token_sentiments: torch.Size([62664, 2])\n",
      "8.1: torch.Size([62664, 64])\n",
      "8.2 refined: torch.Size([62664, 64])\n",
      "9: torch.Size([62664, 64])\n",
      "10: torch.Size([62664, 128])\n",
      "11: torch.Size([62664, 128])\n",
      "12: torch.Size([62664, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([342329])\n",
      "2.5: torch.Size([342329, 64])\n",
      "2.6: torch.Size([342329, 64])\n",
      "2.7: torch.Size([64, 342329])\n",
      "2.8: torch.Size([64, 342329])\n",
      "2.8 refined: torch.Size([64, 342329])\n",
      "2.9: torch.Size([64, 342329])\n",
      "3: torch.Size([64, 342329])\n",
      "4: torch.Size([128, 63434])\n",
      "4.5: torch.Size([64, 63434]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 63434]), torch.Size([2, 0]), torch.Size([342329]), torch.Size([63434, 2]), torch.Size([63434]), torch.Size([224]), torch.Size([224]), torch.Size([63434, 64])\n",
      "1 x: torch.Size([63434, 64])\n",
      "1 token_sentiments: torch.Size([63434, 2])\n",
      "6: torch.Size([63434, 64])\n",
      "6 refined: torch.Size([63434, 64])\n",
      "7: torch.Size([63434, 64])\n",
      "7.1: torch.Size([63434, 64])\n",
      "7.2 refined: torch.Size([63434, 64])\n",
      "8: torch.Size([63434, 64])\n",
      "1 x: torch.Size([63434, 64])\n",
      "1 token_sentiments: torch.Size([63434, 2])\n",
      "8.1: torch.Size([63434, 64])\n",
      "8.2 refined: torch.Size([63434, 64])\n",
      "9: torch.Size([63434, 64])\n",
      "10: torch.Size([63434, 128])\n",
      "11: torch.Size([63434, 128])\n",
      "12: torch.Size([63434, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([341525])\n",
      "2.5: torch.Size([341525, 64])\n",
      "2.6: torch.Size([341525, 64])\n",
      "2.7: torch.Size([64, 341525])\n",
      "2.8: torch.Size([64, 341525])\n",
      "2.8 refined: torch.Size([64, 341525])\n",
      "2.9: torch.Size([64, 341525])\n",
      "3: torch.Size([64, 341525])\n",
      "4: torch.Size([128, 63488])\n",
      "4.5: torch.Size([64, 63488]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 63488]), torch.Size([2, 0]), torch.Size([341525]), torch.Size([63488, 2]), torch.Size([63488]), torch.Size([224]), torch.Size([224]), torch.Size([63488, 64])\n",
      "1 x: torch.Size([63488, 64])\n",
      "1 token_sentiments: torch.Size([63488, 2])\n",
      "6: torch.Size([63488, 64])\n",
      "6 refined: torch.Size([63488, 64])\n",
      "7: torch.Size([63488, 64])\n",
      "7.1: torch.Size([63488, 64])\n",
      "7.2 refined: torch.Size([63488, 64])\n",
      "8: torch.Size([63488, 64])\n",
      "1 x: torch.Size([63488, 64])\n",
      "1 token_sentiments: torch.Size([63488, 2])\n",
      "8.1: torch.Size([63488, 64])\n",
      "8.2 refined: torch.Size([63488, 64])\n",
      "9: torch.Size([63488, 64])\n",
      "10: torch.Size([63488, 128])\n",
      "11: torch.Size([63488, 128])\n",
      "12: torch.Size([63488, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([340245])\n",
      "2.5: torch.Size([340245, 64])\n",
      "2.6: torch.Size([340245, 64])\n",
      "2.7: torch.Size([64, 340245])\n",
      "2.8: torch.Size([64, 340245])\n",
      "2.8 refined: torch.Size([64, 340245])\n",
      "2.9: torch.Size([64, 340245])\n",
      "3: torch.Size([64, 340245])\n",
      "4: torch.Size([128, 63511])\n",
      "4.5: torch.Size([64, 63511]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 63511]), torch.Size([2, 0]), torch.Size([340245]), torch.Size([63511, 2]), torch.Size([63511]), torch.Size([224]), torch.Size([224]), torch.Size([63511, 64])\n",
      "1 x: torch.Size([63511, 64])\n",
      "1 token_sentiments: torch.Size([63511, 2])\n",
      "6: torch.Size([63511, 64])\n",
      "6 refined: torch.Size([63511, 64])\n",
      "7: torch.Size([63511, 64])\n",
      "7.1: torch.Size([63511, 64])\n",
      "7.2 refined: torch.Size([63511, 64])\n",
      "8: torch.Size([63511, 64])\n",
      "1 x: torch.Size([63511, 64])\n",
      "1 token_sentiments: torch.Size([63511, 2])\n",
      "8.1: torch.Size([63511, 64])\n",
      "8.2 refined: torch.Size([63511, 64])\n",
      "9: torch.Size([63511, 64])\n",
      "10: torch.Size([63511, 128])\n",
      "11: torch.Size([63511, 128])\n",
      "12: torch.Size([63511, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([338518])\n",
      "2.5: torch.Size([338518, 64])\n",
      "2.6: torch.Size([338518, 64])\n",
      "2.7: torch.Size([64, 338518])\n",
      "2.8: torch.Size([64, 338518])\n",
      "2.8 refined: torch.Size([64, 338518])\n",
      "2.9: torch.Size([64, 338518])\n",
      "3: torch.Size([64, 338518])\n",
      "4: torch.Size([128, 62673])\n",
      "4.5: torch.Size([64, 62673]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 62673]), torch.Size([2, 0]), torch.Size([338518]), torch.Size([62673, 2]), torch.Size([62673]), torch.Size([224]), torch.Size([224]), torch.Size([62673, 64])\n",
      "1 x: torch.Size([62673, 64])\n",
      "1 token_sentiments: torch.Size([62673, 2])\n",
      "6: torch.Size([62673, 64])\n",
      "6 refined: torch.Size([62673, 64])\n",
      "7: torch.Size([62673, 64])\n",
      "7.1: torch.Size([62673, 64])\n",
      "7.2 refined: torch.Size([62673, 64])\n",
      "8: torch.Size([62673, 64])\n",
      "1 x: torch.Size([62673, 64])\n",
      "1 token_sentiments: torch.Size([62673, 2])\n",
      "8.1: torch.Size([62673, 64])\n",
      "8.2 refined: torch.Size([62673, 64])\n",
      "9: torch.Size([62673, 64])\n",
      "10: torch.Size([62673, 128])\n",
      "11: torch.Size([62673, 128])\n",
      "12: torch.Size([62673, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([338490])\n",
      "2.5: torch.Size([338490, 64])\n",
      "2.6: torch.Size([338490, 64])\n",
      "2.7: torch.Size([64, 338490])\n",
      "2.8: torch.Size([64, 338490])\n",
      "2.8 refined: torch.Size([64, 338490])\n",
      "2.9: torch.Size([64, 338490])\n",
      "3: torch.Size([64, 338490])\n",
      "4: torch.Size([128, 63109])\n",
      "4.5: torch.Size([64, 63109]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 63109]), torch.Size([2, 0]), torch.Size([338490]), torch.Size([63109, 2]), torch.Size([63109]), torch.Size([224]), torch.Size([224]), torch.Size([63109, 64])\n",
      "1 x: torch.Size([63109, 64])\n",
      "1 token_sentiments: torch.Size([63109, 2])\n",
      "6: torch.Size([63109, 64])\n",
      "6 refined: torch.Size([63109, 64])\n",
      "7: torch.Size([63109, 64])\n",
      "7.1: torch.Size([63109, 64])\n",
      "7.2 refined: torch.Size([63109, 64])\n",
      "8: torch.Size([63109, 64])\n",
      "1 x: torch.Size([63109, 64])\n",
      "1 token_sentiments: torch.Size([63109, 2])\n",
      "8.1: torch.Size([63109, 64])\n",
      "8.2 refined: torch.Size([63109, 64])\n",
      "9: torch.Size([63109, 64])\n",
      "10: torch.Size([63109, 128])\n",
      "11: torch.Size([63109, 128])\n",
      "12: torch.Size([63109, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([337595])\n",
      "2.5: torch.Size([337595, 64])\n",
      "2.6: torch.Size([337595, 64])\n",
      "2.7: torch.Size([64, 337595])\n",
      "2.8: torch.Size([64, 337595])\n",
      "2.8 refined: torch.Size([64, 337595])\n",
      "2.9: torch.Size([64, 337595])\n",
      "3: torch.Size([64, 337595])\n",
      "4: torch.Size([128, 62690])\n",
      "4.5: torch.Size([64, 62690]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 62690]), torch.Size([2, 0]), torch.Size([337595]), torch.Size([62690, 2]), torch.Size([62690]), torch.Size([224]), torch.Size([224]), torch.Size([62690, 64])\n",
      "1 x: torch.Size([62690, 64])\n",
      "1 token_sentiments: torch.Size([62690, 2])\n",
      "6: torch.Size([62690, 64])\n",
      "6 refined: torch.Size([62690, 64])\n",
      "7: torch.Size([62690, 64])\n",
      "7.1: torch.Size([62690, 64])\n",
      "7.2 refined: torch.Size([62690, 64])\n",
      "8: torch.Size([62690, 64])\n",
      "1 x: torch.Size([62690, 64])\n",
      "1 token_sentiments: torch.Size([62690, 2])\n",
      "8.1: torch.Size([62690, 64])\n",
      "8.2 refined: torch.Size([62690, 64])\n",
      "9: torch.Size([62690, 64])\n",
      "10: torch.Size([62690, 128])\n",
      "11: torch.Size([62690, 128])\n",
      "12: torch.Size([62690, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "2: torch.Size([342213])\n",
      "2.5: torch.Size([342213, 64])\n",
      "2.6: torch.Size([342213, 64])\n",
      "2.7: torch.Size([64, 342213])\n",
      "2.8: torch.Size([64, 342213])\n",
      "2.8 refined: torch.Size([64, 342213])\n",
      "2.9: torch.Size([64, 342213])\n",
      "3: torch.Size([64, 342213])\n",
      "4: torch.Size([128, 63857])\n",
      "4.5: torch.Size([64, 63857]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 63857]), torch.Size([2, 0]), torch.Size([342213]), torch.Size([63857, 2]), torch.Size([63857]), torch.Size([224]), torch.Size([224]), torch.Size([63857, 64])\n",
      "1 x: torch.Size([63857, 64])\n",
      "1 token_sentiments: torch.Size([63857, 2])\n",
      "6: torch.Size([63857, 64])\n",
      "6 refined: torch.Size([63857, 64])\n",
      "7: torch.Size([63857, 64])\n",
      "7.1: torch.Size([63857, 64])\n",
      "7.2 refined: torch.Size([63857, 64])\n",
      "8: torch.Size([63857, 64])\n",
      "1 x: torch.Size([63857, 64])\n",
      "1 token_sentiments: torch.Size([63857, 2])\n",
      "8.1: torch.Size([63857, 64])\n",
      "8.2 refined: torch.Size([63857, 64])\n",
      "9: torch.Size([63857, 64])\n",
      "10: torch.Size([63857, 128])\n",
      "11: torch.Size([63857, 128])\n",
      "12: torch.Size([63857, 128])\n",
      "13: torch.Size([224, 512])\n",
      "14: torch.Size([224, 2])\n",
      "total_accuracy: 0.4947916666666667\n",
      "total_f1: 0.6039948252105451\n",
      "total_prec: 0.7453902751056454\n",
      "total_rec: 0.5076940243540008\n",
      "total_loss: 1.1011906862258911\n"
     ]
    }
   ],
   "source": [
    "calculate_average_metrics_mean(156, r'logs\\CNN-GNN_False_False_False', start=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_average_metrics_mean(r'logs\\CNN-GNN_False_False_False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import captum\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from captum.attr import LayerIntegratedGradients, TokenReferenceBase, visualization, IntegratedGradients\n",
    "from torch_geometric.nn.models.captum import to_captum_model, CaptumModel\n",
    "from torch_geometric.explain.algorithm import CaptumExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(num_embedding, base_path = 'logs\\CNN-GNN18_mr2k_seeds', version=0, n_steps=50, step_of_test=0):\n",
    "        version_path = join(base_path, f'version_{version}')\n",
    "        checkpoint_path = join(version_path, f'checkpoints')\n",
    "        onlyfiles  = [f for f in listdir(checkpoint_path) if (isfile(join(checkpoint_path, f)) and 'epoch' in f) ]\n",
    "        epoch_numbers = [int(re.search(r'\\d+', f).group()) for f in onlyfiles]\n",
    "        best_chpt_id, loss = get_best_chpt(join(version_path, 'metrics.csv'), epoch_numbers)\n",
    "        chpt_path = join(checkpoint_path, f'{onlyfiles[best_chpt_id]}')\n",
    "        classifier_torch_model = CNN_for_Text_No_Positional_Encoding(num_embedding=num_embedding, hidden_dim=hidden_dim, embedding_dim=embedding_dim, pos_emb_size=4096, dropout=0.2, num_out_features=len(class_id), seed=seed, random_edges=4, lattice_edges=8, lattice_step=2, virtual_nodes=0, lattice_start_distance=2, isXaiTests=True, num_tests=n_steps, step_of_test=step_of_test)\n",
    "        classfier_lightning_model = CnnGnnClassifierLightningModel.load_from_checkpoint(chpt_path, model=classifier_torch_model, num_classes=len(class_id)).eval()\n",
    "        return classfier_lightning_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CaptumModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captum_model = to_captum_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captum_exp = CaptumExplainer(IntegratedGradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captum_exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accumalate couple samples in this array for visualization purposes\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "def prepare_input_indices(graph, id_char_dict):\n",
    "    \n",
    "    characters = [id_char_dict[i.item()] for i in graph.x]\n",
    "    tokens = ['']*len(graph.token_indices)\n",
    "    for i, j in enumerate(graph.token_indices):\n",
    "        j = j.item()\n",
    "        tokens[j] += id_char_dict[graph.x[i].item()]\n",
    "    ref_graph = deepcopy(graph)\n",
    "    ref_graph.x = torch.ones_like(graph.x) * list(id_char_dict.keys())[-1]\n",
    "\n",
    "    return graph, ref_graph, tokens, characters\n",
    "\n",
    "token_reference = TokenReferenceBase(reference_token_idx=211)\n",
    "\n",
    "def interpret_sentence(model, graph, labels, id_char_dict, min_len = 7, label = 0, visualize_on_tokens=False, n_steps=50, dev='cpu', return_delta=True):\n",
    "    labels = labels.argmax().item()\n",
    "    graph.cumulative_token_indices = graph.token_indices\n",
    "    graph.edge_index = torch.zeros((2, 0))\n",
    "    # print(f'1: {graph.character_length.shape}')\n",
    "    graph, ref_graph, tokens, characters = prepare_input_indices(graph, id_char_dict)\n",
    "    #[tok.text for tok in tokenizer(sentence.lower())]\n",
    "    \n",
    "    graph = Batch.from_data_list([graph])\n",
    "    ref_graph = Batch.from_data_list([ref_graph])\n",
    "    \n",
    "    model.zero_grad()\n",
    "    # predict\n",
    "    ref_graph = ref_graph.to(dev)\n",
    "    graph = graph.to(dev)\n",
    "    model = model.to(dev)\n",
    "    \n",
    "    \n",
    "    model_output = model(graph.x, graph.edge_index, graph.cumulative_token_indices, graph.token_sentiments, graph.token_lengths, graph.num_tokens, graph.character_length, graph.token_embeddings)\n",
    "\n",
    "    pred = torch.softmax(model_output, dim=1).detach()\n",
    "    pred_ind = torch.argmax(pred).item()\n",
    "\n",
    "    seq_length = min_len\n",
    "    # generate reference indices for each sample\n",
    "    reference_indices = token_reference.generate_reference(seq_length, device=device).unsqueeze(0)\n",
    "\n",
    "    # compute attributions and approximation delta using layer integrated gradients\n",
    "    output = lig.attribute(graph.x, ref_graph.x, \\\n",
    "                                           additional_forward_args=(graph.edge_index, graph.cumulative_token_indices, graph.token_sentiments, graph.token_lengths, graph.num_tokens, graph.character_length, graph.token_embeddings), \\\n",
    "                                           n_steps=n_steps, return_convergence_delta=return_delta, target=labels)\n",
    "    \n",
    "    if(return_delta):\n",
    "        attributions_ig, delta = output\n",
    "    else:\n",
    "        print(f'outputoutput: {output}')\n",
    "        attributions_ig = output\n",
    "        print(f'attributions_ig: {attributions_ig}')\n",
    "        delta = 0\n",
    "        \n",
    "    captum_exp = CaptumExplainer(IntegratedGradients)\n",
    "\n",
    "    print(f'true: {id_class[labels]}({labels}), pred: {id_class[pred_ind]}({pred_ind}), max delta: {delta}')\n",
    "    \n",
    "    print(f'text: {tokens}, attributions_ig: {attributions_ig.shape}, labels: {labels}, delta: {delta}')\n",
    "\n",
    "    return attributions_ig, tokens, characters, pred, pred_ind, label, delta\n",
    "            \n",
    "def add_attributions_to_visualizer(attributions, text, pred, pred_ind, labels, delta, vis_data_records):\n",
    "    attributions = attributions.sum(dim=1)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    attributions = attributions.cpu().detach().numpy()\n",
    "    print(attributions.shape)\n",
    "    # storing couple samples in an array for visualization purposes\n",
    "    vis_data_records.append(visualization.VisualizationDataRecord(\n",
    "                            attributions,\n",
    "                            pred,\n",
    "                            id_class[pred_ind],\n",
    "                            id_class[labels],\n",
    "                            id_class[1],\n",
    "                            attributions.sum(),\n",
    "                            text,\n",
    "                            delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2Model(\n",
       "  (embeddings): DebertaV2Embeddings(\n",
       "    (word_embeddings): Embedding(128100, 1024, padding_idx=0)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (encoder): DebertaV2Encoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x DebertaV2Layer(\n",
       "        (attention): DebertaV2Attention(\n",
       "          (self): DisentangledSelfAttention(\n",
       "            (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (pos_dropout): StableDropout()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "          (output): DebertaV2SelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): DebertaV2Intermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): DebertaV2Output(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "          (dropout): StableDropout()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (rel_embeddings): Embedding(512, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data_records_ig = []\n",
    "n_steps=50\n",
    "labels = test_df.iloc[test_lengths.argmin()].Topic\n",
    "graph = test_dataset.content_to_graph(test_df.iloc[test_lengths.argmin()].Content)\n",
    "graph.cumulative_token_indices = test_dataset.caluculate_batch_token_positions(torch.tensor([graph.num_tokens]), torch.tensor([graph.character_length]), graph.token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_layers = [\n",
    "    lambda m: m.embedding, \n",
    "    lambda m: m.conv1, \n",
    "    lambda m: m.conv2, \n",
    "    lambda m: m.conv3, \n",
    "    lambda m: m.sentiment1,\n",
    "    lambda m: m.gcnn1,\n",
    "    lambda m: m.sentiment2,\n",
    "    lambda m: m.gcnn2\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>>> <<<<<<<<<<<<<<<<<<<<<<\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'loss_func' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_func'])`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: torch.Size([978])\n",
      "2.5: torch.Size([978, 64])\n",
      "2.6: torch.Size([978, 64])\n",
      "2.7: torch.Size([64, 978])\n",
      "2.8: torch.Size([64, 978])\n",
      "2.8 refined: torch.Size([64, 978])\n",
      "2.9: torch.Size([64, 978])\n",
      "3: torch.Size([64, 978])\n",
      "4: torch.Size([128, 180])\n",
      "4.5: torch.Size([64, 180]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 180]), torch.Size([2, 0]), torch.Size([978]), torch.Size([180, 2]), torch.Size([180]), torch.Size([1]), torch.Size([1]), torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "6: torch.Size([180, 64])\n",
      "6 refined: torch.Size([180, 64])\n",
      "7: torch.Size([180, 64])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "7.3 edge_weights: 2\n",
      "7.3 edge_weights: torch.Size([2, 3600])\n",
      "7.3 edge_weights: torch.Size([3600, 4])\n",
      "7.4 graph.edge_index.shape: torch.Size([2, 3600])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "8: torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "8.1: torch.Size([180, 64])\n",
      "8.2 refined: torch.Size([180, 64])\n",
      "9: torch.Size([180, 64])\n",
      "10: torch.Size([180, 128])\n",
      "11: torch.Size([180, 128])\n",
      "12: torch.Size([180, 128])\n",
      "13: torch.Size([1, 512])\n",
      "14: torch.Size([1, 2])\n",
      "2: torch.Size([978])\n",
      "2.5: torch.Size([978, 64])\n",
      "2.6: torch.Size([978, 64])\n",
      "2.7: torch.Size([64, 978])\n",
      "2.8: torch.Size([64, 978])\n",
      "2.8 refined: torch.Size([64, 978])\n",
      "2.9: torch.Size([64, 978])\n",
      "3: torch.Size([64, 978])\n",
      "4: torch.Size([128, 180])\n",
      "4.5: torch.Size([64, 180]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 180]), torch.Size([2, 0]), torch.Size([978]), torch.Size([180, 2]), torch.Size([180]), torch.Size([1]), torch.Size([1]), torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "6: torch.Size([180, 64])\n",
      "6 refined: torch.Size([180, 64])\n",
      "7: torch.Size([180, 64])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "7.3 edge_weights: 2\n",
      "7.3 edge_weights: torch.Size([2, 3600])\n",
      "7.3 edge_weights: torch.Size([3600, 4])\n",
      "7.4 graph.edge_index.shape: torch.Size([2, 3600])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "8: torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "8.1: torch.Size([180, 64])\n",
      "8.2 refined: torch.Size([180, 64])\n",
      "9: torch.Size([180, 64])\n",
      "10: torch.Size([180, 128])\n",
      "11: torch.Size([180, 128])\n",
      "12: torch.Size([180, 128])\n",
      "13: torch.Size([1, 512])\n",
      "14: torch.Size([1, 2])\n",
      "2: torch.Size([978])\n",
      "2.5: torch.Size([978, 64])\n",
      "2.6: torch.Size([978, 64])\n",
      "2.7: torch.Size([64, 978])\n",
      "2.8: torch.Size([64, 978])\n",
      "2.8 refined: torch.Size([64, 978])\n",
      "2.9: torch.Size([64, 978])\n",
      "3: torch.Size([64, 978])\n",
      "4: torch.Size([128, 180])\n",
      "4.5: torch.Size([64, 180]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 180]), torch.Size([2, 0]), torch.Size([978]), torch.Size([180, 2]), torch.Size([180]), torch.Size([1]), torch.Size([1]), torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "6: torch.Size([180, 64])\n",
      "6 refined: torch.Size([180, 64])\n",
      "7: torch.Size([180, 64])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "7.3 edge_weights: 2\n",
      "7.3 edge_weights: torch.Size([2, 3600])\n",
      "7.3 edge_weights: torch.Size([3600, 4])\n",
      "7.4 graph.edge_index.shape: torch.Size([2, 3600])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "8: torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "8.1: torch.Size([180, 64])\n",
      "8.2 refined: torch.Size([180, 64])\n",
      "9: torch.Size([180, 64])\n",
      "10: torch.Size([180, 128])\n",
      "11: torch.Size([180, 128])\n",
      "12: torch.Size([180, 128])\n",
      "13: torch.Size([1, 512])\n",
      "14: torch.Size([1, 2])\n",
      "2: torch.Size([48900])\n",
      "2.5: torch.Size([48900, 64])\n",
      "2.6: torch.Size([48900, 64])\n",
      "2.7: torch.Size([64, 48900])\n",
      "2.8: torch.Size([64, 48900])\n",
      "2.8 refined: torch.Size([64, 48900])\n",
      "2.9: torch.Size([64, 48900])\n",
      "3: torch.Size([64, 48900])\n",
      "4: torch.Size([128, 9000])\n",
      "4.5: torch.Size([64, 9000]), self.hidden_dim: 64, self.is_tests_token_level: 0\n",
      "abababdadasd\n",
      "5: torch.Size([64, 9000]), torch.Size([100, 0]), torch.Size([48900]), torch.Size([9000, 2]), torch.Size([9000]), torch.Size([50]), torch.Size([50]), torch.Size([9000, 64])\n",
      "1 x: torch.Size([9000, 64])\n",
      "1 token_sentiments: torch.Size([9000, 2])\n",
      "6: torch.Size([9000, 64])\n",
      "6 refined: torch.Size([9000, 64])\n",
      "7: torch.Size([9000, 64])\n",
      "7.1: torch.Size([9000, 64])\n",
      "7.2 refined: torch.Size([9000, 64])\n",
      "7.3 edge_weights: 2\n",
      "7.3 edge_weights: torch.Size([2, 180000])\n",
      "7.3 edge_weights: torch.Size([180000, 4])\n",
      "7.4 graph.edge_index.shape: torch.Size([2, 180000])\n",
      "7.1: torch.Size([9000, 64])\n",
      "7.2 refined: torch.Size([9000, 64])\n",
      "8: torch.Size([9000, 64])\n",
      "1 x: torch.Size([9000, 64])\n",
      "1 token_sentiments: torch.Size([9000, 2])\n",
      "8.1: torch.Size([9000, 64])\n",
      "8.2 refined: torch.Size([9000, 64])\n",
      "9: torch.Size([9000, 64])\n",
      "10: torch.Size([9000, 128])\n",
      "11: torch.Size([9000, 128])\n",
      "12: torch.Size([9000, 128])\n",
      "13: torch.Size([50, 512])\n",
      "14: torch.Size([50, 2])\n",
      "outputoutput: tensor([[-7.6998e-03,  6.5274e-03,  6.0644e-04,  ..., -2.0169e-03,\n",
      "          1.1881e-03, -2.0669e-03],\n",
      "        [ 1.5217e-03,  1.0354e-03, -1.1610e-02,  ...,  9.5808e-05,\n",
      "          8.9406e-03, -4.0873e-04],\n",
      "        [-2.7318e-04,  2.7117e-04, -5.4109e-04,  ...,  6.9438e-04,\n",
      "         -7.3593e-03,  2.4873e-03],\n",
      "        ...,\n",
      "        [-4.2242e-04, -2.8781e-04,  3.9684e-04,  ...,  1.9679e-04,\n",
      "          1.2950e-02, -1.0721e-03],\n",
      "        [-7.7033e-04,  2.3914e-04, -1.4516e-04,  ...,  2.1203e-03,\n",
      "          2.6666e-04, -1.4046e-03],\n",
      "        [ 0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00]], dtype=torch.float64)\n",
      "attributions_ig: tensor([[-7.6998e-03,  6.5274e-03,  6.0644e-04,  ..., -2.0169e-03,\n",
      "          1.1881e-03, -2.0669e-03],\n",
      "        [ 1.5217e-03,  1.0354e-03, -1.1610e-02,  ...,  9.5808e-05,\n",
      "          8.9406e-03, -4.0873e-04],\n",
      "        [-2.7318e-04,  2.7117e-04, -5.4109e-04,  ...,  6.9438e-04,\n",
      "         -7.3593e-03,  2.4873e-03],\n",
      "        ...,\n",
      "        [-4.2242e-04, -2.8781e-04,  3.9684e-04,  ...,  1.9679e-04,\n",
      "          1.2950e-02, -1.0721e-03],\n",
      "        [-7.7033e-04,  2.3914e-04, -1.4516e-04,  ...,  2.1203e-03,\n",
      "          2.6666e-04, -1.4046e-03],\n",
      "        [ 0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00]], dtype=torch.float64)\n",
      "true: Negative(0), pred: Negative(0), max delta: 0\n",
      "text: ['▁Written ', '▁by ', '▁someone ', '▁who ', '▁has ', '▁been ', '▁there ', ', ', '▁you ', '▁can ', '▁tell ', ', ', '▁but ', '▁only ', '▁if ', '▁you ', \"' \", 've ', '▁been ', '▁there ', '. ', '▁Excellent ', '▁performances ', '▁by ', '▁Meryl ', '▁Streep ', '▁( ', 'of ', '▁course ', '! ', ') ', ', ', '▁Renee ', '▁Zell ', 'we ', 'ger ', '▁and ', '▁William ', '▁Hurt ', '. ', '< ', 'br ', '▁/ ', '> ', '< ', 'br ', '▁/ ', '> ', 'Many ', '▁people ', '▁have ', '▁said ', '▁that ', '▁it ', '▁is ', '▁about ', '▁a ', '▁dysfunctional ', '▁family ', ', ', '▁I ', '▁think ', '▁every ', '▁family ', '▁is ', '▁dysfunctional ', '▁when ', '▁they ', '▁are ', '▁facing ', '▁this ', '▁kind ', '▁of ', '▁torment ', '. ', '▁To ', '▁NOT ', '▁be ', '▁dysfunctional ', '▁would ', '▁be ', '▁dysfunctional ', '! ', '▁You ', '▁are ', '▁losing ', '▁your ', '▁family ', '▁as ', '▁you ', '▁know ', '▁it ', ', ', '▁can ', '▁anything ', '▁be ', '▁worse ', '? ', '▁People ', '▁need ', '▁to ', '▁see ', '▁this ', '▁movie ', '▁so ', '▁when ', '▁they ', '▁are ', '▁faced ', '▁with ', '▁this ', '▁nightmare ', '▁maybe ', '▁they ', '▁will ', '▁change ', '▁how ', '▁they ', '▁do ', '▁it ', '. ', '▁Maybe ', '▁they ', '▁will ', '▁see ', '▁that ', '▁the ', '▁father ', '▁is ', '▁denying ', '▁himself ', '▁valuable ', '▁time ', '▁he ', \"' \", 'll ', '▁never ', '▁get ', '▁a ', '▁chance ', '▁at ', '▁again ', '. ', '▁Maybe ', '▁they ', '▁will ', '▁realize ', '▁how ', '▁hard ', '▁it ', '▁is ', '▁to ', '▁die ', ', ', '▁or ', '▁to ', '▁watch ', '▁someone ', '▁you ', '▁love ', '▁die ', '. ', '▁They ', '▁didn ', \"' \", 't ', '▁miss ', '▁much ', '▁of ', '▁the ', '▁nightmare ', ', ', '▁it ', \"' \", 's ', '▁hard ', '▁to ', '▁forget ', '. ', '\\x01', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], attributions_ig: torch.Size([978, 64]), labels: 0, delta: 0\n",
      "(64,)\n",
      ">>>>>>>>>>>>>>>>>>>> <<<<<<<<<<<<<<<<<<<<<<\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'loss_func' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_func'])`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: torch.Size([978])\n",
      "2.5: torch.Size([978, 64])\n",
      "2.6: torch.Size([978, 64])\n",
      "2.7: torch.Size([64, 978])\n",
      "2.8: torch.Size([64, 978])\n",
      "2.8 refined: torch.Size([64, 978])\n",
      "2.9: torch.Size([64, 978])\n",
      "3: torch.Size([64, 978])\n",
      "4: torch.Size([128, 180])\n",
      "4.5: torch.Size([64, 180]), self.hidden_dim: 64, self.is_tests_token_level: 1\n",
      "abababdadasd\n",
      "5: torch.Size([64, 180]), torch.Size([2, 0]), torch.Size([978]), torch.Size([180, 2]), torch.Size([180]), torch.Size([1]), torch.Size([1]), torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "6: torch.Size([180, 64])\n",
      "6 refined: torch.Size([180, 64])\n",
      "7: torch.Size([180, 64])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "7.3 edge_weights: 2\n",
      "7.3 edge_weights: torch.Size([2, 3600])\n",
      "7.3 edge_weights: torch.Size([3600, 4])\n",
      "7.4 graph.edge_index.shape: torch.Size([2, 3600])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "8: torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "8.1: torch.Size([180, 64])\n",
      "8.2 refined: torch.Size([180, 64])\n",
      "9: torch.Size([180, 64])\n",
      "10: torch.Size([180, 128])\n",
      "11: torch.Size([180, 128])\n",
      "12: torch.Size([180, 128])\n",
      "13: torch.Size([1, 512])\n",
      "14: torch.Size([1, 2])\n",
      "2: torch.Size([978])\n",
      "2.5: torch.Size([978, 64])\n",
      "2.6: torch.Size([978, 64])\n",
      "2.7: torch.Size([64, 978])\n",
      "2.8: torch.Size([64, 978])\n",
      "2.8 refined: torch.Size([64, 978])\n",
      "2.9: torch.Size([64, 978])\n",
      "3: torch.Size([64, 978])\n",
      "4: torch.Size([128, 180])\n",
      "4.5: torch.Size([64, 180]), self.hidden_dim: 64, self.is_tests_token_level: 1\n",
      "abababdadasd\n",
      "5: torch.Size([64, 180]), torch.Size([2, 0]), torch.Size([978]), torch.Size([180, 2]), torch.Size([180]), torch.Size([1]), torch.Size([1]), torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "6: torch.Size([180, 64])\n",
      "6 refined: torch.Size([180, 64])\n",
      "7: torch.Size([180, 64])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "7.3 edge_weights: 2\n",
      "7.3 edge_weights: torch.Size([2, 3600])\n",
      "7.3 edge_weights: torch.Size([3600, 4])\n",
      "7.4 graph.edge_index.shape: torch.Size([2, 3600])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "8: torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "8.1: torch.Size([180, 64])\n",
      "8.2 refined: torch.Size([180, 64])\n",
      "9: torch.Size([180, 64])\n",
      "10: torch.Size([180, 128])\n",
      "11: torch.Size([180, 128])\n",
      "12: torch.Size([180, 128])\n",
      "13: torch.Size([1, 512])\n",
      "14: torch.Size([1, 2])\n",
      "2: torch.Size([978])\n",
      "2.5: torch.Size([978, 64])\n",
      "2.6: torch.Size([978, 64])\n",
      "2.7: torch.Size([64, 978])\n",
      "2.8: torch.Size([64, 978])\n",
      "2.8 refined: torch.Size([64, 978])\n",
      "2.9: torch.Size([64, 978])\n",
      "3: torch.Size([64, 978])\n",
      "4: torch.Size([128, 180])\n",
      "4.5: torch.Size([64, 180]), self.hidden_dim: 64, self.is_tests_token_level: 1\n",
      "abababdadasd\n",
      "5: torch.Size([64, 180]), torch.Size([2, 0]), torch.Size([978]), torch.Size([180, 2]), torch.Size([180]), torch.Size([1]), torch.Size([1]), torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "6: torch.Size([180, 64])\n",
      "6 refined: torch.Size([180, 64])\n",
      "7: torch.Size([180, 64])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "7.3 edge_weights: 2\n",
      "7.3 edge_weights: torch.Size([2, 3600])\n",
      "7.3 edge_weights: torch.Size([3600, 4])\n",
      "7.4 graph.edge_index.shape: torch.Size([2, 3600])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "8: torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "8.1: torch.Size([180, 64])\n",
      "8.2 refined: torch.Size([180, 64])\n",
      "9: torch.Size([180, 64])\n",
      "10: torch.Size([180, 128])\n",
      "11: torch.Size([180, 128])\n",
      "12: torch.Size([180, 128])\n",
      "13: torch.Size([1, 512])\n",
      "14: torch.Size([1, 2])\n",
      "2: torch.Size([48900])\n",
      "2.5: torch.Size([48900, 64])\n",
      "2.6: torch.Size([48900, 64])\n",
      "2.7: torch.Size([64, 48900])\n",
      "2.8: torch.Size([3200, 978])\n",
      "2.8 refined: torch.Size([64, 48900])\n",
      "2.9: torch.Size([64, 48900])\n",
      "3: torch.Size([64, 48900])\n",
      "4: torch.Size([128, 9000])\n",
      "4.5: torch.Size([64, 9000]), self.hidden_dim: 64, self.is_tests_token_level: 1\n",
      "abababdadasd\n",
      "5: torch.Size([64, 9000]), torch.Size([100, 0]), torch.Size([48900]), torch.Size([9000, 2]), torch.Size([9000]), torch.Size([50]), torch.Size([50]), torch.Size([9000, 64])\n",
      "1 x: torch.Size([9000, 64])\n",
      "1 token_sentiments: torch.Size([9000, 2])\n",
      "6: torch.Size([9000, 64])\n",
      "6 refined: torch.Size([9000, 64])\n",
      "7: torch.Size([9000, 64])\n",
      "7.1: torch.Size([9000, 64])\n",
      "7.2 refined: torch.Size([9000, 64])\n",
      "7.3 edge_weights: 2\n",
      "7.3 edge_weights: torch.Size([2, 180000])\n",
      "7.3 edge_weights: torch.Size([180000, 4])\n",
      "7.4 graph.edge_index.shape: torch.Size([2, 180000])\n",
      "7.1: torch.Size([9000, 64])\n",
      "7.2 refined: torch.Size([9000, 64])\n",
      "8: torch.Size([9000, 64])\n",
      "1 x: torch.Size([9000, 64])\n",
      "1 token_sentiments: torch.Size([9000, 2])\n",
      "8.1: torch.Size([9000, 64])\n",
      "8.2 refined: torch.Size([9000, 64])\n",
      "9: torch.Size([9000, 64])\n",
      "10: torch.Size([9000, 128])\n",
      "11: torch.Size([9000, 128])\n",
      "12: torch.Size([9000, 128])\n",
      "13: torch.Size([50, 512])\n",
      "14: torch.Size([50, 2])\n",
      "outputoutput: tensor([[ 9.8846e-04, -6.9917e-04,  6.4238e-04,  ..., -1.6508e-03,\n",
      "          2.4722e-04, -9.7256e-04],\n",
      "        [ 1.3793e-03,  1.0346e-03,  2.2523e-03,  ...,  3.5462e-04,\n",
      "          1.0369e-03, -8.0702e-04],\n",
      "        [ 7.1780e-04, -2.9625e-04, -9.9818e-04,  ...,  3.2828e-04,\n",
      "          1.0603e-03, -0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  4.1959e-03,  ...,  2.9868e-04,\n",
      "          0.0000e+00,  2.1876e-04],\n",
      "        [ 1.0548e-03,  3.0288e-03,  5.7045e-04,  ..., -7.1493e-04,\n",
      "          1.8799e-03,  1.8206e-03],\n",
      "        [ 5.3990e-06,  1.1364e-03,  3.5701e-04,  ..., -1.2036e-03,\n",
      "          2.8934e-03, -5.8476e-03]], dtype=torch.float64)\n",
      "attributions_ig: tensor([[ 9.8846e-04, -6.9917e-04,  6.4238e-04,  ..., -1.6508e-03,\n",
      "          2.4722e-04, -9.7256e-04],\n",
      "        [ 1.3793e-03,  1.0346e-03,  2.2523e-03,  ...,  3.5462e-04,\n",
      "          1.0369e-03, -8.0702e-04],\n",
      "        [ 7.1780e-04, -2.9625e-04, -9.9818e-04,  ...,  3.2828e-04,\n",
      "          1.0603e-03, -0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  4.1959e-03,  ...,  2.9868e-04,\n",
      "          0.0000e+00,  2.1876e-04],\n",
      "        [ 1.0548e-03,  3.0288e-03,  5.7045e-04,  ..., -7.1493e-04,\n",
      "          1.8799e-03,  1.8206e-03],\n",
      "        [ 5.3990e-06,  1.1364e-03,  3.5701e-04,  ..., -1.2036e-03,\n",
      "          2.8934e-03, -5.8476e-03]], dtype=torch.float64)\n",
      "true: Negative(0), pred: Negative(0), max delta: 0\n",
      "text: ['▁Written ', '▁by ', '▁someone ', '▁who ', '▁has ', '▁been ', '▁there ', ', ', '▁you ', '▁can ', '▁tell ', ', ', '▁but ', '▁only ', '▁if ', '▁you ', \"' \", 've ', '▁been ', '▁there ', '. ', '▁Excellent ', '▁performances ', '▁by ', '▁Meryl ', '▁Streep ', '▁( ', 'of ', '▁course ', '! ', ') ', ', ', '▁Renee ', '▁Zell ', 'we ', 'ger ', '▁and ', '▁William ', '▁Hurt ', '. ', '< ', 'br ', '▁/ ', '> ', '< ', 'br ', '▁/ ', '> ', 'Many ', '▁people ', '▁have ', '▁said ', '▁that ', '▁it ', '▁is ', '▁about ', '▁a ', '▁dysfunctional ', '▁family ', ', ', '▁I ', '▁think ', '▁every ', '▁family ', '▁is ', '▁dysfunctional ', '▁when ', '▁they ', '▁are ', '▁facing ', '▁this ', '▁kind ', '▁of ', '▁torment ', '. ', '▁To ', '▁NOT ', '▁be ', '▁dysfunctional ', '▁would ', '▁be ', '▁dysfunctional ', '! ', '▁You ', '▁are ', '▁losing ', '▁your ', '▁family ', '▁as ', '▁you ', '▁know ', '▁it ', ', ', '▁can ', '▁anything ', '▁be ', '▁worse ', '? ', '▁People ', '▁need ', '▁to ', '▁see ', '▁this ', '▁movie ', '▁so ', '▁when ', '▁they ', '▁are ', '▁faced ', '▁with ', '▁this ', '▁nightmare ', '▁maybe ', '▁they ', '▁will ', '▁change ', '▁how ', '▁they ', '▁do ', '▁it ', '. ', '▁Maybe ', '▁they ', '▁will ', '▁see ', '▁that ', '▁the ', '▁father ', '▁is ', '▁denying ', '▁himself ', '▁valuable ', '▁time ', '▁he ', \"' \", 'll ', '▁never ', '▁get ', '▁a ', '▁chance ', '▁at ', '▁again ', '. ', '▁Maybe ', '▁they ', '▁will ', '▁realize ', '▁how ', '▁hard ', '▁it ', '▁is ', '▁to ', '▁die ', ', ', '▁or ', '▁to ', '▁watch ', '▁someone ', '▁you ', '▁love ', '▁die ', '. ', '▁They ', '▁didn ', \"' \", 't ', '▁miss ', '▁much ', '▁of ', '▁the ', '▁nightmare ', ', ', '▁it ', \"' \", 's ', '▁hard ', '▁to ', '▁forget ', '. ', '\\x01', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], attributions_ig: torch.Size([64, 978]), labels: 0, delta: 0\n",
      "(978,)\n",
      ">>>>>>>>>>>>>>>>>>>> <<<<<<<<<<<<<<<<<<<<<<\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'loss_func' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_func'])`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: torch.Size([978])\n",
      "2.5: torch.Size([978, 64])\n",
      "2.6: torch.Size([978, 64])\n",
      "2.7: torch.Size([64, 978])\n",
      "2.8: torch.Size([64, 978])\n",
      "2.8 refined: torch.Size([64, 978])\n",
      "2.9: torch.Size([64, 978])\n",
      "3: torch.Size([64, 978])\n",
      "4: torch.Size([128, 180])\n",
      "4.5: torch.Size([64, 180]), self.hidden_dim: 64, self.is_tests_token_level: 2\n",
      "abababdadasd\n",
      "5: torch.Size([64, 180]), torch.Size([2, 0]), torch.Size([978]), torch.Size([180, 2]), torch.Size([180]), torch.Size([1]), torch.Size([1]), torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "6: torch.Size([180, 64])\n",
      "6 refined: torch.Size([180, 64])\n",
      "7: torch.Size([180, 64])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "7.3 edge_weights: 2\n",
      "7.3 edge_weights: torch.Size([2, 3600])\n",
      "7.3 edge_weights: torch.Size([3600, 4])\n",
      "7.4 graph.edge_index.shape: torch.Size([2, 3600])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "8: torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "8.1: torch.Size([180, 64])\n",
      "8.2 refined: torch.Size([180, 64])\n",
      "9: torch.Size([180, 64])\n",
      "10: torch.Size([180, 128])\n",
      "11: torch.Size([180, 128])\n",
      "12: torch.Size([180, 128])\n",
      "13: torch.Size([1, 512])\n",
      "14: torch.Size([1, 2])\n",
      "2: torch.Size([978])\n",
      "2.5: torch.Size([978, 64])\n",
      "2.6: torch.Size([978, 64])\n",
      "2.7: torch.Size([64, 978])\n",
      "2.8: torch.Size([64, 978])\n",
      "2.8 refined: torch.Size([64, 978])\n",
      "2.9: torch.Size([64, 978])\n",
      "3: torch.Size([64, 978])\n",
      "4: torch.Size([128, 180])\n",
      "4.5: torch.Size([64, 180]), self.hidden_dim: 64, self.is_tests_token_level: 2\n",
      "abababdadasd\n",
      "5: torch.Size([64, 180]), torch.Size([2, 0]), torch.Size([978]), torch.Size([180, 2]), torch.Size([180]), torch.Size([1]), torch.Size([1]), torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "6: torch.Size([180, 64])\n",
      "6 refined: torch.Size([180, 64])\n",
      "7: torch.Size([180, 64])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "7.3 edge_weights: 2\n",
      "7.3 edge_weights: torch.Size([2, 3600])\n",
      "7.3 edge_weights: torch.Size([3600, 4])\n",
      "7.4 graph.edge_index.shape: torch.Size([2, 3600])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "8: torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "8.1: torch.Size([180, 64])\n",
      "8.2 refined: torch.Size([180, 64])\n",
      "9: torch.Size([180, 64])\n",
      "10: torch.Size([180, 128])\n",
      "11: torch.Size([180, 128])\n",
      "12: torch.Size([180, 128])\n",
      "13: torch.Size([1, 512])\n",
      "14: torch.Size([1, 2])\n",
      "2: torch.Size([978])\n",
      "2.5: torch.Size([978, 64])\n",
      "2.6: torch.Size([978, 64])\n",
      "2.7: torch.Size([64, 978])\n",
      "2.8: torch.Size([64, 978])\n",
      "2.8 refined: torch.Size([64, 978])\n",
      "2.9: torch.Size([64, 978])\n",
      "3: torch.Size([64, 978])\n",
      "4: torch.Size([128, 180])\n",
      "4.5: torch.Size([64, 180]), self.hidden_dim: 64, self.is_tests_token_level: 2\n",
      "abababdadasd\n",
      "5: torch.Size([64, 180]), torch.Size([2, 0]), torch.Size([978]), torch.Size([180, 2]), torch.Size([180]), torch.Size([1]), torch.Size([1]), torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "6: torch.Size([180, 64])\n",
      "6 refined: torch.Size([180, 64])\n",
      "7: torch.Size([180, 64])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "7.3 edge_weights: 2\n",
      "7.3 edge_weights: torch.Size([2, 3600])\n",
      "7.3 edge_weights: torch.Size([3600, 4])\n",
      "7.4 graph.edge_index.shape: torch.Size([2, 3600])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "8: torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "8.1: torch.Size([180, 64])\n",
      "8.2 refined: torch.Size([180, 64])\n",
      "9: torch.Size([180, 64])\n",
      "10: torch.Size([180, 128])\n",
      "11: torch.Size([180, 128])\n",
      "12: torch.Size([180, 128])\n",
      "13: torch.Size([1, 512])\n",
      "14: torch.Size([1, 2])\n",
      "2: torch.Size([48900])\n",
      "2.5: torch.Size([48900, 64])\n",
      "2.6: torch.Size([48900, 64])\n",
      "2.7: torch.Size([64, 48900])\n",
      "2.8: torch.Size([64, 48900])\n",
      "2.8 refined: torch.Size([64, 48900])\n",
      "2.9: torch.Size([64, 48900])\n",
      "3: torch.Size([64, 48900])\n",
      "4: torch.Size([128, 9000])\n",
      "4.5: torch.Size([64, 9000]), self.hidden_dim: 64, self.is_tests_token_level: 2\n",
      "abababdadasd\n",
      "5: torch.Size([64, 9000]), torch.Size([100, 0]), torch.Size([48900]), torch.Size([9000, 2]), torch.Size([9000]), torch.Size([50]), torch.Size([50]), torch.Size([9000, 64])\n",
      "1 x: torch.Size([9000, 64])\n",
      "1 token_sentiments: torch.Size([9000, 2])\n",
      "6: torch.Size([9000, 64])\n",
      "6 refined: torch.Size([9000, 64])\n",
      "7: torch.Size([9000, 64])\n",
      "7.1: torch.Size([9000, 64])\n",
      "7.2 refined: torch.Size([9000, 64])\n",
      "7.3 edge_weights: 2\n",
      "7.3 edge_weights: torch.Size([2, 180000])\n",
      "7.3 edge_weights: torch.Size([180000, 4])\n",
      "7.4 graph.edge_index.shape: torch.Size([2, 180000])\n",
      "7.1: torch.Size([9000, 64])\n",
      "7.2 refined: torch.Size([9000, 64])\n",
      "8: torch.Size([9000, 64])\n",
      "1 x: torch.Size([9000, 64])\n",
      "1 token_sentiments: torch.Size([9000, 2])\n",
      "8.1: torch.Size([9000, 64])\n",
      "8.2 refined: torch.Size([9000, 64])\n",
      "9: torch.Size([9000, 64])\n",
      "10: torch.Size([9000, 128])\n",
      "11: torch.Size([9000, 128])\n",
      "12: torch.Size([9000, 128])\n",
      "13: torch.Size([50, 512])\n",
      "14: torch.Size([50, 2])\n",
      "outputoutput: tensor([[-0.0041, -0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000, -0.0003,  ..., -0.0002, -0.0002,  0.0030],\n",
      "        [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0007, -0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0033,  0.0018, -0.0015],\n",
      "        [ 0.0006,  0.0006,  0.0009,  ...,  0.0017, -0.0044,  0.0004]],\n",
      "       dtype=torch.float64)\n",
      "attributions_ig: tensor([[-0.0041, -0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000, -0.0003,  ..., -0.0002, -0.0002,  0.0030],\n",
      "        [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0007, -0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0033,  0.0018, -0.0015],\n",
      "        [ 0.0006,  0.0006,  0.0009,  ...,  0.0017, -0.0044,  0.0004]],\n",
      "       dtype=torch.float64)\n",
      "true: Negative(0), pred: Negative(0), max delta: 0\n",
      "text: ['▁Written ', '▁by ', '▁someone ', '▁who ', '▁has ', '▁been ', '▁there ', ', ', '▁you ', '▁can ', '▁tell ', ', ', '▁but ', '▁only ', '▁if ', '▁you ', \"' \", 've ', '▁been ', '▁there ', '. ', '▁Excellent ', '▁performances ', '▁by ', '▁Meryl ', '▁Streep ', '▁( ', 'of ', '▁course ', '! ', ') ', ', ', '▁Renee ', '▁Zell ', 'we ', 'ger ', '▁and ', '▁William ', '▁Hurt ', '. ', '< ', 'br ', '▁/ ', '> ', '< ', 'br ', '▁/ ', '> ', 'Many ', '▁people ', '▁have ', '▁said ', '▁that ', '▁it ', '▁is ', '▁about ', '▁a ', '▁dysfunctional ', '▁family ', ', ', '▁I ', '▁think ', '▁every ', '▁family ', '▁is ', '▁dysfunctional ', '▁when ', '▁they ', '▁are ', '▁facing ', '▁this ', '▁kind ', '▁of ', '▁torment ', '. ', '▁To ', '▁NOT ', '▁be ', '▁dysfunctional ', '▁would ', '▁be ', '▁dysfunctional ', '! ', '▁You ', '▁are ', '▁losing ', '▁your ', '▁family ', '▁as ', '▁you ', '▁know ', '▁it ', ', ', '▁can ', '▁anything ', '▁be ', '▁worse ', '? ', '▁People ', '▁need ', '▁to ', '▁see ', '▁this ', '▁movie ', '▁so ', '▁when ', '▁they ', '▁are ', '▁faced ', '▁with ', '▁this ', '▁nightmare ', '▁maybe ', '▁they ', '▁will ', '▁change ', '▁how ', '▁they ', '▁do ', '▁it ', '. ', '▁Maybe ', '▁they ', '▁will ', '▁see ', '▁that ', '▁the ', '▁father ', '▁is ', '▁denying ', '▁himself ', '▁valuable ', '▁time ', '▁he ', \"' \", 'll ', '▁never ', '▁get ', '▁a ', '▁chance ', '▁at ', '▁again ', '. ', '▁Maybe ', '▁they ', '▁will ', '▁realize ', '▁how ', '▁hard ', '▁it ', '▁is ', '▁to ', '▁die ', ', ', '▁or ', '▁to ', '▁watch ', '▁someone ', '▁you ', '▁love ', '▁die ', '. ', '▁They ', '▁didn ', \"' \", 't ', '▁miss ', '▁much ', '▁of ', '▁the ', '▁nightmare ', ', ', '▁it ', \"' \", 's ', '▁hard ', '▁to ', '▁forget ', '. ', '\\x01', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], attributions_ig: torch.Size([64, 978]), labels: 0, delta: 0\n",
      "(978,)\n",
      ">>>>>>>>>>>>>>>>>>>> <<<<<<<<<<<<<<<<<<<<<<\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'loss_func' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_func'])`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: torch.Size([978])\n",
      "2.5: torch.Size([978, 64])\n",
      "2.6: torch.Size([978, 64])\n",
      "2.7: torch.Size([64, 978])\n",
      "2.8: torch.Size([64, 978])\n",
      "2.8 refined: torch.Size([64, 978])\n",
      "2.9: torch.Size([64, 978])\n",
      "3: torch.Size([64, 978])\n",
      "4: torch.Size([128, 180])\n",
      "4.5: torch.Size([64, 180]), self.hidden_dim: 64, self.is_tests_token_level: 3\n",
      "abababdadasd\n",
      "5: torch.Size([64, 180]), torch.Size([2, 0]), torch.Size([978]), torch.Size([180, 2]), torch.Size([180]), torch.Size([1]), torch.Size([1]), torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "6: torch.Size([180, 64])\n",
      "6 refined: torch.Size([180, 64])\n",
      "7: torch.Size([180, 64])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "7.3 edge_weights: 2\n",
      "7.3 edge_weights: torch.Size([2, 3600])\n",
      "7.3 edge_weights: torch.Size([3600, 4])\n",
      "7.4 graph.edge_index.shape: torch.Size([2, 3600])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "8: torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "8.1: torch.Size([180, 64])\n",
      "8.2 refined: torch.Size([180, 64])\n",
      "9: torch.Size([180, 64])\n",
      "10: torch.Size([180, 128])\n",
      "11: torch.Size([180, 128])\n",
      "12: torch.Size([180, 128])\n",
      "13: torch.Size([1, 512])\n",
      "14: torch.Size([1, 2])\n",
      "2: torch.Size([978])\n",
      "2.5: torch.Size([978, 64])\n",
      "2.6: torch.Size([978, 64])\n",
      "2.7: torch.Size([64, 978])\n",
      "2.8: torch.Size([64, 978])\n",
      "2.8 refined: torch.Size([64, 978])\n",
      "2.9: torch.Size([64, 978])\n",
      "3: torch.Size([64, 978])\n",
      "4: torch.Size([128, 180])\n",
      "4.5: torch.Size([64, 180]), self.hidden_dim: 64, self.is_tests_token_level: 3\n",
      "abababdadasd\n",
      "5: torch.Size([64, 180]), torch.Size([2, 0]), torch.Size([978]), torch.Size([180, 2]), torch.Size([180]), torch.Size([1]), torch.Size([1]), torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "6: torch.Size([180, 64])\n",
      "6 refined: torch.Size([180, 64])\n",
      "7: torch.Size([180, 64])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "7.3 edge_weights: 2\n",
      "7.3 edge_weights: torch.Size([2, 3600])\n",
      "7.3 edge_weights: torch.Size([3600, 4])\n",
      "7.4 graph.edge_index.shape: torch.Size([2, 3600])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "8: torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "8.1: torch.Size([180, 64])\n",
      "8.2 refined: torch.Size([180, 64])\n",
      "9: torch.Size([180, 64])\n",
      "10: torch.Size([180, 128])\n",
      "11: torch.Size([180, 128])\n",
      "12: torch.Size([180, 128])\n",
      "13: torch.Size([1, 512])\n",
      "14: torch.Size([1, 2])\n",
      "2: torch.Size([978])\n",
      "2.5: torch.Size([978, 64])\n",
      "2.6: torch.Size([978, 64])\n",
      "2.7: torch.Size([64, 978])\n",
      "2.8: torch.Size([64, 978])\n",
      "2.8 refined: torch.Size([64, 978])\n",
      "2.9: torch.Size([64, 978])\n",
      "3: torch.Size([64, 978])\n",
      "4: torch.Size([128, 180])\n",
      "4.5: torch.Size([64, 180]), self.hidden_dim: 64, self.is_tests_token_level: 3\n",
      "abababdadasd\n",
      "5: torch.Size([64, 180]), torch.Size([2, 0]), torch.Size([978]), torch.Size([180, 2]), torch.Size([180]), torch.Size([1]), torch.Size([1]), torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "6: torch.Size([180, 64])\n",
      "6 refined: torch.Size([180, 64])\n",
      "7: torch.Size([180, 64])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "7.3 edge_weights: 2\n",
      "7.3 edge_weights: torch.Size([2, 3600])\n",
      "7.3 edge_weights: torch.Size([3600, 4])\n",
      "7.4 graph.edge_index.shape: torch.Size([2, 3600])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "8: torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "8.1: torch.Size([180, 64])\n",
      "8.2 refined: torch.Size([180, 64])\n",
      "9: torch.Size([180, 64])\n",
      "10: torch.Size([180, 128])\n",
      "11: torch.Size([180, 128])\n",
      "12: torch.Size([180, 128])\n",
      "13: torch.Size([1, 512])\n",
      "14: torch.Size([1, 2])\n",
      "2: torch.Size([48900])\n",
      "2.5: torch.Size([48900, 64])\n",
      "2.6: torch.Size([48900, 64])\n",
      "2.7: torch.Size([64, 48900])\n",
      "2.8: torch.Size([64, 48900])\n",
      "2.8 refined: torch.Size([64, 48900])\n",
      "2.9: torch.Size([64, 48900])\n",
      "3: torch.Size([64, 48900])\n",
      "4: torch.Size([128, 9000])\n",
      "4.5: torch.Size([3200, 180]), self.hidden_dim: 64, self.is_tests_token_level: 3\n",
      "abababdadasd\n",
      "5: torch.Size([64, 9000]), torch.Size([100, 0]), torch.Size([48900]), torch.Size([9000, 2]), torch.Size([9000]), torch.Size([50]), torch.Size([50]), torch.Size([9000, 64])\n",
      "1 x: torch.Size([9000, 64])\n",
      "1 token_sentiments: torch.Size([9000, 2])\n",
      "6: torch.Size([9000, 64])\n",
      "6 refined: torch.Size([9000, 64])\n",
      "7: torch.Size([9000, 64])\n",
      "7.1: torch.Size([9000, 64])\n",
      "7.2 refined: torch.Size([9000, 64])\n",
      "7.3 edge_weights: 2\n",
      "7.3 edge_weights: torch.Size([2, 180000])\n",
      "7.3 edge_weights: torch.Size([180000, 4])\n",
      "7.4 graph.edge_index.shape: torch.Size([2, 180000])\n",
      "7.1: torch.Size([9000, 64])\n",
      "7.2 refined: torch.Size([9000, 64])\n",
      "8: torch.Size([9000, 64])\n",
      "1 x: torch.Size([9000, 64])\n",
      "1 token_sentiments: torch.Size([9000, 2])\n",
      "8.1: torch.Size([9000, 64])\n",
      "8.2 refined: torch.Size([9000, 64])\n",
      "9: torch.Size([9000, 64])\n",
      "10: torch.Size([9000, 128])\n",
      "11: torch.Size([9000, 128])\n",
      "12: torch.Size([9000, 128])\n",
      "13: torch.Size([50, 512])\n",
      "14: torch.Size([50, 2])\n",
      "outputoutput: tensor([[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00,  5.0949e-07, -2.5764e-06,  ...,  3.6449e-05,\n",
      "         -7.1350e-06,  1.7200e-04],\n",
      "        ...,\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        [-5.7230e-05, -2.4159e-03, -8.4557e-05,  ...,  2.1307e-04,\n",
      "         -3.4054e-05, -0.0000e+00]], dtype=torch.float64)\n",
      "attributions_ig: tensor([[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00,  5.0949e-07, -2.5764e-06,  ...,  3.6449e-05,\n",
      "         -7.1350e-06,  1.7200e-04],\n",
      "        ...,\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        [-5.7230e-05, -2.4159e-03, -8.4557e-05,  ...,  2.1307e-04,\n",
      "         -3.4054e-05, -0.0000e+00]], dtype=torch.float64)\n",
      "true: Negative(0), pred: Negative(0), max delta: 0\n",
      "text: ['▁Written ', '▁by ', '▁someone ', '▁who ', '▁has ', '▁been ', '▁there ', ', ', '▁you ', '▁can ', '▁tell ', ', ', '▁but ', '▁only ', '▁if ', '▁you ', \"' \", 've ', '▁been ', '▁there ', '. ', '▁Excellent ', '▁performances ', '▁by ', '▁Meryl ', '▁Streep ', '▁( ', 'of ', '▁course ', '! ', ') ', ', ', '▁Renee ', '▁Zell ', 'we ', 'ger ', '▁and ', '▁William ', '▁Hurt ', '. ', '< ', 'br ', '▁/ ', '> ', '< ', 'br ', '▁/ ', '> ', 'Many ', '▁people ', '▁have ', '▁said ', '▁that ', '▁it ', '▁is ', '▁about ', '▁a ', '▁dysfunctional ', '▁family ', ', ', '▁I ', '▁think ', '▁every ', '▁family ', '▁is ', '▁dysfunctional ', '▁when ', '▁they ', '▁are ', '▁facing ', '▁this ', '▁kind ', '▁of ', '▁torment ', '. ', '▁To ', '▁NOT ', '▁be ', '▁dysfunctional ', '▁would ', '▁be ', '▁dysfunctional ', '! ', '▁You ', '▁are ', '▁losing ', '▁your ', '▁family ', '▁as ', '▁you ', '▁know ', '▁it ', ', ', '▁can ', '▁anything ', '▁be ', '▁worse ', '? ', '▁People ', '▁need ', '▁to ', '▁see ', '▁this ', '▁movie ', '▁so ', '▁when ', '▁they ', '▁are ', '▁faced ', '▁with ', '▁this ', '▁nightmare ', '▁maybe ', '▁they ', '▁will ', '▁change ', '▁how ', '▁they ', '▁do ', '▁it ', '. ', '▁Maybe ', '▁they ', '▁will ', '▁see ', '▁that ', '▁the ', '▁father ', '▁is ', '▁denying ', '▁himself ', '▁valuable ', '▁time ', '▁he ', \"' \", 'll ', '▁never ', '▁get ', '▁a ', '▁chance ', '▁at ', '▁again ', '. ', '▁Maybe ', '▁they ', '▁will ', '▁realize ', '▁how ', '▁hard ', '▁it ', '▁is ', '▁to ', '▁die ', ', ', '▁or ', '▁to ', '▁watch ', '▁someone ', '▁you ', '▁love ', '▁die ', '. ', '▁They ', '▁didn ', \"' \", 't ', '▁miss ', '▁much ', '▁of ', '▁the ', '▁nightmare ', ', ', '▁it ', \"' \", 's ', '▁hard ', '▁to ', '▁forget ', '. ', '\\x01', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], attributions_ig: torch.Size([64, 180]), labels: 0, delta: 0\n",
      "(180,)\n",
      ">>>>>>>>>>>>>>>>>>>> <<<<<<<<<<<<<<<<<<<<<<\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'loss_func' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_func'])`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: torch.Size([978])\n",
      "2.5: torch.Size([978, 64])\n",
      "2.6: torch.Size([978, 64])\n",
      "2.7: torch.Size([64, 978])\n",
      "2.8: torch.Size([64, 978])\n",
      "2.8 refined: torch.Size([64, 978])\n",
      "2.9: torch.Size([64, 978])\n",
      "3: torch.Size([64, 978])\n",
      "4: torch.Size([128, 180])\n",
      "4.5: torch.Size([64, 180]), self.hidden_dim: 64, self.is_tests_token_level: 4\n",
      "abababdadasd\n",
      "5: torch.Size([64, 180]), torch.Size([2, 0]), torch.Size([978]), torch.Size([180, 2]), torch.Size([180]), torch.Size([1]), torch.Size([1]), torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "6: torch.Size([180, 64])\n",
      "6 refined: torch.Size([180, 64])\n",
      "7: torch.Size([180, 64])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "7.3 edge_weights: 2\n",
      "7.3 edge_weights: torch.Size([2, 3600])\n",
      "7.3 edge_weights: torch.Size([3600, 4])\n",
      "7.4 graph.edge_index.shape: torch.Size([2, 3600])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "8: torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "8.1: torch.Size([180, 64])\n",
      "8.2 refined: torch.Size([180, 64])\n",
      "9: torch.Size([180, 64])\n",
      "10: torch.Size([180, 128])\n",
      "11: torch.Size([180, 128])\n",
      "12: torch.Size([180, 128])\n",
      "13: torch.Size([1, 512])\n",
      "14: torch.Size([1, 2])\n",
      "2: torch.Size([978])\n",
      "2.5: torch.Size([978, 64])\n",
      "2.6: torch.Size([978, 64])\n",
      "2.7: torch.Size([64, 978])\n",
      "2.8: torch.Size([64, 978])\n",
      "2.8 refined: torch.Size([64, 978])\n",
      "2.9: torch.Size([64, 978])\n",
      "3: torch.Size([64, 978])\n",
      "4: torch.Size([128, 180])\n",
      "4.5: torch.Size([64, 180]), self.hidden_dim: 64, self.is_tests_token_level: 4\n",
      "abababdadasd\n",
      "5: torch.Size([64, 180]), torch.Size([2, 0]), torch.Size([978]), torch.Size([180, 2]), torch.Size([180]), torch.Size([1]), torch.Size([1]), torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "6: torch.Size([180, 64])\n",
      "6 refined: torch.Size([180, 64])\n",
      "7: torch.Size([180, 64])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "7.3 edge_weights: 2\n",
      "7.3 edge_weights: torch.Size([2, 3600])\n",
      "7.3 edge_weights: torch.Size([3600, 4])\n",
      "7.4 graph.edge_index.shape: torch.Size([2, 3600])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "8: torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "8.1: torch.Size([180, 64])\n",
      "8.2 refined: torch.Size([180, 64])\n",
      "9: torch.Size([180, 64])\n",
      "10: torch.Size([180, 128])\n",
      "11: torch.Size([180, 128])\n",
      "12: torch.Size([180, 128])\n",
      "13: torch.Size([1, 512])\n",
      "14: torch.Size([1, 2])\n",
      "2: torch.Size([978])\n",
      "2.5: torch.Size([978, 64])\n",
      "2.6: torch.Size([978, 64])\n",
      "2.7: torch.Size([64, 978])\n",
      "2.8: torch.Size([64, 978])\n",
      "2.8 refined: torch.Size([64, 978])\n",
      "2.9: torch.Size([64, 978])\n",
      "3: torch.Size([64, 978])\n",
      "4: torch.Size([128, 180])\n",
      "4.5: torch.Size([64, 180]), self.hidden_dim: 64, self.is_tests_token_level: 4\n",
      "abababdadasd\n",
      "5: torch.Size([64, 180]), torch.Size([2, 0]), torch.Size([978]), torch.Size([180, 2]), torch.Size([180]), torch.Size([1]), torch.Size([1]), torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "6: torch.Size([180, 64])\n",
      "6 refined: torch.Size([180, 64])\n",
      "7: torch.Size([180, 64])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "7.3 edge_weights: 2\n",
      "7.3 edge_weights: torch.Size([2, 3600])\n",
      "7.3 edge_weights: torch.Size([3600, 4])\n",
      "7.4 graph.edge_index.shape: torch.Size([2, 3600])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "8: torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "8.1: torch.Size([180, 64])\n",
      "8.2 refined: torch.Size([180, 64])\n",
      "9: torch.Size([180, 64])\n",
      "10: torch.Size([180, 128])\n",
      "11: torch.Size([180, 128])\n",
      "12: torch.Size([180, 128])\n",
      "13: torch.Size([1, 512])\n",
      "14: torch.Size([1, 2])\n",
      "2: torch.Size([48900])\n",
      "2.5: torch.Size([48900, 64])\n",
      "2.6: torch.Size([48900, 64])\n",
      "2.7: torch.Size([64, 48900])\n",
      "2.8: torch.Size([64, 48900])\n",
      "2.8 refined: torch.Size([64, 48900])\n",
      "2.9: torch.Size([64, 48900])\n",
      "3: torch.Size([64, 48900])\n",
      "4: torch.Size([128, 9000])\n",
      "4.5: torch.Size([64, 9000]), self.hidden_dim: 64, self.is_tests_token_level: 4\n",
      "abababdadasd\n",
      "5: torch.Size([64, 9000]), torch.Size([100, 0]), torch.Size([48900]), torch.Size([9000, 2]), torch.Size([9000]), torch.Size([50]), torch.Size([50]), torch.Size([9000, 64])\n",
      "1 x: torch.Size([9000, 64])\n",
      "1 token_sentiments: torch.Size([9000, 2])\n",
      "6: torch.Size([9000, 64])\n",
      "6 refined: torch.Size([9000, 64])\n",
      "7: torch.Size([9000, 64])\n",
      "7.1: torch.Size([9000, 64])\n",
      "7.2 refined: torch.Size([9000, 64])\n",
      "7.3 edge_weights: 2\n",
      "7.3 edge_weights: torch.Size([2, 180000])\n",
      "7.3 edge_weights: torch.Size([180000, 4])\n",
      "7.4 graph.edge_index.shape: torch.Size([2, 180000])\n",
      "7.1: torch.Size([9000, 64])\n",
      "7.2 refined: torch.Size([9000, 64])\n",
      "8: torch.Size([9000, 64])\n",
      "1 x: torch.Size([9000, 64])\n",
      "1 token_sentiments: torch.Size([9000, 2])\n",
      "8.1: torch.Size([9000, 64])\n",
      "8.2 refined: torch.Size([9000, 64])\n",
      "9: torch.Size([9000, 64])\n",
      "10: torch.Size([9000, 128])\n",
      "11: torch.Size([9000, 128])\n",
      "12: torch.Size([9000, 128])\n",
      "13: torch.Size([50, 512])\n",
      "14: torch.Size([50, 2])\n",
      "outputoutput: tensor([[ 2.2836e-04,  5.1107e-03,  0.0000e+00,  ...,  3.9174e-04,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [ 1.2905e-07,  3.7437e-03, -2.0182e-04,  ..., -5.7666e-04,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-1.4696e-03,  3.2107e-03, -1.4557e-03,  ..., -4.3291e-04,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        ...,\n",
      "        [ 8.7793e-03,  3.7816e-03,  1.1007e-05,  ..., -1.1855e-03,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  3.1178e-03, -2.0947e-03,  ..., -3.3645e-03,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [ 8.0056e-04,  2.3676e-03,  3.4109e-04,  ..., -3.9615e-03,\n",
      "         -0.0000e+00,  0.0000e+00]], dtype=torch.float64)\n",
      "attributions_ig: tensor([[ 2.2836e-04,  5.1107e-03,  0.0000e+00,  ...,  3.9174e-04,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [ 1.2905e-07,  3.7437e-03, -2.0182e-04,  ..., -5.7666e-04,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-1.4696e-03,  3.2107e-03, -1.4557e-03,  ..., -4.3291e-04,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        ...,\n",
      "        [ 8.7793e-03,  3.7816e-03,  1.1007e-05,  ..., -1.1855e-03,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  3.1178e-03, -2.0947e-03,  ..., -3.3645e-03,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [ 8.0056e-04,  2.3676e-03,  3.4109e-04,  ..., -3.9615e-03,\n",
      "         -0.0000e+00,  0.0000e+00]], dtype=torch.float64)\n",
      "true: Negative(0), pred: Negative(0), max delta: 0\n",
      "text: ['▁Written ', '▁by ', '▁someone ', '▁who ', '▁has ', '▁been ', '▁there ', ', ', '▁you ', '▁can ', '▁tell ', ', ', '▁but ', '▁only ', '▁if ', '▁you ', \"' \", 've ', '▁been ', '▁there ', '. ', '▁Excellent ', '▁performances ', '▁by ', '▁Meryl ', '▁Streep ', '▁( ', 'of ', '▁course ', '! ', ') ', ', ', '▁Renee ', '▁Zell ', 'we ', 'ger ', '▁and ', '▁William ', '▁Hurt ', '. ', '< ', 'br ', '▁/ ', '> ', '< ', 'br ', '▁/ ', '> ', 'Many ', '▁people ', '▁have ', '▁said ', '▁that ', '▁it ', '▁is ', '▁about ', '▁a ', '▁dysfunctional ', '▁family ', ', ', '▁I ', '▁think ', '▁every ', '▁family ', '▁is ', '▁dysfunctional ', '▁when ', '▁they ', '▁are ', '▁facing ', '▁this ', '▁kind ', '▁of ', '▁torment ', '. ', '▁To ', '▁NOT ', '▁be ', '▁dysfunctional ', '▁would ', '▁be ', '▁dysfunctional ', '! ', '▁You ', '▁are ', '▁losing ', '▁your ', '▁family ', '▁as ', '▁you ', '▁know ', '▁it ', ', ', '▁can ', '▁anything ', '▁be ', '▁worse ', '? ', '▁People ', '▁need ', '▁to ', '▁see ', '▁this ', '▁movie ', '▁so ', '▁when ', '▁they ', '▁are ', '▁faced ', '▁with ', '▁this ', '▁nightmare ', '▁maybe ', '▁they ', '▁will ', '▁change ', '▁how ', '▁they ', '▁do ', '▁it ', '. ', '▁Maybe ', '▁they ', '▁will ', '▁see ', '▁that ', '▁the ', '▁father ', '▁is ', '▁denying ', '▁himself ', '▁valuable ', '▁time ', '▁he ', \"' \", 'll ', '▁never ', '▁get ', '▁a ', '▁chance ', '▁at ', '▁again ', '. ', '▁Maybe ', '▁they ', '▁will ', '▁realize ', '▁how ', '▁hard ', '▁it ', '▁is ', '▁to ', '▁die ', ', ', '▁or ', '▁to ', '▁watch ', '▁someone ', '▁you ', '▁love ', '▁die ', '. ', '▁They ', '▁didn ', \"' \", 't ', '▁miss ', '▁much ', '▁of ', '▁the ', '▁nightmare ', ', ', '▁it ', \"' \", 's ', '▁hard ', '▁to ', '▁forget ', '. ', '\\x01', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], attributions_ig: torch.Size([180, 64]), labels: 0, delta: 0\n",
      "(64,)\n",
      ">>>>>>>>>>>>>>>>>>>> <<<<<<<<<<<<<<<<<<<<<<\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'loss_func' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_func'])`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: torch.Size([978])\n",
      "2.5: torch.Size([978, 64])\n",
      "2.6: torch.Size([978, 64])\n",
      "2.7: torch.Size([64, 978])\n",
      "2.8: torch.Size([64, 978])\n",
      "2.8 refined: torch.Size([64, 978])\n",
      "2.9: torch.Size([64, 978])\n",
      "3: torch.Size([64, 978])\n",
      "4: torch.Size([128, 180])\n",
      "4.5: torch.Size([64, 180]), self.hidden_dim: 64, self.is_tests_token_level: 5\n",
      "abababdadasd\n",
      "5: torch.Size([64, 180]), torch.Size([2, 0]), torch.Size([978]), torch.Size([180, 2]), torch.Size([180]), torch.Size([1]), torch.Size([1]), torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "6: torch.Size([180, 64])\n",
      "6 refined: torch.Size([180, 64])\n",
      "7: torch.Size([180, 64])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "7.3 edge_weights: 2\n",
      "7.3 edge_weights: torch.Size([2, 3600])\n",
      "7.3 edge_weights: torch.Size([3600, 4])\n",
      "7.4 graph.edge_index.shape: torch.Size([2, 3600])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "8: torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "8.1: torch.Size([180, 64])\n",
      "8.2 refined: torch.Size([180, 64])\n",
      "9: torch.Size([180, 64])\n",
      "10: torch.Size([180, 128])\n",
      "11: torch.Size([180, 128])\n",
      "12: torch.Size([180, 128])\n",
      "13: torch.Size([1, 512])\n",
      "14: torch.Size([1, 2])\n",
      "2: torch.Size([978])\n",
      "2.5: torch.Size([978, 64])\n",
      "2.6: torch.Size([978, 64])\n",
      "2.7: torch.Size([64, 978])\n",
      "2.8: torch.Size([64, 978])\n",
      "2.8 refined: torch.Size([64, 978])\n",
      "2.9: torch.Size([64, 978])\n",
      "3: torch.Size([64, 978])\n",
      "4: torch.Size([128, 180])\n",
      "4.5: torch.Size([64, 180]), self.hidden_dim: 64, self.is_tests_token_level: 5\n",
      "abababdadasd\n",
      "5: torch.Size([64, 180]), torch.Size([2, 0]), torch.Size([978]), torch.Size([180, 2]), torch.Size([180]), torch.Size([1]), torch.Size([1]), torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "6: torch.Size([180, 64])\n",
      "6 refined: torch.Size([180, 64])\n",
      "7: torch.Size([180, 64])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "7.3 edge_weights: 2\n",
      "7.3 edge_weights: torch.Size([2, 3600])\n",
      "7.3 edge_weights: torch.Size([3600, 4])\n",
      "7.4 graph.edge_index.shape: torch.Size([2, 3600])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "8: torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "8.1: torch.Size([180, 64])\n",
      "8.2 refined: torch.Size([180, 64])\n",
      "9: torch.Size([180, 64])\n",
      "10: torch.Size([180, 128])\n",
      "11: torch.Size([180, 128])\n",
      "12: torch.Size([180, 128])\n",
      "13: torch.Size([1, 512])\n",
      "14: torch.Size([1, 2])\n",
      "2: torch.Size([978])\n",
      "2.5: torch.Size([978, 64])\n",
      "2.6: torch.Size([978, 64])\n",
      "2.7: torch.Size([64, 978])\n",
      "2.8: torch.Size([64, 978])\n",
      "2.8 refined: torch.Size([64, 978])\n",
      "2.9: torch.Size([64, 978])\n",
      "3: torch.Size([64, 978])\n",
      "4: torch.Size([128, 180])\n",
      "4.5: torch.Size([64, 180]), self.hidden_dim: 64, self.is_tests_token_level: 5\n",
      "abababdadasd\n",
      "5: torch.Size([64, 180]), torch.Size([2, 0]), torch.Size([978]), torch.Size([180, 2]), torch.Size([180]), torch.Size([1]), torch.Size([1]), torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "6: torch.Size([180, 64])\n",
      "6 refined: torch.Size([180, 64])\n",
      "7: torch.Size([180, 64])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "7.3 edge_weights: 2\n",
      "7.3 edge_weights: torch.Size([2, 3600])\n",
      "7.3 edge_weights: torch.Size([3600, 4])\n",
      "7.4 graph.edge_index.shape: torch.Size([2, 3600])\n",
      "7.1: torch.Size([180, 64])\n",
      "7.2 refined: torch.Size([180, 64])\n",
      "8: torch.Size([180, 64])\n",
      "1 x: torch.Size([180, 64])\n",
      "1 token_sentiments: torch.Size([180, 2])\n",
      "8.1: torch.Size([180, 64])\n",
      "8.2 refined: torch.Size([180, 64])\n",
      "9: torch.Size([180, 64])\n",
      "10: torch.Size([180, 128])\n",
      "11: torch.Size([180, 128])\n",
      "12: torch.Size([180, 128])\n",
      "13: torch.Size([1, 512])\n",
      "14: torch.Size([1, 2])\n",
      "2: torch.Size([48900])\n",
      "2.5: torch.Size([48900, 64])\n",
      "2.6: torch.Size([48900, 64])\n",
      "2.7: torch.Size([64, 48900])\n",
      "2.8: torch.Size([64, 48900])\n",
      "2.8 refined: torch.Size([64, 48900])\n",
      "2.9: torch.Size([64, 48900])\n",
      "3: torch.Size([64, 48900])\n",
      "4: torch.Size([128, 9000])\n",
      "4.5: torch.Size([64, 9000]), self.hidden_dim: 64, self.is_tests_token_level: 5\n",
      "abababdadasd\n",
      "5: torch.Size([64, 9000]), torch.Size([100, 0]), torch.Size([48900]), torch.Size([9000, 2]), torch.Size([9000]), torch.Size([50]), torch.Size([50]), torch.Size([9000, 64])\n",
      "1 x: torch.Size([9000, 64])\n",
      "1 token_sentiments: torch.Size([9000, 2])\n",
      "6: torch.Size([9000, 64])\n",
      "6 refined: torch.Size([9000, 64])\n",
      "7: torch.Size([9000, 64])\n",
      "7.1: torch.Size([9000, 64])\n",
      "7.2 refined: torch.Size([9000, 64])\n",
      "7.3 edge_weights: 100\n",
      "7.3 edge_weights: torch.Size([3600])\n",
      "7.3 edge_weights: torch.Size([3600])\n",
      "7.4 graph.edge_index.shape: torch.Size([2, 180000])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\CGNet\\FindBestModel\\5_LayersAttributionOnOutput\\with_positional_encoding.ipynb Cell 75\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y212sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mmodel\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y212sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m lig \u001b[39m=\u001b[39m LayerIntegratedGradients(model, ml(model))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y212sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m attributions_ig, tokens, characters, pred, pred_ind, label, delta \u001b[39m=\u001b[39m interpret_sentence(model, graph, labels, vocab_dict_rev, \u001b[39m7\u001b[39;49m, \u001b[39m1\u001b[39;49m, visualize_on_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, n_steps\u001b[39m=\u001b[39;49mn_steps, return_delta\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y212sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m add_attributions_to_visualizer(attributions_ig\u001b[39m.\u001b[39mT, tokens[:attributions_ig\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]], pred[\u001b[39m0\u001b[39m,pred_ind], pred_ind, label, delta, vis_data_records_ig)\n",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\CGNet\\FindBestModel\\5_LayersAttributionOnOutput\\with_positional_encoding.ipynb Cell 75\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y212sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m reference_indices \u001b[39m=\u001b[39m token_reference\u001b[39m.\u001b[39mgenerate_reference(seq_length, device\u001b[39m=\u001b[39mdevice)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y212sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# compute attributions and approximation delta using layer integrated gradients\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y212sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m output \u001b[39m=\u001b[39m lig\u001b[39m.\u001b[39;49mattribute(graph\u001b[39m.\u001b[39;49mx, ref_graph\u001b[39m.\u001b[39;49mx, \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y212sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m                                        additional_forward_args\u001b[39m=\u001b[39;49m(graph\u001b[39m.\u001b[39;49medge_index, graph\u001b[39m.\u001b[39;49mcumulative_token_indices, graph\u001b[39m.\u001b[39;49mtoken_sentiments, graph\u001b[39m.\u001b[39;49mtoken_lengths, graph\u001b[39m.\u001b[39;49mnum_tokens, graph\u001b[39m.\u001b[39;49mcharacter_length, graph\u001b[39m.\u001b[39;49mtoken_embeddings), \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y212sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m                                        n_steps\u001b[39m=\u001b[39;49mn_steps, return_convergence_delta\u001b[39m=\u001b[39;49mreturn_delta, target\u001b[39m=\u001b[39;49mlabels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y212sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mif\u001b[39;00m(return_delta):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y212sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     attributions_ig, delta \u001b[39m=\u001b[39m output\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\captum\\log\\__init__.py:42\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\captum\\attr\\_core\\layer\\layer_integrated_gradients.py:496\u001b[0m, in \u001b[0;36mLayerIntegratedGradients.attribute\u001b[1;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta, attribute_to_layer_input)\u001b[0m\n\u001b[0;32m    489\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mig\u001b[39m.\u001b[39mgradient_func \u001b[39m=\u001b[39m gradient_func\n\u001b[0;32m    490\u001b[0m all_inputs \u001b[39m=\u001b[39m (\n\u001b[0;32m    491\u001b[0m     (inps \u001b[39m+\u001b[39m additional_forward_args)\n\u001b[0;32m    492\u001b[0m     \u001b[39mif\u001b[39;00m additional_forward_args \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    493\u001b[0m     \u001b[39melse\u001b[39;00m inps\n\u001b[0;32m    494\u001b[0m )\n\u001b[1;32m--> 496\u001b[0m attributions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mig\u001b[39m.\u001b[39;49mattribute\u001b[39m.\u001b[39;49m__wrapped__(  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    497\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mig,  \u001b[39m# self\u001b[39;49;00m\n\u001b[0;32m    498\u001b[0m     inputs_layer,\n\u001b[0;32m    499\u001b[0m     baselines\u001b[39m=\u001b[39;49mbaselines_layer,\n\u001b[0;32m    500\u001b[0m     target\u001b[39m=\u001b[39;49mtarget,\n\u001b[0;32m    501\u001b[0m     additional_forward_args\u001b[39m=\u001b[39;49mall_inputs,\n\u001b[0;32m    502\u001b[0m     n_steps\u001b[39m=\u001b[39;49mn_steps,\n\u001b[0;32m    503\u001b[0m     method\u001b[39m=\u001b[39;49mmethod,\n\u001b[0;32m    504\u001b[0m     internal_batch_size\u001b[39m=\u001b[39;49minternal_batch_size,\n\u001b[0;32m    505\u001b[0m     return_convergence_delta\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    506\u001b[0m )\n\u001b[0;32m    508\u001b[0m \u001b[39m# handle multiple outputs\u001b[39;00m\n\u001b[0;32m    509\u001b[0m output: List[Tuple[Tensor, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]] \u001b[39m=\u001b[39m [\n\u001b[0;32m    510\u001b[0m     \u001b[39mtuple\u001b[39m(\n\u001b[0;32m    511\u001b[0m         attributions[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(num_outputs))\n\u001b[0;32m    516\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\captum\\attr\\_core\\integrated_gradients.py:286\u001b[0m, in \u001b[0;36mIntegratedGradients.attribute\u001b[1;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001b[0m\n\u001b[0;32m    274\u001b[0m     attributions \u001b[39m=\u001b[39m _batch_attribution(\n\u001b[0;32m    275\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    276\u001b[0m         num_examples,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    283\u001b[0m         method\u001b[39m=\u001b[39mmethod,\n\u001b[0;32m    284\u001b[0m     )\n\u001b[0;32m    285\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 286\u001b[0m     attributions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attribute(\n\u001b[0;32m    287\u001b[0m         inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[0;32m    288\u001b[0m         baselines\u001b[39m=\u001b[39;49mbaselines,\n\u001b[0;32m    289\u001b[0m         target\u001b[39m=\u001b[39;49mtarget,\n\u001b[0;32m    290\u001b[0m         additional_forward_args\u001b[39m=\u001b[39;49madditional_forward_args,\n\u001b[0;32m    291\u001b[0m         n_steps\u001b[39m=\u001b[39;49mn_steps,\n\u001b[0;32m    292\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[0;32m    293\u001b[0m     )\n\u001b[0;32m    295\u001b[0m \u001b[39mif\u001b[39;00m return_convergence_delta:\n\u001b[0;32m    296\u001b[0m     start_point, end_point \u001b[39m=\u001b[39m baselines, inputs\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\captum\\attr\\_core\\integrated_gradients.py:351\u001b[0m, in \u001b[0;36mIntegratedGradients._attribute\u001b[1;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, step_sizes_and_alphas)\u001b[0m\n\u001b[0;32m    348\u001b[0m expanded_target \u001b[39m=\u001b[39m _expand_target(target, n_steps)\n\u001b[0;32m    350\u001b[0m \u001b[39m# grads: dim -> (bsz * #steps x inputs[0].shape[1:], ...)\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m grads \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgradient_func(\n\u001b[0;32m    352\u001b[0m     forward_fn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_func,\n\u001b[0;32m    353\u001b[0m     inputs\u001b[39m=\u001b[39;49mscaled_features_tpl,\n\u001b[0;32m    354\u001b[0m     target_ind\u001b[39m=\u001b[39;49mexpanded_target,\n\u001b[0;32m    355\u001b[0m     additional_forward_args\u001b[39m=\u001b[39;49minput_additional_args,\n\u001b[0;32m    356\u001b[0m )\n\u001b[0;32m    358\u001b[0m \u001b[39m# flattening grads so that we can multilpy it with step-size\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \u001b[39m# calling contiguous to avoid `memory whole` problems\u001b[39;00m\n\u001b[0;32m    360\u001b[0m scaled_grads \u001b[39m=\u001b[39m [\n\u001b[0;32m    361\u001b[0m     grad\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(n_steps, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    362\u001b[0m     \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mtensor(step_sizes)\u001b[39m.\u001b[39mview(n_steps, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(grad\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    363\u001b[0m     \u001b[39mfor\u001b[39;00m grad \u001b[39min\u001b[39;00m grads\n\u001b[0;32m    364\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\captum\\attr\\_core\\layer\\layer_integrated_gradients.py:472\u001b[0m, in \u001b[0;36mLayerIntegratedGradients.attribute.<locals>.gradient_func\u001b[1;34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[0m\n\u001b[0;32m    468\u001b[0m         hooks\u001b[39m.\u001b[39mappend(hook)\n\u001b[0;32m    470\u001b[0m     \u001b[39m# the inputs is an empty tuple\u001b[39;00m\n\u001b[0;32m    471\u001b[0m     \u001b[39m# coz it is prepended into additional_forward_args\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m     output \u001b[39m=\u001b[39m _run_forward(\n\u001b[0;32m    473\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_func, \u001b[39mtuple\u001b[39;49m(), target_ind, additional_forward_args\n\u001b[0;32m    474\u001b[0m     )\n\u001b[0;32m    475\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    476\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m hooks:\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\captum\\_utils\\common.py:531\u001b[0m, in \u001b[0;36m_run_forward\u001b[1;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[0;32m    528\u001b[0m inputs \u001b[39m=\u001b[39m _format_inputs(inputs)\n\u001b[0;32m    529\u001b[0m additional_forward_args \u001b[39m=\u001b[39m _format_additional_forward_args(additional_forward_args)\n\u001b[1;32m--> 531\u001b[0m output \u001b[39m=\u001b[39m forward_func(\n\u001b[0;32m    532\u001b[0m     \u001b[39m*\u001b[39;49m(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49madditional_forward_args)\n\u001b[0;32m    533\u001b[0m     \u001b[39mif\u001b[39;49;00m additional_forward_args \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m    534\u001b[0m     \u001b[39melse\u001b[39;49;00m inputs\n\u001b[0;32m    535\u001b[0m )\n\u001b[0;32m    536\u001b[0m \u001b[39mreturn\u001b[39;00m _select_targets(output, target)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\CGNet\\FindBestModel\\5_LayersAttributionOnOutput\\with_positional_encoding.ipynb Cell 75\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y212sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m7.3 edge_weights: \u001b[39m\u001b[39m{\u001b[39;00medge_weights[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y212sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m7.4 graph.edge_index.shape: \u001b[39m\u001b[39m{\u001b[39;00mgraph\u001b[39m.\u001b[39medge_index\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y212sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m edge_weights \u001b[39m=\u001b[39m edge_weights[\u001b[39m1\u001b[39;49m][:\u001b[39mmin\u001b[39;49m(graph\u001b[39m.\u001b[39;49medge_index\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m], edge_weights[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]), \u001b[39m0\u001b[39;49m]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y212sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m7.1: \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y212sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrefine_shape(\u001b[39m5\u001b[39m, x, \u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "for i, ml in enumerate(model_layers):\n",
    "    print('>>>>>>>>>>>>>>>>>>>> <<<<<<<<<<<<<<<<<<<<<<')\n",
    "    model = load_model(156, r'logs\\CNN-GNN_False_False_False', 3, n_steps=n_steps, step_of_test=i)\n",
    "    model = model.model\n",
    "    lig = LayerIntegratedGradients(model, ml(model))\n",
    "    attributions_ig, tokens, characters, pred, pred_ind, label, delta = interpret_sentence(model, graph, labels, vocab_dict_rev, 7, 1, visualize_on_tokens=True, n_steps=n_steps, return_delta=False)\n",
    "    add_attributions_to_visualizer(attributions_ig.T, tokens[:attributions_ig.shape[1]], pred[0,pred_ind], pred_ind, label, delta, vis_data_records_ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(15, 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "type([1,4,5]) is tuple or type([1,4,5]) is list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_steps=50\n",
    "# model = load_model(156, r'logs\\CNN-GNN_False_False_False', 3, n_steps=n_steps, step_of_test=5)\n",
    "# model = model.model\n",
    "# lig = LayerIntegratedGradients(model, model.conv3)\n",
    "# labels = test_df.iloc[test_lengths.argmin()].Topic\n",
    "# graph = test_dataset.content_to_graph(test_df.iloc[test_lengths.argmin()].Content)\n",
    "# graph.cumulative_token_indices = test_dataset.caluculate_batch_token_positions(torch.tensor([graph.num_tokens]), torch.tensor([graph.character_length]), graph.token_indices)\n",
    "# attributions_ig, tokens, characters, pred, pred_ind, label, delta = interpret_sentence(model, graph, labels, vocab_dict_rev, 7, 1, visualize_on_tokens=True, n_steps=n_steps, return_delta=False)\n",
    "# add_attributions_to_visualizer(attributions_ig.T, tokens[:attributions_ig.shape[1]], pred[0,pred_ind], pred_ind, label, delta, vis_data_records_ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = visualization.visualize_text(vis_data_records_ig)\n",
    "html = _.data\n",
    "with open(r'FindBestModel\\5_LayersAttributionOnOutput\\VisualizationFiles\\result_2.html', 'w') as f:\n",
    "    f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lig = LayerIntegratedGradients(model, model.conv3)\n",
    "vis_data_records_ig = []\n",
    "graphs, labels = [], []\n",
    "for i in range(3):\n",
    "    graph, label = next(iter(test_dataset))\n",
    "    graphs.append(graph), labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lengths = np.array([len(t) for t in test_df.Content.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Read the book, forget the movie!'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.Content.values[test_lengths.argmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Data(x=[5701], token_positions=[1039], character_length=5701, num_tokens=1039, token_indices=[5701], token_lengths=[1039], token_embeddings=[1039, 64], token_sentiments=[1039, 2]),\n",
       " tensor([1., 0.]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[test_lengths.argmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 232])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributions_ig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis_data_records_ig = []\n",
    "# for i in range(3):\n",
    "#     attributions_ig, tokens, characters, pred, pred_ind, label, delta = interpret_sentence(model, graphs[i], labels[i], vocab_dict_rev, 7, label=1, visualize_on_tokens=False, n_steps=n_steps, return_delta=False)\n",
    "#     add_attributions_to_visualizer(attributions_ig.T, tokens[:attributions_ig.shape[1]], pred[0,pred_ind], pred_ind, label, delta, vis_data_records_ig)\n",
    "# print('Visualize attributions based on Integrated Gradients')\n",
    "# _ = visualization.visualize_text(vis_data_records_ig)\n",
    "# html = _.data\n",
    "# with open(r'FindBestModel\\5_LayersAttributionOnOutput\\VisualizationFiles\\result_conv3.html', 'w') as f:\n",
    "#     f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0., 1.]), tensor([0., 1.]), tensor([1., 0.])]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2820, 64])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_t = torch.randn((128, 2820))\n",
    "conv3 = torch.nn.Conv1d(128, 64, 3, padding=1)\n",
    "F.relu(conv3(input_t)).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,    0,    0,  ..., 1126, 1126, 1127])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([graph.token_indices]*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_steps=50\n",
    "# model = load_model(156, r'logs\\CNN-GNN_False_False_False', 3, n_steps=n_steps, step_of_test=2)\n",
    "# model = model.model\n",
    "# lig = LayerIntegratedGradients(model, model.conv1)\n",
    "# labels = test_df.iloc[test_lengths.argmin()].Topic\n",
    "# graph = test_dataset.content_to_graph(test_df.iloc[test_lengths.argmin()].Content)\n",
    "# # graph.cumulative_token_indices = test_dataset.caluculate_batch_token_positions(torch.tensor([graph.num_tokens]), torch.tensor([graph.character_length]), graph.token_indices)\n",
    "# # graph, labels = next(iter(test_dataset))\n",
    "# attributions_ig, tokens, characters, pred, pred_ind, label, delta = interpret_sentence(model, graph, labels, vocab_dict_rev, 7, 1, visualize_on_tokens=False, n_steps=n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_for_Text_No_Positional_Encoding(\n",
       "  (embedding): Embedding(156, 64)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (conv1): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (conv3): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (conv4): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (sentiment1): Sentiment_Injection(\n",
       "    (conv1): Conv1d(2, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (sentiment2): Sentiment_Injection(\n",
       "    (conv1): Conv1d(2, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (gcnn1): GCNN(\n",
       "    (gnn): GATv2Conv(64, 8, heads=4)\n",
       "    (conv): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (fc): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (gcnn2): GCNN(\n",
       "    (gnn): GATv2Conv(128, 16, heads=4)\n",
       "    (conv): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (fc): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (graph_generator): GenGraph(\n",
       "    (virtual_node_embeddings): Embedding(0, 64)\n",
       "  )\n",
       "  (fc0): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=32, bias=True)\n",
       "  (fc_out): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\CGNet\\FindBestModel\\5_LayersAttributionOnOutput\\with_positional_encoding.ipynb Cell 85\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y144sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m3\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y144sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     interpret_sentence(model, graphs[i], labels[i], vocab_dict_rev, \u001b[39m7\u001b[39m, label\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, visualize_on_tokens\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, n_steps\u001b[39m=\u001b[39mn_steps)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y144sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mVisualize attributions based on Integrated Gradients\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y144sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m _ \u001b[39m=\u001b[39m visualization\u001b[39m.\u001b[39mvisualize_text(vis_data_records_ig)\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    interpret_sentence(model, graphs[i], labels[i], vocab_dict_rev, 7, label=1, visualize_on_tokens=False, n_steps=n_steps)\n",
    "print('Visualize attributions based on Integrated Gradients')\n",
    "_ = visualization.visualize_text(vis_data_records_ig)\n",
    "html = _.data\n",
    "with open(r'FindBestModel\\5_LayersAttributionOnOutput\\VisualizationFiles\\result.html', 'w') as f:\n",
    "    f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 579 but got size 2895 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\CGNet\\FindBestModel\\5_LayersAttributionOnOutput\\with_positional_encoding.ipynb Cell 76\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y216sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m lig \u001b[39m=\u001b[39m LayerIntegratedGradients(model, model\u001b[39m.\u001b[39mconv3)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y216sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m graph, labels \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(test_dataset))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y216sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m interpret_sentence(model, graph, labels, vocab_dict_rev, \u001b[39m7\u001b[39;49m, \u001b[39m1\u001b[39;49m, visualize_on_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, n_steps\u001b[39m=\u001b[39;49mn_steps)\n",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\CGNet\\FindBestModel\\5_LayersAttributionOnOutput\\with_positional_encoding.ipynb Cell 76\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y216sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m reference_indices \u001b[39m=\u001b[39m token_reference\u001b[39m.\u001b[39mgenerate_reference(seq_length, device\u001b[39m=\u001b[39mdevice)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y216sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m# compute attributions and approximation delta using layer integrated gradients\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y216sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m attributions_ig, delta \u001b[39m=\u001b[39m lig\u001b[39m.\u001b[39;49mattribute(((graph\u001b[39m.\u001b[39;49mx, graph\u001b[39m.\u001b[39;49medge_index, graph\u001b[39m.\u001b[39;49mcumulative_token_indices, graph\u001b[39m.\u001b[39;49mtoken_sentiments, graph\u001b[39m.\u001b[39;49mtoken_lengths, graph\u001b[39m.\u001b[39;49mnum_tokens, graph\u001b[39m.\u001b[39;49mcharacter_length, graph\u001b[39m.\u001b[39;49mtoken_embeddings)), \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y216sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m                                        ((ref_graph\u001b[39m.\u001b[39;49mx, ref_graph\u001b[39m.\u001b[39;49medge_index, ref_graph\u001b[39m.\u001b[39;49mcumulative_token_indices, ref_graph\u001b[39m.\u001b[39;49mtoken_sentiments, ref_graph\u001b[39m.\u001b[39;49mtoken_lengths, ref_graph\u001b[39m.\u001b[39;49mnum_tokens, ref_graph\u001b[39m.\u001b[39;49mcharacter_length, ref_graph\u001b[39m.\u001b[39;49mtoken_embeddings)), \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y216sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m                                        n_steps\u001b[39m=\u001b[39;49mn_steps, return_convergence_delta\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, target\u001b[39m=\u001b[39;49mlabels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y216sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m captum_exp \u001b[39m=\u001b[39m CaptumExplainer(IntegratedGradients)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y216sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrue: \u001b[39m\u001b[39m{\u001b[39;00mid_class[labels]\u001b[39m}\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00mlabels\u001b[39m}\u001b[39;00m\u001b[39m), pred: \u001b[39m\u001b[39m{\u001b[39;00mid_class[pred_ind]\u001b[39m}\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00mpred_ind\u001b[39m}\u001b[39;00m\u001b[39m), max delta: \u001b[39m\u001b[39m{\u001b[39;00mdelta\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\captum\\log\\__init__.py:42\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\captum\\attr\\_core\\layer\\layer_integrated_gradients.py:496\u001b[0m, in \u001b[0;36mLayerIntegratedGradients.attribute\u001b[1;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta, attribute_to_layer_input)\u001b[0m\n\u001b[0;32m    489\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mig\u001b[39m.\u001b[39mgradient_func \u001b[39m=\u001b[39m gradient_func\n\u001b[0;32m    490\u001b[0m all_inputs \u001b[39m=\u001b[39m (\n\u001b[0;32m    491\u001b[0m     (inps \u001b[39m+\u001b[39m additional_forward_args)\n\u001b[0;32m    492\u001b[0m     \u001b[39mif\u001b[39;00m additional_forward_args \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    493\u001b[0m     \u001b[39melse\u001b[39;00m inps\n\u001b[0;32m    494\u001b[0m )\n\u001b[1;32m--> 496\u001b[0m attributions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mig\u001b[39m.\u001b[39;49mattribute\u001b[39m.\u001b[39;49m__wrapped__(  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    497\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mig,  \u001b[39m# self\u001b[39;49;00m\n\u001b[0;32m    498\u001b[0m     inputs_layer,\n\u001b[0;32m    499\u001b[0m     baselines\u001b[39m=\u001b[39;49mbaselines_layer,\n\u001b[0;32m    500\u001b[0m     target\u001b[39m=\u001b[39;49mtarget,\n\u001b[0;32m    501\u001b[0m     additional_forward_args\u001b[39m=\u001b[39;49mall_inputs,\n\u001b[0;32m    502\u001b[0m     n_steps\u001b[39m=\u001b[39;49mn_steps,\n\u001b[0;32m    503\u001b[0m     method\u001b[39m=\u001b[39;49mmethod,\n\u001b[0;32m    504\u001b[0m     internal_batch_size\u001b[39m=\u001b[39;49minternal_batch_size,\n\u001b[0;32m    505\u001b[0m     return_convergence_delta\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    506\u001b[0m )\n\u001b[0;32m    508\u001b[0m \u001b[39m# handle multiple outputs\u001b[39;00m\n\u001b[0;32m    509\u001b[0m output: List[Tuple[Tensor, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]] \u001b[39m=\u001b[39m [\n\u001b[0;32m    510\u001b[0m     \u001b[39mtuple\u001b[39m(\n\u001b[0;32m    511\u001b[0m         attributions[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(num_outputs))\n\u001b[0;32m    516\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\captum\\attr\\_core\\integrated_gradients.py:286\u001b[0m, in \u001b[0;36mIntegratedGradients.attribute\u001b[1;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001b[0m\n\u001b[0;32m    274\u001b[0m     attributions \u001b[39m=\u001b[39m _batch_attribution(\n\u001b[0;32m    275\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    276\u001b[0m         num_examples,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    283\u001b[0m         method\u001b[39m=\u001b[39mmethod,\n\u001b[0;32m    284\u001b[0m     )\n\u001b[0;32m    285\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 286\u001b[0m     attributions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attribute(\n\u001b[0;32m    287\u001b[0m         inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[0;32m    288\u001b[0m         baselines\u001b[39m=\u001b[39;49mbaselines,\n\u001b[0;32m    289\u001b[0m         target\u001b[39m=\u001b[39;49mtarget,\n\u001b[0;32m    290\u001b[0m         additional_forward_args\u001b[39m=\u001b[39;49madditional_forward_args,\n\u001b[0;32m    291\u001b[0m         n_steps\u001b[39m=\u001b[39;49mn_steps,\n\u001b[0;32m    292\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[0;32m    293\u001b[0m     )\n\u001b[0;32m    295\u001b[0m \u001b[39mif\u001b[39;00m return_convergence_delta:\n\u001b[0;32m    296\u001b[0m     start_point, end_point \u001b[39m=\u001b[39m baselines, inputs\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\captum\\attr\\_core\\integrated_gradients.py:351\u001b[0m, in \u001b[0;36mIntegratedGradients._attribute\u001b[1;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, step_sizes_and_alphas)\u001b[0m\n\u001b[0;32m    348\u001b[0m expanded_target \u001b[39m=\u001b[39m _expand_target(target, n_steps)\n\u001b[0;32m    350\u001b[0m \u001b[39m# grads: dim -> (bsz * #steps x inputs[0].shape[1:], ...)\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m grads \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgradient_func(\n\u001b[0;32m    352\u001b[0m     forward_fn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_func,\n\u001b[0;32m    353\u001b[0m     inputs\u001b[39m=\u001b[39;49mscaled_features_tpl,\n\u001b[0;32m    354\u001b[0m     target_ind\u001b[39m=\u001b[39;49mexpanded_target,\n\u001b[0;32m    355\u001b[0m     additional_forward_args\u001b[39m=\u001b[39;49minput_additional_args,\n\u001b[0;32m    356\u001b[0m )\n\u001b[0;32m    358\u001b[0m \u001b[39m# flattening grads so that we can multilpy it with step-size\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \u001b[39m# calling contiguous to avoid `memory whole` problems\u001b[39;00m\n\u001b[0;32m    360\u001b[0m scaled_grads \u001b[39m=\u001b[39m [\n\u001b[0;32m    361\u001b[0m     grad\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(n_steps, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    362\u001b[0m     \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mtensor(step_sizes)\u001b[39m.\u001b[39mview(n_steps, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(grad\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    363\u001b[0m     \u001b[39mfor\u001b[39;00m grad \u001b[39min\u001b[39;00m grads\n\u001b[0;32m    364\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\captum\\attr\\_core\\layer\\layer_integrated_gradients.py:472\u001b[0m, in \u001b[0;36mLayerIntegratedGradients.attribute.<locals>.gradient_func\u001b[1;34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[0m\n\u001b[0;32m    468\u001b[0m         hooks\u001b[39m.\u001b[39mappend(hook)\n\u001b[0;32m    470\u001b[0m     \u001b[39m# the inputs is an empty tuple\u001b[39;00m\n\u001b[0;32m    471\u001b[0m     \u001b[39m# coz it is prepended into additional_forward_args\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m     output \u001b[39m=\u001b[39m _run_forward(\n\u001b[0;32m    473\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_func, \u001b[39mtuple\u001b[39;49m(), target_ind, additional_forward_args\n\u001b[0;32m    474\u001b[0m     )\n\u001b[0;32m    475\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    476\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m hooks:\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\captum\\_utils\\common.py:531\u001b[0m, in \u001b[0;36m_run_forward\u001b[1;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[0;32m    528\u001b[0m inputs \u001b[39m=\u001b[39m _format_inputs(inputs)\n\u001b[0;32m    529\u001b[0m additional_forward_args \u001b[39m=\u001b[39m _format_additional_forward_args(additional_forward_args)\n\u001b[1;32m--> 531\u001b[0m output \u001b[39m=\u001b[39m forward_func(\n\u001b[0;32m    532\u001b[0m     \u001b[39m*\u001b[39;49m(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49madditional_forward_args)\n\u001b[0;32m    533\u001b[0m     \u001b[39mif\u001b[39;49;00m additional_forward_args \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m    534\u001b[0m     \u001b[39melse\u001b[39;49;00m inputs\n\u001b[0;32m    535\u001b[0m )\n\u001b[0;32m    536\u001b[0m \u001b[39mreturn\u001b[39;00m _select_targets(output, target)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\CGNet\\FindBestModel\\5_LayersAttributionOnOutput\\with_positional_encoding.ipynb Cell 76\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y216sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(x))\u001b[39m.\u001b[39mT\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y216sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m# if self.use_token_polarity[1]:\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y216sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentiment1(x, token_sentiments)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y216sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m rand_edges, lattice_edges \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_random_edges, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_lattice_edges\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y216sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m graph \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph_generator\u001b[39m.\u001b[39mgen_graph(x, \u001b[39mlen\u001b[39m(token_lengths), num_tokens, rand_edges, lattice_edges, lattice_start_distance\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlattice_start_distance)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\CGNet\\FindBestModel\\5_LayersAttributionOnOutput\\with_positional_encoding.ipynb Cell 76\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y216sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, token_sentiments):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y216sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     x1 \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(token_sentiments\u001b[39m.\u001b[39mT)\u001b[39m.\u001b[39mT))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y216sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(torch\u001b[39m.\u001b[39;49mcat([x, x1], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39mT))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y216sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# x = x + x1\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/CGNet/FindBestModel/5_LayersAttributionOnOutput/with_positional_encoding.ipynb#Y216sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39mT\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 579 but got size 2895 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "lig = LayerIntegratedGradients(model, model.conv3)\n",
    "graph, labels = next(iter(test_dataset))\n",
    "interpret_sentence(model, graph, labels, vocab_dict_rev, 7, 1, visualize_on_tokens=True, n_steps=n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0][1].argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = test_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[4988], token_positions=[924], character_length=4988, num_tokens=924, token_indices=[4988], token_lengths=[924], token_embeddings=[924, 64], token_sentiments=[924, 2])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/CNN-GNN_False_False_False\\\\version_1\\\\checkpoints\\\\epoch=18-step=2128.ckpt'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_manager.trainer.checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = test_dataset[0][0]\n",
    "graph.cumulative_token_indices = graph.token_indices\n",
    "graph = Batch.from_data_list([graph])\n",
    "pred = torch.softmax(model(graph.to(device)), dim=1).detach()\n",
    "pred_ind = torch.argmax(pred).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6096, 0.3904]], device='cuda:0')"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[326], token_positions=[59], character_length=326, num_tokens=59, token_indices=[326], token_lengths=[59], token_embeddings=[59, 64], token_sentiments=[59, 2])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0)\n",
      "1 tensor(0)\n",
      "2 tensor(0)\n",
      "3 tensor(0)\n",
      "4 tensor(0)\n",
      "5 tensor(0)\n",
      "6 tensor(1)\n",
      "7 tensor(1)\n",
      "8 tensor(1)\n",
      "9 tensor(1)\n",
      "10 tensor(1)\n",
      "11 tensor(1)\n",
      "12 tensor(1)\n",
      "13 tensor(2)\n",
      "14 tensor(2)\n",
      "15 tensor(2)\n",
      "16 tensor(2)\n",
      "17 tensor(2)\n",
      "18 tensor(2)\n",
      "19 tensor(2)\n",
      "20 tensor(3)\n",
      "21 tensor(3)\n",
      "22 tensor(3)\n",
      "23 tensor(3)\n",
      "24 tensor(3)\n",
      "25 tensor(4)\n",
      "26 tensor(4)\n",
      "27 tensor(4)\n",
      "28 tensor(4)\n",
      "29 tensor(4)\n",
      "30 tensor(4)\n",
      "31 tensor(5)\n",
      "32 tensor(5)\n",
      "33 tensor(5)\n",
      "34 tensor(5)\n",
      "35 tensor(5)\n",
      "36 tensor(5)\n",
      "37 tensor(6)\n",
      "38 tensor(6)\n",
      "39 tensor(6)\n",
      "40 tensor(6)\n",
      "41 tensor(6)\n",
      "42 tensor(6)\n",
      "43 tensor(6)\n",
      "44 tensor(7)\n",
      "45 tensor(7)\n",
      "46 tensor(7)\n",
      "47 tensor(7)\n",
      "48 tensor(7)\n",
      "49 tensor(8)\n",
      "50 tensor(8)\n",
      "51 tensor(8)\n",
      "52 tensor(8)\n",
      "53 tensor(8)\n",
      "54 tensor(9)\n",
      "55 tensor(9)\n",
      "56 tensor(9)\n",
      "57 tensor(9)\n",
      "58 tensor(9)\n",
      "59 tensor(9)\n",
      "60 tensor(10)\n",
      "61 tensor(10)\n",
      "62 tensor(10)\n",
      "63 tensor(11)\n",
      "64 tensor(11)\n",
      "65 tensor(11)\n",
      "66 tensor(11)\n",
      "67 tensor(11)\n",
      "68 tensor(11)\n",
      "69 tensor(12)\n",
      "70 tensor(12)\n",
      "71 tensor(13)\n",
      "72 tensor(13)\n",
      "73 tensor(13)\n",
      "74 tensor(13)\n",
      "75 tensor(13)\n",
      "76 tensor(14)\n",
      "77 tensor(14)\n",
      "78 tensor(14)\n",
      "79 tensor(14)\n",
      "80 tensor(15)\n",
      "81 tensor(15)\n",
      "82 tensor(15)\n",
      "83 tensor(15)\n",
      "84 tensor(15)\n",
      "85 tensor(16)\n",
      "86 tensor(16)\n",
      "87 tensor(16)\n",
      "88 tensor(16)\n",
      "89 tensor(16)\n",
      "90 tensor(17)\n",
      "91 tensor(17)\n",
      "92 tensor(17)\n",
      "93 tensor(17)\n",
      "94 tensor(17)\n",
      "95 tensor(17)\n",
      "96 tensor(17)\n",
      "97 tensor(17)\n",
      "98 tensor(18)\n",
      "99 tensor(18)\n",
      "100 tensor(18)\n",
      "101 tensor(19)\n",
      "102 tensor(19)\n",
      "103 tensor(20)\n",
      "104 tensor(20)\n",
      "105 tensor(20)\n",
      "106 tensor(21)\n",
      "107 tensor(21)\n",
      "108 tensor(21)\n",
      "109 tensor(21)\n",
      "110 tensor(21)\n",
      "111 tensor(21)\n",
      "112 tensor(22)\n",
      "113 tensor(22)\n",
      "114 tensor(22)\n",
      "115 tensor(22)\n",
      "116 tensor(22)\n",
      "117 tensor(22)\n",
      "118 tensor(22)\n",
      "119 tensor(22)\n",
      "120 tensor(23)\n",
      "121 tensor(23)\n",
      "122 tensor(23)\n",
      "123 tensor(23)\n",
      "124 tensor(23)\n",
      "125 tensor(24)\n",
      "126 tensor(24)\n",
      "127 tensor(24)\n",
      "128 tensor(24)\n",
      "129 tensor(25)\n",
      "130 tensor(25)\n",
      "131 tensor(26)\n",
      "132 tensor(26)\n",
      "133 tensor(26)\n",
      "134 tensor(26)\n",
      "135 tensor(27)\n",
      "136 tensor(27)\n",
      "137 tensor(28)\n",
      "138 tensor(28)\n",
      "139 tensor(29)\n",
      "140 tensor(29)\n",
      "141 tensor(29)\n",
      "142 tensor(29)\n",
      "143 tensor(29)\n",
      "144 tensor(29)\n",
      "145 tensor(30)\n",
      "146 tensor(30)\n",
      "147 tensor(30)\n",
      "148 tensor(30)\n",
      "149 tensor(30)\n",
      "150 tensor(30)\n",
      "151 tensor(31)\n",
      "152 tensor(31)\n",
      "153 tensor(31)\n",
      "154 tensor(31)\n",
      "155 tensor(32)\n",
      "156 tensor(32)\n",
      "157 tensor(32)\n",
      "158 tensor(32)\n",
      "159 tensor(32)\n",
      "160 tensor(32)\n",
      "161 tensor(32)\n",
      "162 tensor(33)\n",
      "163 tensor(33)\n",
      "164 tensor(33)\n",
      "165 tensor(33)\n",
      "166 tensor(34)\n",
      "167 tensor(34)\n",
      "168 tensor(34)\n",
      "169 tensor(35)\n",
      "170 tensor(35)\n",
      "171 tensor(35)\n",
      "172 tensor(35)\n",
      "173 tensor(35)\n",
      "174 tensor(35)\n",
      "175 tensor(35)\n",
      "176 tensor(36)\n",
      "177 tensor(36)\n",
      "178 tensor(36)\n",
      "179 tensor(36)\n",
      "180 tensor(36)\n",
      "181 tensor(36)\n",
      "182 tensor(36)\n",
      "183 tensor(37)\n",
      "184 tensor(37)\n",
      "185 tensor(37)\n",
      "186 tensor(37)\n",
      "187 tensor(37)\n",
      "188 tensor(37)\n",
      "189 tensor(38)\n",
      "190 tensor(38)\n",
      "191 tensor(38)\n",
      "192 tensor(38)\n",
      "193 tensor(38)\n",
      "194 tensor(38)\n",
      "195 tensor(39)\n",
      "196 tensor(39)\n",
      "197 tensor(39)\n",
      "198 tensor(39)\n",
      "199 tensor(39)\n",
      "200 tensor(40)\n",
      "201 tensor(40)\n",
      "202 tensor(40)\n",
      "203 tensor(40)\n",
      "204 tensor(40)\n",
      "205 tensor(40)\n",
      "206 tensor(41)\n",
      "207 tensor(41)\n",
      "208 tensor(41)\n",
      "209 tensor(41)\n",
      "210 tensor(41)\n",
      "211 tensor(41)\n",
      "212 tensor(41)\n",
      "213 tensor(41)\n",
      "214 tensor(42)\n",
      "215 tensor(42)\n",
      "216 tensor(43)\n",
      "217 tensor(43)\n",
      "218 tensor(43)\n",
      "219 tensor(43)\n",
      "220 tensor(43)\n",
      "221 tensor(43)\n",
      "222 tensor(44)\n",
      "223 tensor(44)\n",
      "224 tensor(45)\n",
      "225 tensor(45)\n",
      "226 tensor(46)\n",
      "227 tensor(46)\n",
      "228 tensor(47)\n",
      "229 tensor(47)\n",
      "230 tensor(47)\n",
      "231 tensor(48)\n",
      "232 tensor(48)\n",
      "233 tensor(48)\n",
      "234 tensor(48)\n",
      "235 tensor(48)\n",
      "236 tensor(48)\n",
      "237 tensor(48)\n",
      "238 tensor(48)\n",
      "239 tensor(48)\n",
      "240 tensor(49)\n",
      "241 tensor(49)\n",
      "242 tensor(49)\n",
      "243 tensor(49)\n",
      "244 tensor(49)\n",
      "245 tensor(49)\n",
      "246 tensor(49)\n",
      "247 tensor(49)\n",
      "248 tensor(50)\n",
      "249 tensor(50)\n",
      "250 tensor(50)\n",
      "251 tensor(50)\n",
      "252 tensor(50)\n",
      "253 tensor(50)\n",
      "254 tensor(50)\n",
      "255 tensor(51)\n",
      "256 tensor(51)\n",
      "257 tensor(51)\n",
      "258 tensor(51)\n",
      "259 tensor(52)\n",
      "260 tensor(52)\n",
      "261 tensor(53)\n",
      "262 tensor(53)\n",
      "263 tensor(53)\n",
      "264 tensor(53)\n",
      "265 tensor(53)\n",
      "266 tensor(54)\n",
      "267 tensor(54)\n",
      "268 tensor(54)\n",
      "269 tensor(54)\n",
      "270 tensor(54)\n",
      "271 tensor(55)\n",
      "272 tensor(55)\n",
      "273 tensor(55)\n",
      "274 tensor(55)\n",
      "275 tensor(55)\n",
      "276 tensor(55)\n",
      "277 tensor(55)\n",
      "278 tensor(56)\n",
      "279 tensor(56)\n",
      "280 tensor(57)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"▁This ▁movie ▁makes ▁you ▁wish ▁imdb ▁would ▁let ▁you ▁vote ▁a ▁zero . ▁One ▁of ▁the ▁two ▁movies ▁I ' ve ▁ever ▁walked ▁out ▁of . ▁It ' s ▁very ▁hard ▁to ▁think ▁of ▁a ▁worse ▁movie ▁with ▁such ▁big ▁name ▁actors . ▁Well . . . Ar mageddon ▁almost ▁takes ▁it , ▁but ▁not ▁quite . \\x01\""
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = test_dataset[0][0]\n",
    "tokens = ['']*len(graph.token_indices)\n",
    "for i, j in enumerate(graph.token_indices):\n",
    "    print(i, j)\n",
    "    j = j.item()\n",
    "    tokens[j] += vocab_dict_rev[graph.x[i].item()]\n",
    "''.join(tokens)\n",
    "# test_dataset[0][0].x\n",
    "# test_dataset[0][0].token_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([157,  56, 129,  ..., 141,  70, 211])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0][0].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁When ▁the ▁Romulan s ▁come , ▁they ▁will ▁not ▁be ▁bearing ▁gifts ; ▁no , ▁they ▁bring ▁with ▁them ▁war ▁- ▁war ▁and ▁conquest . ▁As ▁any ▁familiar ▁with ▁this ▁episode ▁know , ▁it ▁is ▁a ▁red ux ▁of ▁the ▁war ▁film ▁\" The ▁Enemy ▁Below \" ▁from ▁the ▁fifties . ▁The ▁obvious ▁difference ▁is ▁that ▁instead ▁of ▁a ▁battleship ▁and ▁a ▁submarine ▁( or ▁an ▁American ▁Destroyer ▁& ▁German ▁U - boat ) ▁engaged ▁in ▁lethal ▁war ▁games , ▁it ▁is ▁two ▁starship s ▁in ▁outer ▁space . ▁In ▁Trek ▁history , ▁about ▁100 ▁years ▁before ▁the ▁events ▁here , ▁according ▁to ▁this ▁episode , ▁Earth ▁fought ▁the ▁Romulan ▁Wars . ▁After ▁about ▁5 ▁years ▁of ▁conflict , ▁a ▁stalemate ▁brought ▁about ▁a ▁treaty ▁and ▁the ▁institution ▁of ▁the ▁Neutral ▁Zone , ▁a ▁boundary ▁between ▁us ▁and ▁the ▁Romulan ▁Empire . ▁Now , ▁on ▁this ▁star date , ▁the ▁treaty ▁appears ▁to ▁be ▁broken , ▁as ▁our ▁outposts ▁are ▁being ▁attacked ▁and ▁destroyed ▁by ▁some ▁weapon ▁of ▁immense ▁power . ▁Yes , ▁the ▁Romulan s ▁are ▁back , ▁testing ▁their ▁new ▁war ▁toy , ▁and ▁Kirk ▁must ▁now ▁earn ▁his ▁pay : ▁he ▁must ▁make ▁decisions ▁that ▁would ▁affect ▁this ▁sector ▁of ▁the ▁galaxy , ▁such ▁as ▁figuring ▁out ▁how ▁to ▁avoid ▁a . . . oh , ▁I ▁dunno ▁- ▁an ▁interstellar ▁war , ▁maybe ? ▁< br ▁/ > < br ▁/ > I ▁think ▁what ▁makes ▁this ▁episode ▁so ▁effective ▁is ▁that ▁it ▁doesn \\' t ▁shy ▁away ▁from ▁the ▁grim ▁aspects ▁of ▁war , ▁as ▁one ▁would ▁expect ▁of ▁a ▁mere ▁TV ▁episode ▁from ▁the ▁sixties ▁- ▁especially ▁an ▁episode ▁from ▁a ▁science ▁fiction ▁show . ▁It \\' s ▁all ▁very ▁tense ▁and ▁gripping , ▁like ▁the ▁best ▁war ▁films , ▁such ▁as ▁when ▁Kirk ▁sits ▁down ▁with ▁his ▁key ▁officers ▁for ▁what ▁amounts ▁to ▁a ▁war ▁council . ▁The ▁writers ▁and ▁the ▁actors ▁aren \\' t ▁kidding ▁around ▁here : ▁this ▁is ▁all ▁preparation ▁for ▁a ▁ghastly ▁conflict , ▁potentially ▁the ▁beginning ▁of ▁another ▁years - long ▁battleground . ▁In ▁the ▁final ▁analysis , ▁Kirk \\' s ▁aim ▁is ▁to ▁keep ▁this ▁battleground ▁to ▁just ▁the ▁two ▁ships ▁- ▁but ▁even ▁then ▁it \\' s ▁an ▁endeavor ▁fraught ▁with ▁peril ▁and ▁probable ▁casualties . ▁In ▁fact , ▁I ▁believe ▁this ▁episode ▁holds ▁the ▁record ▁for ▁ship ▁casualties ▁by ▁the ▁end ▁of ▁it . ▁Right ▁at ▁the ▁start ▁of ▁the ▁episode , ▁we ▁see ▁the ▁devastation ▁such ▁battle ▁can ▁produce , ▁in ▁that ▁supposedly ▁well - protected ▁outpost . ▁Then ▁begin ▁the ▁cat - and - mouse ▁war ▁games ▁between ▁the ▁Enterprise ▁and ▁the ▁Romulan ▁ship ▁- ▁it \\' s ▁as ▁exciting ▁as ▁any ▁conflict ▁we \\' ve ▁seen ▁on ▁the ▁big ▁screen . ▁Of ▁course , ▁if ▁you \\' re ▁not ▁into ▁war ▁films , ▁you \\' d ▁have ▁to ▁look ▁for ▁other ▁things ▁to ▁admire ▁in ▁this ▁episode . < br ▁/ > < br ▁/ > What ▁elevates ▁this ▁episode ▁even ▁further ▁is ▁the ▁revelation ▁of ▁just ▁what ▁and ▁who ▁the ▁Romulan s ▁are ▁- ▁it \\' s ▁an ▁electric ▁shock ▁of ▁a ▁sort . ▁Now ▁we ▁have ▁even ▁further ▁inter - crew ▁conflict ▁on ▁the ▁bridge ▁of ▁the ▁Enterprise ▁- ▁war ▁does ▁tend ▁to ▁bring ▁out ▁the ▁worst ▁in ▁some ▁people . ▁Due ▁to ▁still ▁nasty ▁attitudes ▁about ▁race ▁in ▁this ▁future , ▁the ▁tension ▁is ▁ratcheted ▁up ▁even ▁further ▁- ▁Kirk ▁has ▁his ▁hands ▁full ▁in ▁this ▁one . ▁I ▁suppose ▁the ▁one ▁weakness ▁in ▁the ▁story ▁is ▁the ▁convenient ▁relent ing ▁of ▁the ▁bigotry ▁issue ▁by ▁the ▁conclusion . ▁On ▁the ▁Romulan ▁side , ▁actor ▁Lena rd ▁makes ▁his ▁first ▁appearance ▁in ▁the ▁Trek ▁universe ▁as ▁the ▁Romulan ▁commander ; ▁he \\' s ▁terrific ▁in ▁the ▁role , ▁the ▁flip ▁side ▁of ▁Capt . ▁Kirk ▁or ▁Capt . ▁Pike , ▁take ▁your ▁pick , ▁done ▁up ▁to ▁resemble ▁Spock ▁more ▁than ▁a ▁little . ▁Surprisingly , ▁his ▁character ▁is ▁not ▁war ▁hungry ▁as ▁we ▁would ▁expect , ▁another ▁eye - opener ▁for ▁this ▁episode . ▁The ▁actor ▁would ▁next ▁return ▁to ▁this ▁universe ▁as ▁S arek , ▁Spock \\' s ▁father , ▁so ▁he \\' s ▁nothing ▁if ▁not ▁versatile . ▁It \\' s ▁also ▁telling ▁how ▁the ▁first ▁appearance ▁of ▁such ▁characters ▁as ▁the ▁Romulan s ▁is ▁usually ▁their ▁best ▁shot , ▁as ▁it ▁is ▁here . ▁They ▁showed ▁up ▁in ▁\" The ▁Enterprise ▁Incident \" ▁next . \\x01'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([vocab_dict_rev[i.item()] for i in test_dataset[0][0].x])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
