{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import time\n",
    "from torch_scatter import scatter_max, scatter_mean, scatter_std\n",
    "from sklearn.model_selection import KFold\n",
    "import torchmetrics\n",
    "import lightning as L\n",
    "from torch_geometric.data import Batch, Data\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "from transformers import AutoModel, DebertaV2Tokenizer, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_vocab = {v:k for k,v in tokenizer.vocab.items()}\n",
    "all_vocab_indices = list(id_vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# all_vocab_str = []\n",
    "# vector_keys = list(nlp.vocab.vectors.keys())\n",
    "# for i in range(len(vector_keys)):\n",
    "#     try:\n",
    "#         t = nlp.vocab.strings[vector_keys[i]]\n",
    "#         all_vocab_str.append(t)\n",
    "#     except:\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# Create a set of all English characters, numbers, and punctuation\n",
    "allowed_chars = string.ascii_letters + string.digits + string.punctuation\n",
    "all_chars = set(allowed_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'Data\\ReducedEmbeddings\\deberta_larg_reduced_embeddings_64.npy', 'rb') as f:\n",
    "    embeddings = np.load(f)\n",
    "embeddings = torch.from_numpy(embeddings)\n",
    "all_vocab_str = []\n",
    "for i in range(len(id_vocab)):\n",
    "    all_vocab_str.append(id_vocab[i])\n",
    "    \n",
    "# embeddings = (embeddings - torch.min(embeddings)) / (torch.max(embeddings)-torch.min(embeddings))\n",
    "token_vocab_dict = dict(zip(all_vocab_str, embeddings))\n",
    "# del all_vocab_str\n",
    "# del all_vocab_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(r'Data\\ReducedEmbeddings\\polarity_debertav3_tokens_gpt_mini_emb.npy', 'rb') as f:\n",
    "    polarities_subjectivities= np.load(f)\n",
    "polarities_subjectivities = torch.from_numpy(polarities_subjectivities)\n",
    "polarity_vocab_dict = dict(zip(all_vocab_str, polarities_subjectivities))\n",
    "polarity_vocab_dict['<n>'] = torch.tensor([0.0, 0.0])\n",
    "len(token_vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128001, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarities_subjectivities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1085 tensor([0.7000, 0.6000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_vocab_str)):\n",
    "    if 'nice' in all_vocab_str[i]:\n",
    "        print(i, polarities_subjectivities[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# Create a set of all English characters, numbers, and punctuation\n",
    "allowed_chars = string.ascii_letters + string.digits + string.punctuation + ' \\t'\n",
    "all_chars = set(allowed_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 50\n",
    "folder_path = r'data\\TextClassification\\review_polarity'\n",
    "# t_tokenizer = TweetTokenizer()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path, shuffle = False):\n",
    "    pos_path = os.path.join(dataset_path, 'pos')\n",
    "    neg_path = os.path.join(dataset_path, 'neg')\n",
    "    pos_names = os.listdir(pos_path)\n",
    "    neg_names = os.listdir(neg_path)\n",
    "    df = []\n",
    "    for pname in pos_names:\n",
    "        file_path = os.path.join(pos_path, pname)\n",
    "        with open(file_path, 'rt', encoding='utf8') as f:\n",
    "            df.append([1, f.read()])\n",
    "    \n",
    "    for nname in neg_names:\n",
    "        file_path = os.path.join(neg_path, nname)\n",
    "        with open(file_path, 'rt', encoding='utf8') as f:\n",
    "            df.append([0, f.read()])\n",
    "    df = pd.DataFrame(data=df, columns=['label', 'text'])\n",
    "    if shuffle:\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_ratio = 1.0\n",
    "df = load_dataset(r'Data\\TextClassification\\review_polarity')\n",
    "# df.text = df.text.apply(lambda d: d[:50])\n",
    "df.dropna(inplace=True)\n",
    "df = df.iloc[:int(keep_ratio*df.shape[0])]\n",
    "target_classes = [\"Negative\", \"Positive\"]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14957\n"
     ]
    }
   ],
   "source": [
    "doc_lengths = np.array([len(df.text[i]) for i in df.index])\n",
    "print(np.max(doc_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = df.label.unique()\n",
    "class_id = {target_classes[i]:i for i in class_list}\n",
    "id_class = {i:target_classes[i] for i in class_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_set = set()\n",
    "for doc in df.text.values:\n",
    "    char_set.update(set(' '.join(tokenizer.tokenize(doc))))\n",
    "len(char_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_dict = {c:i for i, c in enumerate(allowed_chars)}\n",
    "# if '\\x01' not in vocab_dict:\n",
    "#     vocab_dict['\\x01'] = len(vocab_dict)\n",
    "# char_Set = set(vocab_dict.keys())\n",
    "# len(char_Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_chars = string.ascii_letters + string.digits + string.punctuation + ' '\n",
    "all_chars = set(''.join(char_set).join(allowed_chars))\n",
    "vocab_dict = {c:i for i, c in enumerate(all_chars)}\n",
    "if '\\x01' not in vocab_dict:\n",
    "    vocab_dict['\\x01'] = len(vocab_dict)\n",
    "char_Set = set(vocab_dict.keys())\n",
    "num_embedding = len(vocab_dict)\n",
    "vocab_dict_rev = dict(zip(list(vocab_dict.values()), list(vocab_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFoldCharacterandTokenLevelDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, num_classes, char_dict, token_dict, sentiment_dict, tokenizer, shuffle=True, batch_size=128, k_folds=10, fold_idx=0) -> None:\n",
    "        super().__init__()\n",
    "        self.fold_idx = fold_idx\n",
    "        kf = KFold(k_folds, shuffle=True, random_state=42)\n",
    "        self.k_folds = list(kf.split(np.arange(len(y))))\n",
    "        self.k_folds = [(np.array(f[0], dtype=np.longlong), np.array(f[1], dtype=np.longlong)) for f in self.k_folds]\n",
    "        \n",
    "        if batch_size > len(y)//k_folds:\n",
    "            batch_size = len(y)//k_folds\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        if len(y) % batch_size*k_folds != 0:\n",
    "            \n",
    "            self.shortage = ((len(y) // (batch_size*k_folds))+1)*(batch_size*k_folds) - len(y)\n",
    "            print(f\"!!!! This amount of data added: {self.shortage}, change batch size or k-fold to reduce it!\")\n",
    "            empty_labels = [i%2 for i in range(self.shortage)]\n",
    "            empty_strings = [id_class[l] for l in empty_labels]\n",
    "            y = np.concatenate([y, empty_labels])\n",
    "            X = np.concatenate([X, empty_strings])\n",
    "        \n",
    "        y = torch.from_numpy(y)\n",
    "        self.shuffle = shuffle\n",
    "        self.y = torch.nn.functional.one_hot(y, num_classes=num_classes).float()\n",
    "        self.X = X\n",
    "        self.char_dict = char_dict\n",
    "        self.char_Set = set(char_dict.keys())\n",
    "        self.vocab_size = len(self.char_dict)\n",
    "        self.tokenizer = tokenizer\n",
    "        # self.token_dict = token_dict\n",
    "        self.sentiment_dict = sentiment_dict\n",
    "        self.max_token_count = 0\n",
    "        \n",
    "        self.all_data = []\n",
    "        self.token_lengths = []\n",
    "        self.token_embeddign_ids = []\n",
    "        \n",
    "        self.sum_a = 0\n",
    "        \n",
    "        for doc in tqdm(self.X):\n",
    "            # tokens = self.tokenizer(''.join(c for c in doc if c in self.char_Set))\n",
    "            # tokens = [t.text for t in tokens]\n",
    "            tokens = self.tokenizer(doc)\n",
    "            # tokens = [t for t in tokens if t in token_dict]\n",
    "            if len(tokens) == 0:\n",
    "                tokens = ['empty']\n",
    "                            \n",
    "            token_lengths = [len(t) for t in tokens]\n",
    "            tokens.append('\\x01')\n",
    "            token_embs = [token_dict[t] if t in token_dict else torch.zeros((64, ), dtype=torch.float32) for t in tokens]\n",
    "            token_embs = torch.from_numpy(np.array(token_embs, dtype=np.float32))\n",
    "            doc = ' '.join(tokens)\n",
    "            \n",
    "            token_lengths.append(len(tokens[-1])-1)\n",
    "            token_lengths = torch.from_numpy(np.array(token_lengths, dtype=np.longlong))+1\n",
    "            # token_embs = [self.token_dict[t] if t in self.token_dict else torch.zeros((64, ), dtype=torch.float32) for t in tokens]\n",
    "            token_sentiments = [self.sentiment_dict[t] if t in self.sentiment_dict else (0.0, 0.0) for t in tokens]\n",
    "            # token_embs = torch.from_numpy(np.array(token_embs, dtype=np.float32))\n",
    "            token_sentiments = torch.from_numpy(np.array(token_sentiments, dtype=np.float32))\n",
    "            doc = ' '.join(tokens)\n",
    "            characters = torch.from_numpy(np.array([self.char_dict[t] for t in doc], dtype=np.longlong))\n",
    "            token_positions = torch.arange(len(token_lengths), dtype=torch.long)\n",
    "            token_indices = torch.repeat_interleave(token_positions, token_lengths)\n",
    "            num_tokens = len(token_lengths)\n",
    "            if num_tokens > self.max_token_count:\n",
    "                self.max_token_count = num_tokens\n",
    "            g_data = Data(x=characters,\n",
    "                            token_positions=token_positions,\n",
    "                            character_length = len(characters),\n",
    "                            num_tokens = num_tokens,\n",
    "                            token_indices=token_indices,\n",
    "                            token_lengths=token_lengths,\n",
    "                            token_embeddings=token_embs,\n",
    "                            token_sentiments=token_sentiments\n",
    "                            )\n",
    "            self.all_data.append(g_data)\n",
    "            \n",
    "        self.all_data = Batch.from_data_list(self.all_data)\n",
    "        self.update_split(self.fold_idx)\n",
    "        \n",
    "    def update_split(self, fold_idx):\n",
    "        self.active_fold = self.k_folds[fold_idx]\n",
    "        \n",
    "        self.train_y = self.y[self.active_fold[0]]\n",
    "        self.test_y = self.y[self.active_fold[1]]\n",
    "        \n",
    "        self.all_train_data = self.all_data[self.active_fold[0]]\n",
    "        self.all_test_data = self.all_data[self.active_fold[1]]\n",
    "        self.train()\n",
    "        \n",
    "    def train(self):\n",
    "        self.update_sections(self.train_y, self.all_train_data)\n",
    "        self.is_train=True\n",
    "        \n",
    "    def eval(self):\n",
    "        self.update_sections(self.test_y, self.all_test_data)\n",
    "        self.is_train=False\n",
    "        \n",
    "    def update_sections(self, y, all_data):\n",
    "        \n",
    "        self.num_sections = len(y) // batch_size\n",
    "        self.x_lengths = np.array([all_data[i].character_length[0] for i in range(len(all_data))])\n",
    "        self.x_len_args = np.argsort(self.x_lengths)[::-1]\n",
    "        \n",
    "        self.section_ranges = np.linspace(0, len(self.x_len_args), self.num_sections+1)\n",
    "        self.section_ranges = [(int(self.section_ranges[i-1]), int(self.section_ranges[i])) for i in range(1, len(self.section_ranges))]\n",
    "\n",
    "        self.position_j = 0\n",
    "        self.section_i = 0\n",
    "        self.epoch = 0\n",
    "        self.each_section_i = np.zeros((self.num_sections, ), dtype=int)\n",
    "        \n",
    "        self.sections, self.section_size = self.split_into_k_groups(self.x_len_args, self.x_lengths, self.num_sections)\n",
    "        return self.sections, self.section_size, self.x_len_args, self.x_lengths, self.num_sections\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        index = self.get_section_index(index+1)\n",
    "        if self.is_train:\n",
    "            return self.all_train_data[index], self.train_y[index]\n",
    "        else:\n",
    "            return self.all_test_data[index], self.test_y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def get_section_index(self, index):\n",
    "        position_j = index % self.section_size\n",
    "        section_i = (index //self.section_size) % self.num_sections\n",
    "        target_index = self.sections[section_i, position_j]\n",
    "        \n",
    "        # self.position_j = (self.position_j + 1) % self.section_size\n",
    "        # if self.position_j == 0:\n",
    "        #     self.section_i = (self.section_i + 1) % self.num_sections\n",
    "        #     if self.shuffle and self.section_i == 0:\n",
    "        #         self.sections, self.section_size = self.split_into_k_groups(self.x_len_args, self.x_lengths, self.num_sections)\n",
    "        return target_index \n",
    "    \n",
    "    # def get_section_index(self):\n",
    "    #     target_index = self.sections[self.section_i, self.position_j]\n",
    "        \n",
    "    #     self.position_j = (self.position_j + 1) % self.section_size\n",
    "    #     if self.position_j == 0:\n",
    "    #         self.section_i = (self.section_i + 1) % self.num_sections\n",
    "    #         if self.shuffle and self.section_i == 0:\n",
    "    #             self.sections, self.section_size = self.split_into_k_groups(self.x_len_args, self.x_lengths, self.num_sections)\n",
    "    #     return target_index    \n",
    "\n",
    "    def reset_params(self):\n",
    "        self.section_i = 0\n",
    "        self.position_j = 0\n",
    "        self.each_section_i = np.zeros((self.num_sections, ), dtype=int)\n",
    "        \n",
    "    def split_into_k_groups(self, len_sorted_args, lengths:np.array, k):\n",
    "        if self.shuffle and self.epoch > 0:\n",
    "            randomize_sections = np.concatenate([np.random.choice(np.arange(r[0], r[1]), size=r[1]-r[0], replace=False) for r in self.section_ranges])\n",
    "            len_sorted_args = len_sorted_args[randomize_sections]\n",
    "        \n",
    "        nums = lengths[len_sorted_args]\n",
    "        groups_size = len(len_sorted_args) // k\n",
    "        \n",
    "        groups = [[] for _ in range(k)]\n",
    "        group_sums = np.zeros(k, dtype=int)\n",
    "        group_sizes = np.zeros(k, dtype=int)\n",
    "        \n",
    "        for i, num in enumerate(nums):\n",
    "            candidate_indices = np.where(group_sizes<groups_size)[0]\n",
    "            min_group_idx = candidate_indices[np.argmin(group_sums[candidate_indices])]\n",
    "            groups[min_group_idx].append(len_sorted_args[i])\n",
    "            group_sums[min_group_idx] += num\n",
    "            group_sizes[min_group_idx] += 1\n",
    "        self.epoch += 1\n",
    "        \n",
    "        groups = np.array(groups)\n",
    "        group_sums_argsort = np.argsort(group_sums)[::-1]\n",
    "        groups = groups[group_sums_argsort]\n",
    "                \n",
    "        return np.array(groups), groups_size\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTestDatasetWrapper(Dataset):\n",
    "    \n",
    "    def __init__(self, target_dataset, is_train=True) -> None:\n",
    "        super().__init__()\n",
    "        self.dataset = target_dataset\n",
    "        self.is_train = is_train\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.is_train:\n",
    "            if not self.dataset.is_train:\n",
    "                self.dataset.train()\n",
    "            return self.dataset[index]\n",
    "        else:\n",
    "            if self.dataset.is_train:\n",
    "                self.dataset.eval()\n",
    "            return self.dataset[index]\n",
    "    \n",
    "    def set_active_fold(self, fold_idx):\n",
    "        self.dataset.update_split(fold_idx)\n",
    "    \n",
    "    def reset_params(self):\n",
    "        self.dataset.reset_params()\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.is_train:\n",
    "            return len(self.dataset.train_y)\n",
    "        else:\n",
    "            return len(self.dataset.test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "class CharacterandTokenLevelDataLoader(DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size: int = 1,\n",
    "        shuffle: bool = False,\n",
    "        follow_batch: List[str] | None = None,\n",
    "        exclude_keys: List[str] | None = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(CharacterandTokenLevelDataLoader, self).__init__(\n",
    "            dataset, batch_size, shuffle, follow_batch, exclude_keys, **kwargs\n",
    "        )\n",
    "\n",
    "    def __iter__(self):\n",
    "        base_iterator = super(CharacterandTokenLevelDataLoader, self).__iter__()\n",
    "        for batch in base_iterator:\n",
    "            cumsum_vals = torch.cumsum(batch[0].num_tokens, dim=0).roll(1)\n",
    "            cumsum_vals[0] = 0\n",
    "            additions = torch.repeat_interleave(cumsum_vals, batch[0].character_length)\n",
    "            batch[0].cumulative_token_indices = batch[0].token_indices + additions\n",
    "            yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def double_tokenizer(doc):\n",
    "#     tokens = t_tokenizer.tokenize(doc)\n",
    "#     tokens = nlp(' '.join(tokens))\n",
    "#     tokens = [t.text for t in tokens]\n",
    "#     return tokens\n",
    "\n",
    "# def nlp_tokenizer(doc):\n",
    "#     tokens = nlp(doc)\n",
    "#     tokens = [t.text for t in tokens]\n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(df.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'have' in token_vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t for t in tokens if t not in token_vocab_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/2000 [00:00<01:37, 20.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:20<00:00, 24.99it/s]\n"
     ]
    }
   ],
   "source": [
    "main_dataset = KFoldCharacterandTokenLevelDataset(df.text.values, df.label.values, len(class_id), vocab_dict, token_vocab_dict, polarity_vocab_dict, tokenizer.tokenize, batch_size=batch_size, shuffle=False)\n",
    "batch_size = main_dataset.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainTestDatasetWrapper(main_dataset, True)\n",
    "test_dataset = TrainTestDatasetWrapper(main_dataset, False)\n",
    "train_dataset.set_active_fold(0)\n",
    "test_dataset.set_active_fold(0)\n",
    "# max_token_count = max(train_dataset.max_token_count, test_dataset.max_token_count)\n",
    "train_dataloader = CharacterandTokenLevelDataLoader(train_dataset, batch_size=batch_size, drop_last=False, shuffle=False)\n",
    "test_dataloader = CharacterandTokenLevelDataLoader(test_dataset, batch_size=batch_size, drop_last=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Data(x=[9149], token_positions=[1581], character_length=[1], num_tokens=[1], token_indices=[9149], token_lengths=[1581], token_embeddings=[1581, 64], token_sentiments=[1581, 2]),\n",
       " tensor([0., 1.]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3080\n",
      "2804\n"
     ]
    }
   ],
   "source": [
    "train_lengths = np.array([train_dataset[i][0].num_tokens for i in range(len(train_dataset))])\n",
    "test_lengths = np.array([test_dataset[i][0].num_tokens for i in range(len(test_dataset))])\n",
    "print(np.max(train_lengths))\n",
    "print(np.max(test_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Data(x=[8786], token_positions=[1576], character_length=[1], num_tokens=[1], token_indices=[8786], token_lengths=[1576], token_embeddings=[1576, 64], token_sentiments=[1576, 2]),\n",
       " tensor([0., 1.]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label                                                    1\n",
       "text     no , i did not read the novel by thomas hardy ...\n",
       "Name: 131, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[np.argwhere(['hardy' in c and 'obscure' in c for c in  df.text.values]).squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ▁the ▁happy ▁bastard ' s ▁quick ▁movie ▁review ▁wild ▁wild ▁west ▁a ▁better ▁name ▁for ▁this ▁movie ▁might ' ve ▁been ▁\" ▁wild ▁wild ▁waste ▁\" ▁. ▁war ner ▁bros ▁. ▁, ▁in ▁an ▁attempt ▁to ▁get ▁their ▁own ▁men ▁in ▁black ▁style ▁of ▁movie ▁, ▁had ▁managed ▁to ▁lasso ▁in ▁some ▁big ▁names ▁( ▁actor ▁will ▁smith ▁and ▁director ▁bar ry ▁son n enfeld ▁, ▁the ▁duo ▁behind ▁mi b ' s ▁success ▁) ▁in ▁order ▁to ▁get ▁their ▁own ▁fourth ▁of ▁july ▁blockbuster ▁, ▁a ▁contemporary ▁big - screen ▁update ▁to ▁the ▁classic ▁western / sci - fi ▁series ▁the ▁wild ▁wild ▁west ▁starring ▁robert ▁con rad ▁. ▁but ▁somehow ▁, ▁they ▁ran ▁into ▁a ▁problem ▁along ▁the ▁way ▁. ▁they ▁were ▁so ▁busy ▁trying ▁to ▁fill ▁specific ▁roles ▁that ▁they ▁forgot ▁one ▁in ▁general ▁that ▁would ' ve ▁made ▁all ▁the ▁difference - ▁a ▁story ▁writer ▁. ▁wild ▁wild ▁west ' s ▁story ▁and ▁script ▁was ▁compiled ▁by ▁six ▁different ▁people ▁, ▁rather ▁than ▁just ▁the ▁one ▁who ▁put ▁the ▁brilliant ▁touches ▁on ▁men ▁in ▁black ▁, ▁ed ▁solo mon ▁. ▁if ▁war ner ▁bros ▁. ▁had ▁gotten ▁him ▁, ▁the ▁movie ▁would ' ve ▁possibly ▁been ▁five ▁times ▁better ▁than ▁what ▁it ▁is ▁. ▁heck ▁, ▁ten ▁times ▁. ▁as ▁is ▁, ▁however ▁, ▁the ▁story ▁and ▁screenplay ▁is ▁a ▁mess ▁, ▁filled ▁with ▁dead ▁laughs ▁, ▁enough ▁racist ▁and ▁sex ▁jokes ▁to ▁make ▁even ▁will ▁cringe ▁as ▁he ' s ▁performing ▁them ▁, ▁and ▁a ▁complete ▁lack ▁of ▁chemistry ▁that ▁made ▁the ▁tv ▁show ▁work ▁so ▁well ▁. ▁here ' s ▁the ▁story ▁: ▁us ▁army ▁member ▁james ▁west ▁( ▁will ▁smith ▁) ▁teams ▁up ▁with ▁creative ▁genius ▁art emu s ▁go rd on ▁( ▁kevin ▁k line ▁) ▁to ▁take ▁on ▁a ▁ruthless ▁villain ▁by ▁the ▁name ▁of ▁a r liss ▁love less ▁( ▁ken ne th ▁bra nagh ▁) ▁, ▁whose ▁lower ▁half ▁was ▁blown ▁off ▁during ▁the ▁civil ▁war ▁, ▁leaving ▁him ▁to ▁roll ▁around ▁in ▁a ▁steam - controlled ▁wheelchair ▁. ▁they ▁catch ▁onto ▁a ▁plot ▁of ▁his ▁involving ▁a ▁super weapon ▁that ▁can ▁basically ▁be ▁considered ▁an ▁80 - foot ▁tarantula ▁( ▁although ▁it ▁looks ▁bigger ▁than ▁that ▁) ▁, ▁hell bent ▁on ▁destroying ▁anything ▁in ▁its ▁path ▁. ▁as ▁i ▁said ▁, ▁the ▁way ▁the ▁script ▁unfolds ▁is ▁a ▁complete ▁mess ▁. ▁but ▁the ▁acting ▁doesn ' t ▁help ▁either ▁. ▁kevin ▁k line ▁is ▁miserable ▁as ▁go rd on ▁, ▁failing ▁to ▁display ▁even ▁a ▁smidge on ▁of ▁care ▁as ▁he ▁did ▁in ▁the ▁1985 ▁western ▁silverado ▁. ▁smith ▁seems ▁to ▁be ▁having ▁a ▁better ▁time ▁as ▁west ▁, ▁although ▁he ' s ▁not ▁nearly ▁as ▁charming ▁as ▁he ▁was ▁in ▁mi b ▁. ▁bra nagh ▁goes ▁excessively ▁over ▁the ▁top ▁as ▁the ▁villain ▁, ▁perhaps ▁to ▁the ▁point ▁where ▁we ▁can ' t ▁even ▁stand ▁to ▁look ▁at ▁him ▁or ▁his ▁strange ▁beard ▁. ▁and ▁sal ma ▁hay ek ▁is ▁along ▁for ▁the ▁ride ▁to ▁search ▁for ▁her ▁missing ▁father ▁, ▁but ▁mostly ▁she ▁exists ▁just ▁for ▁sexual ▁attention ▁. ▁her ▁acting ▁is ▁barely ▁passable ▁, ▁but ▁what ▁a ▁\" ▁breath ▁of ▁fresh ▁as s ▁\" ▁. ▁director ▁bar ry ▁son n enfeld ▁isn ' t ▁of ▁great ▁help ▁either ▁. ▁even ▁though ▁he ▁shows ▁some ▁good ▁creativity ▁at ▁some ▁points ▁, ▁his ▁overall ▁urgency ▁for ▁directing ▁is ▁lost ▁. ▁it ' s ▁as ▁if ▁he ▁feels ▁he ' s ▁directing ▁a ▁tv ▁movie ▁, ▁a ▁big ▁no - no ▁when ▁you ' re ▁helm ing ▁what ' s ▁supposed ▁to ▁be ▁a ▁big - screen ▁hit ▁. ▁last ▁but ▁not ▁least ▁, ▁there ▁are ▁huge ▁gaps ▁of ▁logic ▁that ▁are ▁just ▁plain ▁unacceptable ▁. ▁at ▁one ▁point ▁, ▁smith ▁defies ▁gravity ▁when ▁first ▁boarding ▁k line ' s ▁train ▁. ▁he ▁jumps ▁on ▁the ▁back ▁, ▁gets ▁launched ▁straight ▁up ▁in ▁the ▁air ▁, ▁and ▁somehow ▁manages ▁to ▁land ▁three ▁cars ▁ahead ▁on ▁the ▁train - ▁while ▁it ' s ▁still ▁in ▁motion ▁! ▁also ▁, ▁he ▁seems ▁to ▁be ▁strangely ▁comfortable ▁talking ▁about ▁racism ▁in ▁front ▁of ▁a ▁lynch ▁mob ▁, ▁particularly ▁considering ▁the ▁fact ▁that ▁his ▁family ▁was ▁killed ▁by ▁the ▁likes ▁of ▁such ▁folks ▁. ▁last ▁but ▁not ▁least ▁, ▁why ▁is ▁he ▁still ▁making ▁kissing ▁faces ▁when ▁he ▁knows ▁he ' s ▁not ▁kissing ▁a ▁woman ▁as ▁he ' s ▁looking ▁through ▁a ▁peephole ▁at ▁the ▁enemy ▁? ▁! ▁the ▁only ▁saving ▁grace ▁for ▁wild ▁wild ▁west ▁come ▁mostly ▁in ▁the ▁form ▁of ▁special ▁effects ▁. ▁the ▁huge ▁mechanical ▁spider ▁is ▁a ▁technical ▁marvel ▁, ▁very ▁authentic ▁looking ▁and ▁considerable ▁to ▁the ▁creativity ▁of ▁the ▁show ▁. ▁there ' s ▁also ▁a ▁good ▁sequence ▁involving ▁metal ▁magnet ▁neck brace s ▁and ▁spinning ▁saw ▁blades ▁, ▁but ▁, ▁again ▁, ▁a ▁logic ▁question ▁comes ▁into ▁play ▁regarding ▁their ▁polarity ▁. ▁i ▁would ▁say ▁sit ▁back ▁and ▁have ▁a ▁good ▁time ▁with ▁wild ▁wild ▁west ▁, ▁but ▁it ▁really ▁isn ' t ▁possible ▁. ▁if ▁the ▁racist ▁and ▁sex ▁jokes ▁don ' t ▁bother ▁you ▁, ▁the ▁performances ▁will ▁. ▁if ▁the ▁performances ▁don ' t ▁bother ▁you ▁, ▁the ▁story ▁will ▁. ▁if ▁the ▁story ▁doesn ' t ▁bother ▁you ▁, ▁the ▁racist ▁and ▁sex ▁jokes ▁will ▁. ▁there ' s ▁no ▁end ▁to ▁the ▁vicious ▁circle ▁. ▁go ▁rent ▁men ▁in ▁black ▁instead ▁and ▁pretend ▁smith ▁and ▁tommy ▁lee ▁jones ▁are ▁wearing ▁cowboy ▁hats ▁. ▁you ' ll ▁have ▁a ▁better ▁time ▁. ▁if ▁you ▁do ▁go ▁, ▁keep ▁an ▁eye ▁out ▁for ▁robert ▁con rad ▁in ▁a ▁role ▁as ▁president ▁grant ▁. ▁i ▁bet ▁he ▁wishes ▁he ▁were ▁somewhere ▁else ▁. ▁. ▁. \u0001\n",
      "0 ▁fact ▁that ▁charles ▁bron son ▁represents ▁one ▁of ▁the ▁most ▁important ▁movie ▁icons ▁of ▁the ▁1980 s ▁represents ▁one ▁of ▁the ▁biggest ▁and ▁almost ▁tragic ▁ironies ▁of ▁that ▁decade ▁. ▁tragedy ▁lies ▁in ▁the ▁fact ▁that ▁the ▁icon ▁status ▁was ▁earned ▁less ▁by ▁quality ▁of ▁his ▁work ▁in ▁movies ▁, ▁but ▁the ▁quantity ▁. ▁most ▁of ▁those ▁movies ▁were ▁produced ▁by ▁cannon ▁group ▁, ▁company ▁led ▁by ▁israel i ▁producers ▁men ahem ▁go lan ▁and ▁yo ram ▁glob us ▁. ▁those ▁two ▁men ▁probably ▁thought ▁that ▁they ▁could ▁be ▁the ▁next ▁roger ▁cor man ▁, ▁b - ▁movie ▁mentors ▁of ▁future ▁hollywood ▁legends ▁. ▁unfortunately ▁, ▁that ▁didn ' t ▁happened ▁, ▁and ▁when ▁cannon ▁finally ▁went ▁bankrupt ▁at ▁the ▁end ▁of ▁the ▁decade ▁, ▁behind ▁it ▁stood ▁the ▁huge ▁pile ▁of ▁cinematic ▁garbage ▁, ▁that ▁would ▁require ▁at ▁least ▁few ▁centuries ▁before ▁it ▁reaches ▁the ▁camp ▁appeal ▁. ▁sadly ▁for ▁bron son ▁, ▁that ▁garbage ▁also ▁contained ▁numerous ▁movies ▁in ▁which ▁that ▁capable ▁character ▁actor ▁and ▁action ▁hero ▁of ▁the ▁1970 s ▁tried ▁to ▁raise ▁their ▁worth ▁simply ▁by ▁being ▁the ▁main ▁lead ▁, ▁and ▁lowering ▁his ▁own ▁reputation ▁in ▁process ▁. ▁on ▁the ▁other ▁hand ▁, ▁bron son ▁could ▁take ▁comfort ▁in ▁a ▁fact ▁that ▁those ▁movies ▁were ▁extremely ▁popular ▁, ▁especially ▁among ▁the ▁audience ▁3 ▁or ▁4 ▁times ▁younger ▁than ▁bron son ▁himself ▁. ▁one ▁of ▁such ▁movies ▁that ▁seriously ▁marred ▁bron son ' s ▁reputation ▁is ▁death ▁wish ▁3 ▁, ▁third ▁sequel ▁in ▁the ▁series ▁which ▁began ▁with ▁death ▁wish ▁in ▁1974 ▁. ▁in ▁the ▁original ▁movie ▁, ▁bron son ▁played ▁paul ▁ker sey ▁, ▁mild - mannered ▁new ▁york ▁architect ▁who ▁turns ▁into ▁deadly ▁street ▁vigilante ▁after ▁his ▁family ▁fell ▁victim ▁to ▁urban ▁violence ▁. ▁that ▁movie ▁was ▁far ▁from ▁masterpiece ▁; ▁yet ▁, ▁in ▁it ▁the ▁director ▁michael ▁winner ▁was ▁skillfully ▁offering ▁the ▁cinematic ▁remedy ▁for ▁very ▁real ▁disease ▁of ▁growing ▁crime ▁rates ▁of ▁the ▁time ▁( ▁on ▁the ▁same ▁lines ▁like ▁siege l ▁in ▁dirty ▁harry ▁) ▁. ▁unfortunately ▁, ▁six ▁years ▁later ▁cannon ▁group ▁got ▁rights ▁to ▁the ▁character ▁of ▁paul ▁ker sey ▁and ▁began ▁destroying ▁it ▁by ▁pumping ▁out ▁sequels ▁; ▁even ▁the ▁presence ▁of ▁its ▁original ▁director ▁didn ' t ▁stop ▁the ▁rapid ▁decline ▁of ▁the ▁quality ▁. ▁death ▁wish ▁3 ▁begins ▁when ▁ker sey ▁comes ▁to ▁visit ▁an ▁old ▁friend ▁, ▁living ▁in ▁the ▁urban ▁wasteland ▁of ▁east ▁new ▁york ▁, ▁populated ▁by ▁young ▁criminals ▁and ▁people ▁too ▁old ▁or ▁too ▁poor ▁to ▁move ▁out ▁. ▁before ▁the ▁reunion ▁, ▁ker sey ' s ▁friend ▁falls ▁victim ▁to ▁the ▁street ▁gang ▁led ▁by ▁evil ▁fra ker ▁( ▁played ▁by ▁g avan ▁o ' her li hy ▁, ▁probably ▁the ▁only ▁noteworthy ▁role ▁in ▁the ▁film ▁) ▁. ▁ker sey ▁decides ▁to ▁avenge ▁his ▁death ▁and ▁slowly ▁prepares ▁for ▁his ▁crusade ▁, ▁while ▁the ▁police ▁inspector ▁sh rik er ▁( ▁ed ▁la uter ▁) ▁, ▁ants ▁to ▁use ▁him ▁as ▁a ▁secret ▁weapon ▁in ▁his ▁losing ▁war ▁against ▁the ▁urban ▁crime ▁. ▁bron son ▁, ▁the ▁main ▁asset ▁in ▁this ▁movie ▁, ▁plays ▁the ▁character ▁who ▁is ▁nothing ▁more ▁than ▁an ▁efficient ▁killing ▁machine ▁. ▁although ▁bron son ' s ▁charisma ▁does ▁help ▁in ▁overcoming ▁some ▁imp laus i bilities ▁( ▁single ▁man ▁in ▁his ▁60 s ▁and ▁armed ▁with ▁a ▁single ▁pistol ▁manages ▁to ▁wipe ▁out ▁dozens ▁of ▁opponents ▁with ▁superior ▁firepower ▁) ▁, ▁the ▁lack ▁of ▁emotions ▁or ▁bron son ' s ▁own ▁commitment ▁could ▁be ▁seen ▁in ▁a ▁very ▁few ▁lines ▁spoken ▁in ▁a ▁film ▁. ▁the ▁movie ▁authors ▁were ▁somewhat ▁aware ▁of ▁that ▁emotional ▁shallow ness ▁, ▁so ▁they ▁added ▁romantic ▁interest ▁for ▁their ▁hero ▁- ▁public ▁defender ▁played ▁by ▁deb orah ▁raff in ▁and ▁conveniently ▁terminated ▁in ▁order ▁to ▁give ▁some ▁more ▁motives ▁for ▁ker sey ' s ▁crusade ▁. ▁on ▁the ▁other ▁hand ▁, ▁emotions ▁are ▁much ▁better ▁played ▁by ▁confronting ▁law - abiding ▁, ▁yet ▁ethnically ▁stereotyped ▁citizens ▁with ▁their ▁daily ▁nemesis ▁of ▁street ▁punk s ▁- ▁ruthless ▁enough ▁to ▁exercise ▁their ▁reign ▁of ▁terror ▁on ▁the ▁entire ▁city ▁blocks ▁, ▁and ▁stupid ▁enough ▁to ▁be ▁killed ▁in ▁droves ▁by ▁ker sey ▁. ▁unfortunately ▁, ▁michael ▁winner ▁doesn ' t ▁know ▁how ▁to ▁work ▁out ▁the ▁plot ▁, ▁and ▁after ▁torturing ▁the ▁viewers ▁with ▁mostly ▁uninteresting ▁characters ▁and ▁cliched ▁and ▁formulaic ▁situations ▁, ▁ends ▁this ▁movie ▁with ▁a ▁bang ▁. ▁the ▁big ▁showdown ▁at ▁the ▁end ▁- ▁that ▁turns ▁east ▁new ▁york ▁into ▁the ▁s araj evo - like ▁battle ▁zone ▁- ▁is ▁probably ▁the ▁worst ▁part ▁of ▁the ▁movie ▁, ▁because ▁of ▁the ▁poor ▁editing ▁and ▁the ▁cheap ▁sets ▁and ▁props ▁that ▁give ▁away ▁the ▁low ▁budget ▁. ▁in ▁short ▁, ▁this ▁movie ▁could ▁be ▁recommended ▁only ▁to ▁the ▁most ▁fanatical ▁charles ▁bron son ▁fans ▁or ▁for ▁the ▁people ▁who ▁are ▁already ▁desperate ▁for ▁1980 s ▁nostalgia ▁. ▁( ▁special ▁note ▁for ▁trek kies ▁: ▁marina ▁sir tis ▁, ▁the ▁actress ▁who ▁played ▁counsellor ▁de anna ▁tro i ▁in ▁star ▁trek ▁: ▁the ▁next ▁generation ▁could ▁be ▁spotted ▁in ▁a ▁small ▁role ▁of ▁port or ican ▁wife ▁) ▁. \u0001\n",
      "0 ▁forget ▁get ▁car ter ▁. ▁instead ▁. ▁. ▁. ▁get ▁me ▁a ▁cup ▁of ▁coffee ▁. ▁what ▁the ▁hell ▁has ▁happened ▁to ▁all ▁good ▁american ▁action ▁movies ▁? ▁did ▁i ▁unknowingly ▁miss ▁a ▁meeting ▁somewhere ▁? ▁when ▁did ▁all ▁of ▁the ▁bad - as s ▁, ▁kicking ▁but t ▁and ▁taking ▁names ▁, ▁gun - to ting ▁, ▁crazed ▁, ▁vengeful ▁characters ▁of ▁the ▁1980 ' s ▁- - ▁from ▁such ▁films ▁as ▁commando ▁, ▁cobra ▁, ▁predator ▁, ▁raw ▁deal ▁, ▁first ▁blood ▁- - ▁suddenly ▁turn ▁into ▁innocent ▁, ▁compassionate ▁, ▁sensitive ▁, ▁teary - eyed ▁knuckle heads ▁. ▁the ▁only ▁place ▁to ▁turn ▁these ▁days ▁for ▁an ▁honest ▁action ▁film ▁is ▁towards ▁the ▁east ▁- - ▁and ▁i ▁don ' t ▁mean ▁new ▁york ▁city ▁. ▁get ▁car ter ▁- - ▁the ▁latest ▁masterpiece ▁from ▁uber - the s pian ▁sylv ester ▁stall one ▁- - ▁is ▁a ▁prime ▁example ▁of ▁large ▁and ▁in ▁charge ▁80 ' s ▁action ▁stars ▁trying ▁to ▁fit ▁back ▁into ▁action ▁roles ▁they ▁have ▁long ▁since ▁outgrown ▁. ▁stall one ▁seems ▁like ▁that ▁one ▁uncle ▁you ▁have ▁who ▁tries ▁to ▁be ▁cool ▁with ▁his ▁members ▁only ▁jacket ▁and ▁iz od ▁polo ▁shirt ▁with ▁the ▁collar ▁popped ▁up ▁. ▁a ▁few ▁years ▁ago ▁, ▁stall one ▁made ▁a ▁movie ▁that ▁gave ▁him ▁the ▁opportunity ▁to ▁gracefully ▁exit ▁the ▁roles ▁that ▁type cast ▁him ▁as ▁an ▁action ▁monkey ▁. ▁that ▁role ▁was ▁sheriff ▁f reddy ▁he flin ▁in ▁cop land ▁- - ▁a ▁strange ▁film ▁about ▁redemption ▁within ▁a ▁broken ▁soul ▁. ▁stall one ▁actually ▁gave ▁an ▁amazing ▁performance ▁and ▁it ▁seemed ▁he ▁had ▁shaken ▁off ▁the ▁past ▁. ▁too ▁bad ▁get ▁car ter ▁returns ▁stall one ▁to ▁action ▁, ▁but ▁with ▁the ▁shiny ▁paint ▁rusted ▁off ▁on ▁the ▁edges ▁. ▁get ▁car ter ▁is ▁a ▁simple ▁story ▁. ▁stall one ▁plays ▁frank ▁car ter ▁, ▁a ▁vegas ▁bruise r ▁for ▁a ▁loan ▁shark ▁( ▁played ▁with ▁amazing ▁gusto ▁by ▁the ▁uncredited ▁voice ▁of ▁actor ▁to m ▁size more ▁) ▁. ▁when ▁frank ' s ▁brother ▁gets ▁himself ▁killed ▁in ▁a ▁drunk ▁driving ▁accident ▁, ▁frank ▁, ▁feeling ▁all ▁guilty ▁and ▁mushy ▁inside ▁, ▁thinks ▁foul ▁play ▁is ▁involved ▁and ▁travels ▁to ▁seattle ▁to ▁set ▁right ▁all ▁the ▁wrong s ▁with ▁the ▁patented ▁\" ▁car ter ' s ▁way ▁\" ▁. ▁he ▁talks ▁tough ▁with ▁his ▁brother ' s ▁wife ▁, ▁lends ▁a ▁helping ▁had ▁to ▁his ▁brother ' s ▁daughter ▁do reen ▁( ▁rachel ▁ leigh ▁cook ▁) ▁, ▁and ▁walks ▁around ▁seattle ▁in ▁the ▁pouring ▁rain ▁dressed ▁like ▁a ▁lost ▁member ▁of ▁the ▁rat ▁pack ▁with ▁a ▁really ▁bad ▁goatee ▁. ▁car ter ▁finds ▁out ▁that ▁his ▁brother ▁was ▁involved ▁in ▁some ▁bad ▁stuff ▁with ▁a ▁slimy ▁porn ▁king ▁played ▁by ▁ultra - cool ▁, ▁mc queen - esque ▁mickey ▁rou rke ▁, ▁a ▁multi - millionaire ▁computer ▁geek ▁( ▁a lan ▁c umming ▁) ▁, ▁and ▁a ▁strange ▁foreign ▁guy ▁( ▁michael ▁ca ine ▁) ▁who ▁speaks ▁in ▁riddles ▁and ▁talks ▁tough ▁. ▁car ter ▁stalks ▁all ▁of ▁them ▁while ▁trying ▁to ▁figure ▁out ▁who ▁did ▁in ▁his ▁brother ▁and ▁how ▁to ▁extract ▁proper ▁revenge ▁on ▁the ▁responsible ▁parties ▁. ▁what ▁a ▁minute ▁! ▁this ▁sounds ▁just ▁like ▁another ▁film ▁i ▁saw ▁last ▁year ▁, ▁the ▁lime y ▁. ▁better ▁not ▁tell ▁ter ence ▁stamp ▁about ▁stall one ▁ripping ▁him ▁off ▁. ▁actually ▁, ▁get ▁car ter ▁is ▁a ▁remake ▁of ▁the ▁1971 ▁british ▁production ▁of ▁the ▁same ▁name ▁, ▁starring ▁ca ine ▁in ▁the ▁title ▁role ▁( ▁and ▁what ▁with ▁his ▁cameo ▁here ▁, ▁the ▁cleverness ▁is ▁astonishing ▁) ▁. ▁while ▁stall one ▁still ▁carries ▁his ▁own ▁weight ▁here ▁, ▁the ▁movie ▁lacks ▁what ▁the ▁original ▁did ▁as ▁well ▁: ▁purpose ▁. ▁throughout ▁the ▁film ▁, ▁stall one ▁looks ▁like ▁an ▁old ▁guy ▁trying ▁to ▁act ▁tough ▁, ▁while ▁nobody ▁is ▁taking ▁him ▁seriously ▁. ▁his ▁one - liners ▁fall ▁flat ▁, ▁and ▁he ▁seems ▁tired ▁and ▁uncertain ▁of ▁all ▁the ▁actions ▁, ▁mental ▁and ▁physical ▁, ▁required ▁of ▁his ▁character ▁. ▁there ▁is ▁even ▁a ▁strange ▁homo er otic ism ▁between ▁rou rke ▁and ▁stall one ▁that ▁lends ▁a ▁bizarre ▁tone ▁to ▁their ▁numerous ▁conversations ▁- - ▁in ▁both ▁fists ▁and ▁words ▁. ▁the ▁biggest ▁surprise ▁in ▁get ▁car ter ▁is ▁that ▁the ▁best ▁job ▁done ▁in ▁the ▁film ▁is ▁by ▁the ▁versatile ▁mickey ▁rou rke ▁. ▁an ▁amazing ▁method ▁actor ▁in ▁the ▁eighties ▁who ▁fell ▁into ▁drugs ▁, ▁spousal ▁abuse ▁, ▁a ▁boxing ▁career ▁, ▁and ▁an ▁intolerable ▁attitude ▁towards ▁not ▁getting ▁his ▁way ▁, ▁rou rke ▁still ▁brings ▁a ▁dangerous ▁sense ▁of ▁purpose ▁to ▁his ▁porn ▁king ▁character ▁. ▁he ▁may ▁not ▁win ▁any ▁oscar s ▁, ▁but ▁he ▁still ▁ranks ▁highly ▁in ▁my ▁book ▁. ▁get ▁car ter ▁has ▁great ▁directing ▁, ▁strong ▁acting ▁by ▁rou rke ▁and ▁ca ine ▁, ▁and ▁energetic ▁car ▁chases ▁that ▁would ▁make ▁william ▁fried kin ▁proud ▁. ▁the ▁only ▁thing ▁it ▁lacks ▁- ▁as ▁with ▁most ▁hollywood ▁productions ▁- - ▁is ▁a ▁good ▁script ▁and ▁proper ▁casting ▁. ▁never ▁mind ▁that ▁it ▁should ▁never ▁have ▁been ▁made ▁at ▁all ▁. \u0001\n",
      "1 ▁while ▁watching ▁boiler ▁room ▁, ▁i ▁was ▁constantly ▁reminded ▁of ▁last ▁year ' s ▁masterpiece ▁fight ▁club ▁. ▁both ▁films ▁consist ▁of ▁a ▁predominately ▁male ▁cast ▁. ▁both ▁films ▁follow ▁young ▁men ▁as ▁they ▁illicit ly ▁fight ▁the ▁traditional ▁system ▁for ▁their ▁own ▁desires ▁. ▁and ▁both ▁films ▁are ▁seen ▁through ▁the ▁eyes ▁of ▁one ▁narrator ▁, ▁who ▁eventually ▁realizes ▁that ▁these ▁men ▁have ▁to ▁be ▁stopped ▁. ▁while ▁boiler ▁room ▁writer / director ▁be n ▁younger ▁does ▁not ▁get ▁his ▁point ▁across ▁as ▁well ▁as ▁david ▁finch er ▁does ▁for ▁fight ▁club ▁, ▁he ▁does ▁contribute ▁another ▁impressive ▁work ▁to ▁a ▁series ▁of ▁films ▁aiming ▁to ▁represent ▁the ▁new ▁generation ▁. ▁a ▁generation ▁which ▁has ▁seen ▁the ▁internet ▁prosper ▁and ▁where ▁everyone ▁wants ▁to ▁be ▁a ▁millionaire ▁. ▁paying ▁homage ▁to ▁olive r ▁stone ' s ▁1987 ▁classic ▁wall ▁street ▁, ▁younger ▁is ▁almost ▁modernizing ▁the ▁tale ▁by ▁using ▁younger ▁, ▁hip per ▁actors ▁to ▁play ▁the ▁greedy ▁villains ▁as ▁opposed ▁to ▁the ▁older ▁, ▁more ▁experienced ▁types ▁. ▁as ▁is ▁true ▁in ▁real ▁life ▁, ▁younger ▁minds ▁are ▁becoming ▁richer ▁and ▁richer ▁from ▁their ▁knowledge ▁of ▁more ▁standard ▁technology ▁. ▁boiler ▁room ▁dismisses ▁the ▁notion ▁of ▁ingenuity ▁and ▁shows ▁that ▁greed ▁and ▁desire ▁for ▁power ▁come ▁in ▁all ▁ages ▁. ▁another ▁similarity ▁with ▁fight ▁club ▁is ▁that ▁both ▁films ▁are ▁not ▁action ▁flicks ▁. ▁some ▁people ▁are ▁convinced ▁that ▁an ▁all - male ▁cast ▁automatically ▁means ▁there ▁must ▁be ▁gory ▁violence ▁, ▁here ▁is ▁proof ▁that ▁this ▁is ▁not ▁true ▁. ▁if ▁you ▁want ▁to ▁see ▁an ▁action ▁movie ▁starring ▁be n ▁aff leck ▁, ▁go ▁see ▁reindeer ▁games ▁this ▁weekend ▁. ▁if ▁you ▁want ▁to ▁see ▁a ▁smart ▁, ▁insightful ▁film ▁with ▁excellent ▁acting ▁and ▁a ▁clever ▁script ▁, ▁see ▁boiler ▁room ▁. ▁gi ovan ni ▁rib isi ▁gives ▁an ▁outstanding ▁performance ▁as ▁the ▁film ' s ▁narrator ▁, ▁set h ▁. ▁after ▁dropping ▁out ▁of ▁college ▁and ▁running ▁a ▁lucrative ▁gambling ▁center ▁for ▁college ▁students ▁in ▁his ▁apartment ▁, ▁set h ▁is ▁offered ▁a ▁high ▁paying ▁job ▁by ▁a ▁wealthy ▁man ▁( ▁nick y ▁kat t ▁) ▁. ▁he ▁agrees ▁to ▁take ▁the ▁job ▁( ▁in ▁which ▁you ▁are ▁guaranteed ▁to ▁become ▁a ▁millionaire ▁within ▁three ▁years ▁) ▁of ▁selling ▁stock ▁to ▁well - off ▁americans ▁from ▁the ▁mid - west ▁over ▁the ▁telephone ▁and ▁begins ▁to ▁fit ▁in ▁quite ▁well ▁with ▁his ▁co - workers ▁. ▁learning ▁tricky ▁techniques ▁to ▁deceive ▁innocent ▁people ▁into ▁buying ▁shares ▁of ▁a ▁good ▁in ▁production ▁, ▁set h ▁figures ▁this ▁is ▁too ▁good ▁to ▁be ▁true ▁. ▁after ▁stumbling ▁into ▁a ▁room ▁at ▁the ▁wrong ▁time ▁, ▁he ▁knows ▁there ▁is ▁something ▁no ▁good ▁about ▁this ▁company ▁. ▁at ▁this ▁point ▁, ▁set h ▁is ▁left ▁with ▁the ▁ultimate ▁choice ▁; ▁continue ▁with ▁the ▁american ▁dream ▁and ▁make ▁millions ▁or ▁tell ▁the ▁authorities ▁that ▁something ▁fishy ▁is ▁going ▁on ▁. ▁rib isi ▁is ▁believable ▁as ▁set h ▁especially ▁when ▁he ▁shares ▁scenes ▁with ▁ron ▁r if kin ▁, ▁playing ▁set h ' s ▁dad ▁. ▁the ▁two ▁have ▁perfect ▁chemistry ▁as ▁a ▁troubled ▁father ▁and ▁son ▁trying ▁to ▁impress ▁each ▁other ▁and ▁simultaneously ▁impress ▁themselves ▁. ▁the ▁transitions ▁from ▁anger ▁to ▁sympathy ▁that ▁these ▁scenes ▁contain ▁are ▁the ▁standout ▁segments ▁of ▁the ▁entire ▁film ▁. ▁the ▁supporting ▁cast ▁of ▁greedy ▁co - workers ▁is ▁also ▁flawless ▁. ▁be n ▁aff leck ▁shines ▁in ▁a ▁short ▁but ▁sweet ▁performance ▁as ▁a ▁recruiter ▁for ▁the ▁company ▁, ▁nick y ▁kat t ▁is ▁fabulous ▁as ▁the ▁ostensibly ▁friendly ▁boss ▁who ▁eventually ▁becomes ▁extremely ▁jealous ▁of ▁set h ▁, ▁and ▁vin ▁diesel ▁gives ▁his ▁best ▁performance ▁of ▁his ▁career ▁as ▁the ▁foil ▁character ▁of ▁nick y ▁kat t ▁. ▁the ▁energy ▁of ▁the ▁cast ▁as ▁a ▁whole ▁makes ▁boiler ▁room ▁well ▁paced ▁and ▁never ▁boring ▁. ▁the ▁only ▁major ▁error ▁in ▁the ▁film ▁is ▁that ▁nothing ▁major ▁happens ▁. ▁there ▁is ▁no ▁big ▁plot ▁twist ▁or ▁climatic ▁point ▁to ▁make ▁the ▁film ▁more ▁memorable ▁. ▁due ▁to ▁the ▁lack ▁of ▁a ▁major ▁event ▁, ▁boiler ▁room ▁never ▁finds ▁a ▁suitable ▁genre ▁to ▁fit ▁into ▁. ▁the ▁movie ▁is ▁not ▁intense ▁enough ▁to ▁be ▁a ▁thriller ▁, ▁the ▁romantic ▁segments ▁involving ▁set h ▁and ▁a bby ▁( ▁n ia ▁long ▁) ▁are ▁not ▁properly ▁finalized ▁, ▁and ▁the ▁dialogue ▁isn ' t ▁funny ▁enough ▁to ▁make ▁it ▁a ▁comedy ▁. ▁in ▁having ▁trouble ▁to ▁characterize ▁the ▁movie ▁as ▁a ▁whole ▁, ▁boiler ▁room ▁is ▁slightly ▁confused ▁at ▁times ▁. ▁it ▁doesn ' t ▁seem ▁to ▁know ▁which ▁category ▁to ▁fit ▁itself ▁into ▁. ▁one ▁satisfying ▁concluding ▁scene ▁could ▁have ▁changed ▁the ▁whole ▁film ▁for ▁the ▁better ▁. ▁otherwise ▁, ▁the ▁movie ▁is ▁fun ▁to ▁watch ▁thanks ▁to ▁its ▁lively ▁cast ▁of ▁young ▁actors ▁. \u0001\n",
      "0 ▁i ▁wont ▁even ▁pretend ▁that ▁i ▁have ▁seen ▁the ▁other ▁3 ▁alien ▁films ▁. ▁i ▁saw ▁glimpses ▁of ▁alien ▁and ▁aliens ▁and ▁fragments ▁of ▁alien ▁3 ▁, ▁but ▁i ▁have ▁by ▁no ▁means ▁actually ▁sat ▁down ▁and ▁watched ▁any ▁of ▁them ▁. ▁so ▁take ▁my ▁opinion ▁from ▁an ▁unbiased ▁and ▁impartial ▁perspective ▁: ▁alien ▁resurrection ▁is ▁not ▁worth ▁the ▁large ▁bag ▁of ▁lollies ▁i ▁was ▁munching ▁into ▁after ▁a ▁hard ▁days ▁work ▁. ▁i ▁didnt ▁just ▁dislike ▁it ▁so ▁much ▁because ▁the ▁plot ▁was ▁awful ▁; ▁most ▁of ▁the ▁acting ▁was ▁very ▁average ▁or ▁the ▁special ▁effects ▁got ▁tiresome ▁after ▁5 ▁minutes ▁of ▁painful ▁viewing ▁but ▁also ▁the ▁fact ▁that ▁every ▁tedious ▁ingredient ▁thrown ▁in ▁attempts ▁to ▁give ▁this ▁pointless ▁dribble ▁some ▁meaning ▁. ▁sig our ney ▁weaver ▁, ▁who s ▁role ▁in ▁alien ▁earned ▁her ▁an ▁academy ▁award ▁nomination ▁, ▁plays ▁the ▁character ▁of ▁rip ley ▁, ▁who ▁died ▁fighting ▁against ▁extraterrestrial ▁scum ▁in ▁alien ▁3 ▁. ▁200 ▁years ▁later ▁, ▁scientists ▁use ▁a ▁sample ▁of ▁blood ▁found ▁at ▁the ▁site ▁of ▁her ▁death ▁to ▁recreate ▁rip ley ▁- ▁including ▁the ▁alien ▁that ▁was ▁stuck ▁inside ▁her ▁. ▁in ▁no ▁time ▁they ▁remove ▁the ▁alien ▁from ▁her ▁body ▁and ▁separate ▁the ▁two ▁; ▁yet ▁rip ley ▁is ▁now ▁not ▁completely ▁human ▁, ▁possessing ▁strange ▁amounts ▁of ▁strength ▁and ▁being ▁able ▁to ▁withhold ▁greater ▁pain ▁than ▁a ▁normal ▁being ▁. ▁im ▁sure ▁someone ▁can ▁explain ▁exactly ▁why ▁this ▁is ▁so ▁, ▁but ▁for ▁now ▁well ▁just ▁ignore ▁it ▁and ▁move ▁on ▁. ▁why ▁, ▁you ▁ask ▁, ▁would ▁rip ley ▁and ▁the ▁alien ▁inside ▁her ▁be ▁recreated ▁? ▁well ▁, ▁the ▁smart ▁little ▁lab ▁researchers ▁believed ▁that ▁they ▁would ▁be ▁able ▁to ▁discover ▁many ▁advancements ▁in ▁science ▁from ▁studying ▁the ▁alien ▁creature ▁. ▁these ▁were ▁the ▁same ▁people ▁who ▁said ▁they ▁could ▁control ▁the ▁alien ▁and ▁that ▁their ▁was ▁no ▁danger ▁in ▁their ▁research ▁. ▁they ▁( ▁yawn ▁) ▁were ▁wrong ▁. ▁it ▁doesnt ▁take ▁a ▁genius ▁to ▁figure ▁out ▁that ▁the ▁rest ▁of ▁the ▁film ▁is ▁a ▁continuous ▁cat ▁and ▁mouse ▁chase ▁between ▁the ▁humans ▁who ▁die ▁off ▁one ▁by ▁one ▁and ▁the ▁aliens ▁who ▁open ▁their ▁mouths ▁on ▁every ▁possible ▁occasion ▁to ▁show ▁off ▁their ▁frightful ly ▁scary ▁un - flo ssed ▁teeth ▁. ▁there s ▁a ▁little ▁saying ▁that ▁goes ▁something ▁like ▁this ▁: ▁if ▁you re ▁going ▁to ▁make ▁a ▁bad ▁film ▁, ▁do ▁it ▁well ▁. ▁i ▁have ▁seen ▁plenty ▁of ▁disappointing ▁and ▁under achieving ▁movies ▁, ▁but ▁they ▁are ▁not ▁the ▁ones ▁that ▁really ▁bother ▁me ▁. ▁alien ▁resurrection ▁, ▁a ▁good ▁example ▁of ▁a ▁film ▁that ▁really ▁gets ▁up ▁my ▁nose ▁, ▁is ▁not ▁so ▁much ▁a ▁discouraging ▁experience ▁but ▁an ▁off ▁putting ▁one ▁. ▁there ▁is ▁no ▁way ▁to ▁describe ▁why ▁the ▁australian ▁ma ▁rating ▁was ▁given ▁to ▁this ▁film ▁other ▁than ▁unnecessary ▁and ▁repulsive ▁go re ▁, ▁which ▁attempts ▁to ▁distract ▁us ▁from ▁the ▁wandering - but - not - going - anywhere ▁plot ▁. ▁in ▁one ▁scene ▁my ▁stomach ▁churned ▁as ▁i ▁witnessed ▁a ▁maw k ish ▁looking ▁alien ▁put ▁its ▁hand ▁on ▁a ▁characters ▁head ▁and ▁literally ▁ripped ▁most ▁of ▁his ▁face ▁off ▁. ▁another ▁one ▁featured ▁rip ley ▁sticking ▁a ▁knife ▁through ▁her ▁hand ▁just ▁to ▁impress ▁someone ▁she ▁was ▁talking ▁to ▁. ▁fair ▁enough ▁if ▁it ▁were ▁in ▁a ▁horror ▁film ▁or ▁even ▁if ▁it ▁bar ed ▁any ▁significance ▁to ▁the ▁story ▁, ▁but ▁this ▁is ▁unfortunately ▁not ▁the ▁case ▁, ▁and ▁this ▁sort ▁of ▁bizarre ▁go re ▁is ▁inexcusable ▁. ▁if ▁you ▁think ▁im ▁struggling ▁to ▁find ▁one ▁good ▁aspect ▁about ▁the ▁film ▁, ▁then ▁you ▁thought ▁right ▁. ▁one ▁exciting ▁scene ▁is ▁a ▁credit ▁( ▁and ▁the ▁only ▁part ▁remotely ▁worth ▁seeing ▁) ▁to ▁the ▁film ▁, ▁in ▁which ▁an ▁alien ▁chases ▁two ▁men ▁( ▁one ▁a ▁cripple ▁) ▁up ▁a ▁tall ▁ladder ▁with ▁interesting ▁results ▁. ▁but ▁of ▁course ▁after ▁that ▁thrilling ▁scene ▁alien ▁resurrection ▁had ▁no ▁problem ▁in ▁returning ▁to ▁its ▁pathetically ▁low ▁standard ▁, ▁and ▁ends ▁up ▁crashing ▁into ▁the ▁shores ▁of ▁movie ▁dullness ▁more ▁often ▁than ▁japanese ▁kamikaze ▁jet ▁fighters ▁fall ▁off ▁their ▁skis ▁. ▁so ▁in ▁a ▁film ▁where ▁the ▁one ▁liners ▁come ▁as ▁bad ▁as ▁earth ▁man ▁, ▁what ▁a ▁shit ▁hole ▁and ▁the ▁primary ▁means ▁for ▁one ▁man s ▁strategic ▁attack ▁is ▁to ▁bounce ▁bullets ▁off ▁walls ▁to ▁hit ▁an ▁opponent ▁, ▁its ▁no ▁wonder ▁why ▁alien ▁: ▁resurrection ▁turned ▁out ▁to ▁be ▁as ▁off ▁putting ▁and ▁clumsy ▁as ▁it ▁is ▁. ▁oh ▁, ▁and ▁in ▁case ▁you re ▁wondering ▁about ▁my ▁bag ▁of ▁lollies ▁the ▁less ▁i ▁enjoy ▁a ▁film ▁, ▁the ▁more ▁i ▁eat ▁. ▁so ▁all ▁the ▁lollies ▁were ▁gone ▁in ▁15 ▁minutes ▁; ▁but ▁however ▁empty ▁the ▁bag ▁turned ▁out ▁to ▁be ▁, ▁it ▁was ▁still ▁a ▁long ▁way ▁ahead ▁of ▁the ▁movie \u0001\n"
     ]
    }
   ],
   "source": [
    "for i in range(15,20):\n",
    "    char_ids = X[i].x\n",
    "    text_for_ids = ''.join([vocab_dict_rev[ci.item()] for ci in char_ids])\n",
    "    print(torch.argmax(y[i]).item(), text_for_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rand = torch.randn((64, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0029)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rand.var(unbiased=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[231269], token_positions=[41047], character_length=[50], num_tokens=[50], token_indices=[231269], token_lengths=[41047], token_embeddings=[41047, 64], token_sentiments=[41047, 2], batch=[231269], ptr=[51], cumulative_token_indices=[231269])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = torch.randn((len(X.token_positions), 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([41047])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.repeat_interleave(torch.arange(len(X.num_tokens)), X.num_tokens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,    1,    2,  ..., 2010, 2011, 2012])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.token_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3275,  1.1275, -0.1709,  ...,  0.9102,  1.4914,  1.2868],\n",
       "        [-1.1187,  0.6399,  0.6390,  ..., -0.2899,  1.0811,  1.2505],\n",
       "        [ 0.4530, -0.6708, -0.8336,  ...,  0.1828, -0.8456,  3.1728],\n",
       "        ...,\n",
       "        [-1.0184,  0.4587,  1.4228,  ...,  1.1129, -0.6870, -0.6287],\n",
       "        [ 0.1125, -0.6503, -0.7957,  ...,  0.0954,  0.0142,  1.0554],\n",
       "        [ 0.1875,  0.6259,  1.3763,  ..., -0.7604, -0.5921, -0.7044]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATv2Conv, SimpleConv\n",
    "\n",
    "# Normalization on each feature of all tokens, for this we used batch norm class but with tokens at batch dimention\n",
    "class GCNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, *args, **kwargs):\n",
    "        super(GCNN, self).__init__(*args, **kwargs)\n",
    "        self.gnn = GATv2Conv(hidden_dim, hidden_dim//8, heads=4, add_self_loops=False)\n",
    "        self.conv = nn.Conv1d(hidden_dim, hidden_dim//2, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(hidden_dim//2, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim//2)\n",
    "        \n",
    "    def forward(self, x, edge_data, return_attention_weights = False):\n",
    "        x1, edge_weights = self.gnn(x, edge_data, return_attention_weights=return_attention_weights) \n",
    "        x2 = F.relu(self.conv(x.T).T)\n",
    "        x1 = F.leaky_relu_(self.bn1(x1))\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        return x, edge_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0221, -0.0623,  0.0713,  0.0384,  0.0750])\n",
      "tensor([1.0305, 0.9600, 0.9648, 1.0107, 1.0308])\n",
      "tensor([1.5497e-08, 2.3842e-09, 0.0000e+00, 5.9605e-09, 1.4305e-08])\n",
      "tensor([1.0012, 1.0012, 1.0012, 1.0012, 1.0012])\n"
     ]
    }
   ],
   "source": [
    "sample_t = torch.randn((400, 5))\n",
    "sample_t.shape\n",
    "print(torch.mean(sample_t, dim=0))\n",
    "print(torch.std(sample_t, dim=0))\n",
    "bn1 = nn.BatchNorm1d(5)\n",
    "print(torch.mean(bn1(sample_t), dim=0).detach())\n",
    "print(torch.std(bn1(sample_t), dim=0).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from copy import copy, deepcopy\n",
    "# gcnn_model = GCNN(64)\n",
    "# flopt_counter = FlopCounterMode(gcnn_model)\n",
    "# X.edge_index = torch.randint(0, len(X.token_positions), size=(2,200))\n",
    "# with flopt_counter:\n",
    "#     x_input = deepcopy(torch.randn((len(X.token_positions), 64)))\n",
    "#     edge_data = deepcopy(X.edge_index)\n",
    "#     gcnn_model(x_input, edge_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx, to_undirected\n",
    "\n",
    "class GenGraph(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, virtual_nodes, lattice_step, lattice_pattern=None, *args, **kwargs):\n",
    "        super(GenGraph, self).__init__(*args, **kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.virtual_nodes = virtual_nodes\n",
    "        self.lattice_step = lattice_step\n",
    "        self.lp = lattice_pattern if lattice_pattern is None else torch.tensor(lattice_pattern)\n",
    "        self.virtual_node_embeddings = nn.Embedding(self.virtual_nodes, hidden_dim)\n",
    "        \n",
    "    def gen_graph(self, x, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance=2, seed=-1):\n",
    "        random_links, lattice_links, tc_range = self.calculate_graph(x, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance, seed)\n",
    "        v_n_e_counts = total_token_coutns*self.virtual_nodes\n",
    "        base_numel = random_links.numel() + lattice_links.numel()*2\n",
    "        edge_index = torch.empty((2, base_numel + v_n_e_counts*2), dtype=torch.int64, device=x.device)\n",
    "        self.fill_lattice_and_random_edges(edge_index, random_links, lattice_links, tc_range)\n",
    "            \n",
    "        if self.virtual_nodes > 0:\n",
    "            virtual_nodes_range = torch.arange(self.virtual_nodes, device=x.device).view(1, -1)\n",
    "            virtual_nodes_ids = torch.repeat_interleave(virtual_nodes_range, len(token_counts), dim=0)\n",
    "            v_n_idx = (virtual_nodes_ids + torch.arange(0, len(token_counts)*self.virtual_nodes, self.virtual_nodes, device=x.device).view(-1, 1) + total_token_coutns )\n",
    "            virtual_edge_ids = torch.repeat_interleave(v_n_idx.view(-1), token_counts.view(-1, 1).expand(len(token_counts), self.virtual_nodes).reshape(-1), dim=0).view(1, -1)\n",
    "            \n",
    "            embs = self.virtual_node_embeddings(virtual_nodes_ids.T).view(-1, self.hidden_dim)\n",
    "            x_extended = torch.cat([x, embs], dim=0)\n",
    "            x_index = torch.arange(total_token_coutns, device=x.device).repeat(self.virtual_nodes).view(1, -1)\n",
    "            edge_index[:, base_numel:base_numel+v_n_e_counts] = torch.cat([x_index, virtual_edge_ids], dim=0)\n",
    "            edge_index[:, base_numel+v_n_e_counts:] = torch.cat([virtual_edge_ids, x_index], dim=0)\n",
    "            x = x_extended\n",
    "        \n",
    "        return Batch.from_data_list([Data(x=x, edge_index=edge_index)])\n",
    "        \n",
    "    def re_gen_graph(self, x, edge_index, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance=2, seed=-1):\n",
    "        random_links, lattice_links, tc_range = self.calculate_graph(x, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance, seed)\n",
    "        base_numel = random_links.numel() + lattice_links.numel()*2\n",
    "        \n",
    "        self.fill_lattice_and_random_edges(edge_index, random_links, lattice_links, tc_range)\n",
    "        # for i in range(base.shape[1]):\n",
    "        #     edge_index[:, i*base.shape[0]:(i+1)*base.shape[0]] = torch.cat([tc_range, base[:,i].view(1,-1)], dim=0)\n",
    "            \n",
    "        return Batch.from_data_list([Data(x=x, edge_index=edge_index)])\n",
    "    \n",
    "    def replace_unimportant_edges(self, edge_weights, x, edge_index, total_token_coutns, token_counts, random_edges, lattice_edges, p_keep=1, lattice_start_distance=2, seed=-1):\n",
    "        v_n_e_counts = total_token_coutns*self.virtual_nodes\n",
    "        if v_n_e_counts>0:\n",
    "            important_indices = torch.topk(edge_weights[:-2*v_n_e_counts].view(-1, total_token_coutns), p_keep, dim=0).indices\n",
    "        else:\n",
    "            important_indices = torch.topk(edge_weights.view(-1, total_token_coutns), p_keep, dim=0).indices\n",
    "        # important_indices = torch.topk(edge_weights[:-1*total_token_coutns].view(-1, total_token_coutns), 1, dim=0).indices.squeeze()\n",
    "\n",
    "        important_indices = torch.arange(total_token_coutns, dtype=torch.int64, device=x.device) + important_indices*total_token_coutns\n",
    "        important_indices = important_indices.view(-1)\n",
    "        random_links, lattice_links, tc_range = self.calculate_graph(x, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance, seed)\n",
    "        base_numel = random_links.numel() + lattice_links.numel()*2\n",
    "        \n",
    "        new_edge_index = torch.empty((2, base_numel + important_indices.shape[0] + 2*v_n_e_counts), dtype=torch.int64, device=x.device)\n",
    "        self.fill_lattice_and_random_edges(new_edge_index, random_links, lattice_links, tc_range)\n",
    "        new_edge_index[:, base_numel:base_numel+important_indices.shape[0]] = edge_index[:, important_indices]\n",
    "        if(self.virtual_nodes>0):\n",
    "            new_edge_index[:, -2*v_n_e_counts:] = edge_index[:, -2*v_n_e_counts:]\n",
    "            \n",
    "        # for i in range(base.shape[1]):\n",
    "        #     new_edge_index[:, i*base.shape[0]:(i+1)*base.shape[0]] = torch.cat([tc_range, base[:,i].view(1,-1)], dim=0)\n",
    "        \n",
    "        return Batch.from_data_list([Data(x=x, edge_index=new_edge_index)])\n",
    "         \n",
    "    def calculate_graph(self, x, total_token_counts, token_counts, random_edges, lattice_edges, lattice_start_distance, seed=-1):\n",
    "        if seed>-1:\n",
    "            torch.manual_seed(seed)\n",
    "        tc_extended = torch.repeat_interleave(token_counts, token_counts, dim=0).view(-1,1)\n",
    "        tc_lower_bound = torch.empty((len(token_counts)+1), dtype=torch.long, device=x.device) #torch.cuda.IntTensor(len(token_counts)+1) #\n",
    "        tc_lower_bound[0] = 0\n",
    "        tc_lower_bound[1:] = torch.cumsum(token_counts, dim=0)\n",
    "        tc_lower_bound_extended = torch.repeat_interleave(tc_lower_bound[:-1], token_counts, dim=0).view(-1,1)\n",
    "        tc_range = torch.arange(tc_lower_bound[-1], device=x.device).view(-1,1)\n",
    "        # torch.arange(tc_lower_bound[-1], dtype=torch.int32, device=x.device).view(-1,1)\n",
    "        random_ints = torch.randint(0, 2*total_token_counts, (total_token_counts, random_edges), device=x.device) # torch.cuda.IntTensor(len(token_lengths), random_edges).random_()\n",
    "        lattice = self.lp.to(x.device) if self.lp is not None else torch.arange(lattice_start_distance, max(lattice_start_distance, self.lattice_step*lattice_edges+1), self.lattice_step, device=x.device).view(1, -1)\n",
    "        \n",
    "\n",
    "        # exponentials = torch.pow(2, torch.arange(1, self.exp_edges+1, device=x.device)).view(1, -1)\n",
    "        tc_local_range = tc_range - tc_lower_bound_extended\n",
    "        random_links = (((random_ints % (tc_extended - 1))+1 + tc_local_range) % tc_extended)+tc_lower_bound_extended\n",
    "        lattice_links = ((lattice + tc_local_range) % tc_extended)+tc_lower_bound_extended\n",
    "        # base = torch.cat([base1, base2], dim=1)\n",
    "        tc_range = tc_range.view(1,-1)\n",
    "        return random_links, lattice_links, tc_range\n",
    "    \n",
    "    def fill_lattice_and_random_edges(self, edge_index, random_links, lattice_links, tc_range):\n",
    "        for i in range(0, lattice_links.shape[1]*2, 2):\n",
    "            edge_index[:, i*lattice_links.shape[0]:(i+1)*lattice_links.shape[0]] = torch.cat([lattice_links[:,i//2].view(1,-1), tc_range], dim=0)\n",
    "            edge_index[:, (i+1)*lattice_links.shape[0]:(i+2)*lattice_links.shape[0]] = edge_index[:, i*lattice_links.shape[0]:(i+1)*lattice_links.shape[0]][[1, 0]]\n",
    "            \n",
    "        for i in range(random_links.shape[1]):\n",
    "            j = i + lattice_links.shape[1]*2\n",
    "            edge_index[:, j*random_links.shape[0]:(j+1)*random_links.shape[0]] = torch.cat([random_links[:,i].view(1,-1), tc_range], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment_Injection(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)        \n",
    "        self.conv1 = nn.Conv1d(2, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim*2, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "    def forward(self, x, token_sentiments):\n",
    "        x1 = F.relu_(self.bn1(self.conv1(token_sentiments.T).T))\n",
    "        x = F.relu_(self.conv2(torch.cat([x, x1], dim=1).T))\n",
    "        # x = x + x1\n",
    "        return x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment1  = Sentiment_Injection(64)\n",
    "# sentiment1(torch.ones((41047, 64)), torch.ones((41047, 2))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATv2Conv, SimpleConv, summary\n",
    "\n",
    "class CNN_for_Text(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embedding, pos_emb_size=8192, embedding_dim=64, hidden_dim=64, dropout=0.3, num_out_features=4, seed=-1, random_edges=4, lattice_edges=10, virtual_nodes=1, lattice_step=2, lattice_start_distance=2, inject_embedding_dim=64, use_token_polarity=[False, False, False], *args, **kwargs) -> None:\n",
    "        super(CNN_for_Text, self).__init__(*args, **kwargs)\n",
    "        self.pos_emb_size = pos_emb_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.virtual_nodes = virtual_nodes\n",
    "        self.base_random_edges = random_edges\n",
    "        self.base_lattice_edges = lattice_edges\n",
    "        self.lattice_start_distance = lattice_start_distance\n",
    "        self.use_token_polarity = use_token_polarity\n",
    "        if seed>-1:\n",
    "            torch.manual_seed(seed)\n",
    " \n",
    "        self.embedding = nn.Embedding(num_embedding, embedding_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.positional_encoding = nn.Embedding(pos_emb_size, embedding_dim)\n",
    "        self.positional_encoding.weight = self.create_positional_encoding()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(embedding_dim, hidden_dim, kernel_size=5, padding=2)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, padding=2)\n",
    "        \n",
    "        if self.use_token_polarity[0]:\n",
    "            self.conv3 = nn.Conv1d(2*hidden_dim + 2, hidden_dim, kernel_size=3, padding=1)\n",
    "        else:\n",
    "            self.conv3 = nn.Conv1d(2*hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv4 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        \n",
    "        if self.use_token_polarity[1]:\n",
    "            self.sentiment1  = Sentiment_Injection(hidden_dim)\n",
    "        if self.use_token_polarity[2]:\n",
    "            self.sentiment2  = Sentiment_Injection(hidden_dim)\n",
    "            \n",
    "        self.gcnn1 = GCNN(hidden_dim)\n",
    "        self.gcnn2 = GCNN(hidden_dim+inject_embedding_dim)\n",
    "        self.graph_generator = GenGraph(hidden_dim, virtual_nodes, lattice_step)\n",
    "        \n",
    "        k = 4\n",
    "        self.fc0 = nn.Linear(hidden_dim , hidden_dim+inject_embedding_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim+inject_embedding_dim , hidden_dim * k)\n",
    "        self.fc2 = nn.Linear(hidden_dim * (2+virtual_nodes) * k , 32)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(32, num_out_features)\n",
    "        self.max_length = 0\n",
    "    \n",
    "    def forward(self, g_data):\n",
    "            \n",
    "        x = self.embedding(g_data.x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.T\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        x1 = scatter_max(x, g_data.cumulative_token_indices, dim=1)[0]\n",
    "        x2 = scatter_mean(x, g_data.cumulative_token_indices, dim=1)\n",
    "\n",
    "        if self.use_token_polarity[0]:\n",
    "            x = torch.cat([x1, x2, g_data.token_sentiments.T], dim=0)\n",
    "        else:\n",
    "            x = torch.cat([x1, x2], dim=0)\n",
    "            \n",
    "        x = F.relu(self.conv3(x)).T\n",
    "        \n",
    "        if self.use_token_polarity[1]:\n",
    "            x = self.sentiment1(x, g_data.token_sentiments)\n",
    "            \n",
    "        x = x + self.positional_encoding(g_data.token_positions)\n",
    "\n",
    "        rand_edges, lattice_edges = self.base_random_edges, self.base_lattice_edges\n",
    "            \n",
    "        graph = self.graph_generator.gen_graph(x, len(g_data.token_lengths), g_data.num_tokens, rand_edges, lattice_edges, lattice_start_distance=self.lattice_start_distance)\n",
    "        rand_edges = rand_edges-1\n",
    "        lattice_edges = lattice_edges-1\n",
    "        \n",
    "        \n",
    "        doc_token_index = torch.repeat_interleave(torch.arange(len(g_data.num_tokens), device=x.device), g_data.num_tokens)\n",
    "        x, edge_weights = self.gcnn1(graph.x, graph.edge_index, return_attention_weights = True)\n",
    "        edge_weights = edge_weights[1][:graph.edge_index.shape[1], 0]\n",
    "        \n",
    "        graph = self.graph_generator.replace_unimportant_edges(edge_weights, x, graph.edge_index, len(g_data.token_lengths), g_data.num_tokens, rand_edges, lattice_edges, p_keep=2, lattice_start_distance=self.lattice_start_distance+1)\n",
    "        \n",
    "        \n",
    "        if self.use_token_polarity[2]:\n",
    "            x = self.sentiment2(x, g_data.token_sentiments)\n",
    "            \n",
    "        x = torch.cat([graph.x[:g_data.token_embeddings.shape[0]], g_data.token_embeddings], dim=1)\n",
    "        x1 = F.relu(self.fc0(graph.x[g_data.token_embeddings.shape[0]:]))\n",
    "        x = torch.cat([x, x1], dim=0)\n",
    "        \n",
    "        x, edge_weights = self.gcnn2(x, graph.edge_index)\n",
    "\n",
    "        x = F.elu_(self.fc1(x))\n",
    "        x1 = scatter_max(x[:len(g_data.token_lengths)], doc_token_index, dim=0)[0]\n",
    "        x2 = scatter_mean(x[:len(g_data.token_lengths)], doc_token_index, dim=0)\n",
    "        vn_embs = x[len(g_data.token_lengths):]\n",
    "        x_for_cat = [x1, x2]\n",
    "        x_for_cat.extend([vn_embs[i*x1.shape[0]:(i+1)*x1.shape[0]] for i in range(self.virtual_nodes)])\n",
    "        x = torch.cat(x_for_cat, dim=1)\n",
    "        \n",
    "        x = F.elu_(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def create_positional_encoding(self):\n",
    "        position = torch.arange(self.pos_emb_size).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.hidden_dim, 2) * (-math.log(10000.0) / self.hidden_dim))\n",
    "        pe = torch.zeros(self.pos_emb_size, self.hidden_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return torch.nn.Parameter(pe, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module                       FLOP    % Total\n",
      "------------------------  -------  ---------\n",
      "CNN_for_Text              31.956B    100.00%\n",
      " - aten.convolution       27.583B     86.32%\n",
      " - aten.addmm              4.373B     13.68%\n",
      " CNN_for_Text.conv1        9.473B     29.64%\n",
      "  - aten.convolution       9.473B     29.64%\n",
      " CNN_for_Text.conv2        9.473B     29.64%\n",
      "  - aten.convolution       9.473B     29.64%\n",
      " CNN_for_Text.conv3        2.018B      6.31%\n",
      "  - aten.convolution       2.018B      6.31%\n",
      " CNN_for_Text.sentiment1   2.049B      6.41%\n",
      "  - aten.convolution       2.049B      6.41%\n",
      " CNN_for_Text.gcnn1        0.841B      2.63%\n",
      "  - aten.addmm             0.336B      1.05%\n",
      "  - aten.convolution       0.504B      1.58%\n",
      " CNN_for_Text.sentiment2   2.049B      6.41%\n",
      "  - aten.convolution       2.049B      6.41%\n",
      " CNN_for_Text.fc0          0.000B      0.00%\n",
      "  - aten.addmm             0.000B      0.00%\n",
      " CNN_for_Text.gcnn2        3.363B     10.52%\n",
      "  - aten.addmm             1.345B      4.21%\n",
      "  - aten.convolution       2.018B      6.31%\n",
      " CNN_for_Text.fc1          2.690B      8.42%\n",
      "  - aten.addmm             2.690B      8.42%\n",
      " CNN_for_Text.fc2          0.002B      0.01%\n",
      "  - aten.addmm             0.002B      0.01%\n",
      " CNN_for_Text.fc_out       0.000B      0.00%\n",
      "  - aten.addmm             0.000B      0.00%\n"
     ]
    }
   ],
   "source": [
    "# for p1 in [False, True]:\n",
    "#     for p2 in [False, True]:\n",
    "#         for p3 in [False, True]:\n",
    "# print(f'\\n{p1}, {p2}, {p3}: \\n')\n",
    "classifier_torch_model = CNN_for_Text(num_embedding=num_embedding, hidden_dim=64, embedding_dim=64, pos_emb_size=3072, dropout=0.2, num_out_features=len(class_id), seed=911, random_edges=4, lattice_edges=4, lattice_step=2, virtual_nodes=0, lattice_start_distance=2, use_token_polarity=[False, True, True]).eval()\n",
    "flopt_counter = FlopCounterMode(classifier_torch_model)\n",
    "with flopt_counter:\n",
    "    classifier_torch_model(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(classifier_torch_model.parameters())).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CnnGnnClassifierLightningModel(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        num_classes,\n",
    "        optimizer=None,\n",
    "        loss_func=None,\n",
    "        learning_rate=0.01,\n",
    "        batch_size=64,\n",
    "        lr_scheduler=None,\n",
    "        user_lr_scheduler=False,\n",
    "        min_lr=0.0,\n",
    "    ):\n",
    "        super(CnnGnnClassifierLightningModel, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "        self.min_lr = min_lr\n",
    "        # self.save_hyperparameters(ignore=[\"model\"])\n",
    "        self.save_hyperparameters(logger=False)\n",
    "        self.optimizer = self._get_optimizer(optimizer)\n",
    "        self.lr_scheduler = (\n",
    "            self._get_lr_scheduler(lr_scheduler) if user_lr_scheduler else None\n",
    "        )\n",
    "        self.loss_func = loss_func\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.model(x)\n",
    "\n",
    "    def on_train_epoch_start(self) -> None:\n",
    "        param_groups = next(iter(self.optimizer.param_groups))\n",
    "        if \"lr\" in param_groups and param_groups[\"lr\"] is not None:\n",
    "            current_learning_rate = float(param_groups[\"lr\"])\n",
    "            self.log(\n",
    "                \"lr\",\n",
    "                current_learning_rate,\n",
    "                batch_size=self.batch_size,\n",
    "                on_epoch=True,\n",
    "                on_step=False,\n",
    "            )\n",
    "\n",
    "    def training_step(self, batch, *args, **kwargs):\n",
    "        X, y = batch\n",
    "        X.to(self.device)\n",
    "        y.to(self.device)\n",
    "        \n",
    "        self.model.train()\n",
    "        y_out = self(X)\n",
    "\n",
    "        loss = self.loss_func(y_out.view(y.shape), y )\n",
    "        self.train_losses.append(loss.detach().item())\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            batch_size=self.batch_size,\n",
    "            on_epoch=True,\n",
    "            on_step=True,\n",
    "        )\n",
    "        \n",
    "        self.train_acc(torch.argmax(y_out, dim=1), torch.argmax(y, dim=1))\n",
    "        self.log('train_acc', self.train_acc, prog_bar=True, on_epoch=True, on_step=True, batch_size=self.batch_size)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, *args, **kwargs):\n",
    "        X, y = batch\n",
    "        X.to(self.device)\n",
    "        y.to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        y_out = self(X)\n",
    "        loss = self.loss_func(y_out.view(y.shape), y )\n",
    "        self.val_losses.append(loss.detach().item())\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            batch_size=self.batch_size,\n",
    "            on_epoch=True,\n",
    "            on_step=True,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.val_acc(torch.argmax(y_out, dim=1), torch.argmax(y, dim=1))\n",
    "        self.log('val_acc', self.val_acc, prog_bar=True, on_epoch=True, on_step=True, batch_size=self.batch_size)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.lr_scheduler is None:\n",
    "            return self.optimizer\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": self.optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": self.lr_scheduler,\n",
    "                \"monitor\": \"train_loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def update_learning_rate(self, learning_rate: float):\n",
    "        self.learning_rate = learning_rate\n",
    "        for g in self.optimizer.param_groups:\n",
    "            g[\"lr\"] = learning_rate\n",
    "\n",
    "    def _get_optimizer(self, optimizer):\n",
    "        return (\n",
    "            optimizer\n",
    "            if optimizer is not None\n",
    "            else torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        )\n",
    "\n",
    "    def _get_lr_scheduler(self, lr_scheduler):\n",
    "        return (\n",
    "            lr_scheduler\n",
    "            if lr_scheduler is not None\n",
    "            else torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer, patience=5, factor=0.5, mode=\"min\", min_lr=self.min_lr\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from torchmetrics import ConfusionMatrix\n",
    "\n",
    "def calculate_metrics(cl_model, dataloader):\n",
    "    cm = ConfusionMatrix(task=\"multiclass\", num_classes=len(class_id))\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    cl_model = cl_model.eval()\n",
    "    cl_model.to(device)\n",
    "    for X, y in tqdm(dataloader):\n",
    "        X = X.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_p = cl_model(X)\n",
    "            y_p = y_p.cpu()\n",
    "        y_pred.append(y_p)\n",
    "        y_true.append(y)\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "    y_true = torch.cat(y_true, dim=0)\n",
    "    y_pred2 = torch.argmax(y_pred, dim=1)\n",
    "    y_true2 = torch.argmax(y_true, dim=1)\n",
    "    print(f'classification report: \\n {classification_report(y_true2, y_pred2, digits=4)}')\n",
    "    print(f'confusion matrix:\\n {cm(y_pred2, y_true2)}')\n",
    "    print('================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fardin Rastakhiz @ 2023\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from lightning.pytorch.loggers import Logger, CSVLogger\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from typing import List\n",
    "from pytorch_lightning.core.saving import save_hparams_to_yaml\n",
    "\n",
    "class ModelManager(ABC):\n",
    "\n",
    "    def __init__(self,\n",
    "                 torch_model: torch.nn.Module,\n",
    "                 lightning_model,\n",
    "                 model_save_dir: str = '~/Desktop',\n",
    "                 log_dir: str = 'logs/',\n",
    "                 log_name: str = 'model_logs',\n",
    "                 device='cpu',\n",
    "                 max_epochs = 100,\n",
    "                 ckpt_path: str|None=None,\n",
    "                 accumulate_grad_batches=1):\n",
    "        self.torch_model = torch_model\n",
    "        self.lightning_model = lightning_model\n",
    "        self.log_dir = log_dir\n",
    "        self.log_name = log_name\n",
    "        self.model_save_dir = model_save_dir\n",
    "        self.device = device\n",
    "        self.accelerator = 'cpu' if self.device=='cpu' else 'gpu'\n",
    "        self.max_epochs = max_epochs\n",
    "        self.ckpt_path = ckpt_path\n",
    "\n",
    "        self.logger = self._create_logger()\n",
    "        self.callbacks = self._create_callbacks()\n",
    "        self.trainer: L.Trainer = self._create_trainer(accumulate_grad_batches)\n",
    "        self.tuner = Tuner(self.trainer)\n",
    "        self.tuning_result = None\n",
    "\n",
    "    def tune(self, data_manager=None, train_dataloaders=None, val_dataloaders=None, datamodule=None, draw_result=True, min_lr=0.0000001, max_lr=0.1):\n",
    "        self.tuning_result = self.tuner.lr_find(self.lightning_model, datamodule=data_manager, train_dataloaders=train_dataloaders,\n",
    "                         val_dataloaders=val_dataloaders, min_lr=min_lr,max_lr=max_lr, num_training=150)\n",
    "        if draw_result:\n",
    "            fig = self.tuning_result.plot(suggest=True)\n",
    "            fig.show()\n",
    "        self.update_learning_rate(self.tuning_result.suggestion())\n",
    "        return self.tuning_result.suggestion()\n",
    "    \n",
    "    def update_learning_rate(self, lr):\n",
    "        self.lightning_model.update_learning_rate(lr)\n",
    "\n",
    "    def fit(self, train_dataloaders=None, val_dataloaders=None, datamodule=None, max_epochs = -1, ckpt_path=None):\n",
    "        if ckpt_path is not None and ckpt_path != '':\n",
    "            self.ckpt_path = ckpt_path\n",
    "        if max_epochs>0:\n",
    "            self.trainer.fit_loop.max_epochs = max_epochs\n",
    "            # self.max_epochs = max_epochs\n",
    "            # self.trainer = self._create_trainer()\n",
    "        self.trainer.fit(self.lightning_model,\n",
    "                         datamodule=datamodule,\n",
    "                         train_dataloaders=train_dataloaders,\n",
    "                         val_dataloaders=val_dataloaders,\n",
    "                         ckpt_path = self.ckpt_path\n",
    "                         )\n",
    "\n",
    "    def validate(self, dataloaders=None, datamodule=None):\n",
    "        return self.trainer.validate(self.lightning_model,\n",
    "                             datamodule=datamodule,\n",
    "                             dataloaders=dataloaders)\n",
    "\n",
    "    def predict(self, dataloaders=None, datamodule=None):\n",
    "        return self.trainer.predict(self.lightning_model,\n",
    "                             datamodule=datamodule,\n",
    "                             dataloaders=dataloaders)\n",
    "\n",
    "    def _create_trainer(self, accumulate_grad_batches) -> L.Trainer:\n",
    "        return L.Trainer(\n",
    "            callbacks=self.callbacks,\n",
    "            max_epochs=self.max_epochs,\n",
    "            accelerator=self.accelerator,\n",
    "            logger=self.logger,\n",
    "            num_sanity_val_steps=0,\n",
    "            default_root_dir=self.model_save_dir,\n",
    "            accumulate_grad_batches=accumulate_grad_batches\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def _create_callbacks(self) -> List[Callback]:\n",
    "        pass\n",
    "\n",
    "    def _create_logger(self) -> Logger:\n",
    "        return CSVLogger(save_dir=self.log_dir, name=self.log_name)\n",
    "\n",
    "    @abstractmethod\n",
    "    def draw_summary(self, dataloader):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def plot_csv_logger(self, loss_names, eval_names):\n",
    "        pass\n",
    "    \n",
    "    def save_hyper_parameters(self):\n",
    "        mhparams = {\n",
    "            'start_lr': 0.045,\n",
    "            'ckpt_lrs' :  {51: 0.002, 65: 0.00058},\n",
    "            'last_lr' : 0.0003,\n",
    "            'ac_loss_factor': 0.0002,\n",
    "            'weight_decay': 0.0012\n",
    "        }\n",
    "        save_hparams_to_yaml(config_yaml=r'logs\\hetero_model_17_AG\\version_12\\hparams.yaml',\n",
    "                     hparams=mhparams)\n",
    "        \n",
    "    # def find_best_settings(data_manager,\n",
    "    #                        lrs: List[float]=[0.001], dropouts: List[float]=[0.2], \n",
    "    #                        weight_decays: List[float]=[0.00055], emb_factors: List[float]=[0.1], \n",
    "    #                        batch_sizes: List[int]=[128], log_name='find_best_settings'):\n",
    "    #     for lr in lrs:\n",
    "    #         for dropout in dropouts:\n",
    "    #             for wd in weight_decays:\n",
    "    #                 for emb_factor in emb_factors:\n",
    "    #                     for bs in batch_sizes:\n",
    "    #                         data_manager.update_batch_size(bs)\n",
    "    #                         torch_model = HeteroGcnGatModel1(300, 1, X1.metadata(), 128, dropout=dropout)\n",
    "    #                         lightning_model = HeteroBinaryLightningModel(torch_model,\n",
    "    #                                         torch.optim.Adam(torch_model.parameters(), lr=lr, weight_decay=wd),\n",
    "    #                                             loss_func=HeteroLoss1(exception_keys='word', enc_factor=emb_factor),\n",
    "    #                                             learning_rate=lr,\n",
    "    #                                             batch_size=bs,\n",
    "    #                                             user_lr_scheduler=True\n",
    "    #                                             ).to(device)\n",
    "    #                         model_manager = ClassifierModelManager(torch_model, lightning_model, log_name=log_name, device=device, num_train_epoch=10)\n",
    "    #                         model_manager.fit(datamodule=data_manager)\n",
    "    #                         model_manager.save_plot_csv_logger(name_prepend=f'{lr}_{dropout}_{wd}_{emb_factor}_{bs}', loss_names=['train_loss', 'val_loss'], eval_names=['train_acc_epoch', 'val_acc_epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fardin Rastakhiz @ 2023\n",
    "import torch\n",
    "# from scripts.managers.ModelManager import ModelManager\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from torch_geometric.nn import summary\n",
    "from lightning.pytorch.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from os import path\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, hinge_loss\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "class ClassifierModelManager(ModelManager):\n",
    "\n",
    "    def __init__(self,\n",
    "                 torch_model: torch.nn.Module,\n",
    "                 lightning_model,\n",
    "                 model_save_dir: str = '~/Desktop',\n",
    "                 log_dir: str = 'logs/',\n",
    "                 log_name: str = 'model_logs',\n",
    "                 device='cpu',\n",
    "                 num_train_epoch = 100,\n",
    "                 accumulate_grad_batches=1):\n",
    "        super(ClassifierModelManager, self).__init__(torch_model, lightning_model, model_save_dir, log_dir, log_name, device, num_train_epoch, accumulate_grad_batches=accumulate_grad_batches)\n",
    "\n",
    "    def _create_callbacks(self) -> List[Callback]:\n",
    "        return [\n",
    "            ModelCheckpoint(save_top_k=2, mode='max', monitor='val_acc', save_last=True),\n",
    "            # EarlyStopping(patience=50, mode='max', monitor='val_acc')\n",
    "        ]\n",
    "\n",
    "    def draw_summary(self, dataloader):\n",
    "        X, y = next(iter(dataloader))\n",
    "        print(summary(self.torch_model, X.to(self.device)))\n",
    "\n",
    "    def plot_csv_logger(self, loss_names=['train_loss', 'val_loss'], eval_names=['train_acc', 'val_acc']):\n",
    "        csv_path = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', 'metrics.csv')\n",
    "        metrics = pd.read_csv(csv_path)\n",
    "\n",
    "        aggregation_metrics = []\n",
    "        agg_col = 'epoch'\n",
    "        for i, dfg in metrics.groupby(agg_col):\n",
    "            agg = dict(dfg.mean())\n",
    "            agg[agg_col] = i\n",
    "            aggregation_metrics.append(agg)\n",
    "\n",
    "        df_metrics = pd.DataFrame(aggregation_metrics)\n",
    "        df_metrics[loss_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='loss')\n",
    "        df_metrics[eval_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='accuracy')\n",
    "        plt.show()\n",
    "\n",
    "    def save_plot_csv_logger(self, loss_names=['train_loss', 'val_loss'], eval_names=['train_acc', 'val_acc'], name_prepend: str=\"\"):\n",
    "        csv_path = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', 'metrics.csv')\n",
    "        metrics = pd.read_csv(csv_path)\n",
    "\n",
    "        aggregation_metrics = []\n",
    "        agg_col = 'epoch'\n",
    "        for i, dfg in metrics.groupby(agg_col):\n",
    "            agg = dict(dfg.mean())\n",
    "            agg[agg_col] = i\n",
    "            aggregation_metrics.append(agg)\n",
    "\n",
    "        df_metrics = pd.DataFrame(aggregation_metrics)\n",
    "        df_metrics[loss_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='loss')\n",
    "        \n",
    "        loss_png = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', f'{name_prepend}_loss_metric.png')\n",
    "        plt.savefig(loss_png)\n",
    "        \n",
    "        df_metrics[eval_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='accuracy')\n",
    "        \n",
    "        acc_png = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', f'{name_prepend}_acc_metric.png')\n",
    "        plt.savefig(acc_png)\n",
    "        \n",
    "        plt.close()\n",
    "    \n",
    "    def evaluate(self, eval_dataloader,\n",
    "                 give_confusion_matrix: bool=True, \n",
    "                 give_report: bool=True, \n",
    "                 give_f1_score: bool=False, \n",
    "                 give_accuracy_score: bool=False, \n",
    "                 give_precision_score: bool=False, \n",
    "                 give_recall_score: bool=False, \n",
    "                 give_hinge_loss: bool=False):\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        self.lightning_model.eval()\n",
    "        for X, y in eval_dataloader:\n",
    "            y_p = self.lightning_model(X.to(self.device))\n",
    "            if type(y_p) is tuple:\n",
    "                y_p = y_p[0]\n",
    "            y_pred.append((y_p>0).to(torch.int32).detach().to(y.device))\n",
    "            y_true.append(y.to(torch.int32))\n",
    "        y_true = torch.concat(y_true)\n",
    "        y_pred = torch.concat(y_pred)\n",
    "        if(give_confusion_matrix):\n",
    "            print(f'confusion_matrix: \\n{confusion_matrix(y_true, y_pred)}')\n",
    "        if(give_report):\n",
    "            print(classification_report(y_true, y_pred))\n",
    "        if(give_f1_score):\n",
    "            print(f'f1_score: {f1_score(y_true, y_pred)}')\n",
    "        if(give_accuracy_score):\n",
    "            print(f'accuracy_score: {accuracy_score(y_true, y_pred)}')\n",
    "        if(give_precision_score):\n",
    "            print(f'precision_score: {precision_score(y_true, y_pred)}')\n",
    "        if(give_recall_score):\n",
    "            print(f'recall_score: {recall_score(y_true, y_pred)}')\n",
    "        # if(give_hinge_loss):\n",
    "        #     print(f'hinge_loss: {hinge_loss(y_true, y_pred)}')\n",
    "                \n",
    "    def evaluate_best_models(self, lightning_type: L.LightningModule, eval_dataloader,\n",
    "                             give_confusion_matrix: bool=True, \n",
    "                             give_report: bool=True, \n",
    "                             give_f1_score: bool=False, \n",
    "                             give_accuracy_score: bool=False, \n",
    "                             give_precision_score: bool=False, \n",
    "                             give_recall_score: bool=False, \n",
    "                             give_hinge_loss: bool=False,\n",
    "                             multi_class: bool=False, **kwargs):\n",
    "        self.lightning_model = lightning_type.load_from_checkpoint(rf'{self.trainer.checkpoint_callback.best_model_path}', map_location=None, hparams_file=None, strict=True, **kwargs).eval()\n",
    "        self.save_evaluation(eval_dataloader, 'best_model', give_confusion_matrix, give_report,\n",
    "                             give_f1_score, give_accuracy_score, give_precision_score, give_recall_score, give_hinge_loss, multi_class)\n",
    "            \n",
    "    def save_evaluation(self, eval_dataloader, name_prepend: str='',\n",
    "                    give_confusion_matrix: bool=True, \n",
    "                    give_report: bool=True, \n",
    "                    give_f1_score: bool=False, \n",
    "                    give_accuracy_score: bool=False, \n",
    "                    give_precision_score: bool=False, \n",
    "                    give_recall_score: bool=False, \n",
    "                    give_hinge_loss: bool=False,\n",
    "                    multi_class: bool=False\n",
    "                    ):\n",
    "            \n",
    "            test_metrics_path = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', f'{name_prepend}_test_metrics.txt')\n",
    "            \n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            self.lightning_model.eval()\n",
    "            self.lightning_model.model.eval()\n",
    "            self.torch_model.eval()\n",
    "            self.trainer.model.eval()\n",
    "            for X, y in eval_dataloader:\n",
    "                with torch.no_grad():\n",
    "                    y_p = self.lightning_model(X.to(self.device))\n",
    "                if type(y_p) is tuple:\n",
    "                    y_p = y_p[0]\n",
    "                \n",
    "                if multi_class:\n",
    "                    y_pred.append(y_p.detach().to(y.device))\n",
    "                    y_true.append(y)\n",
    "                else:\n",
    "                    y_pred.append((y_p>0).to(torch.int32).detach().to(y.device))\n",
    "                    y_true.append(y.to(torch.int32))\n",
    "                    \n",
    "            y_true = torch.concat(y_true)\n",
    "            y_pred = torch.concat(y_pred)\n",
    "            print(y_true.shape)\n",
    "            print(y_pred.shape)\n",
    "            if multi_class:\n",
    "                y_true_num = torch.argmax(y_true, dim=1)\n",
    "                y_pred_num = torch.argmax(y_pred, dim=1)\n",
    "            else:\n",
    "                y_true_num = y_true\n",
    "                y_pred_num = y_pred\n",
    "                \n",
    "            print(y_true_num.shape)\n",
    "            print(y_pred_num.shape)\n",
    "            with open(test_metrics_path, 'at+') as f:\n",
    "                if(give_confusion_matrix):\n",
    "                    print(f'confusion_matrix: \\n{confusion_matrix(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_report):\n",
    "                    print(classification_report(y_true_num, y_pred_num), file=f)\n",
    "                if(give_f1_score):\n",
    "                    if multi_class:\n",
    "                        print(f'f1_score: {f1_score(y_true_num, y_pred_num, average=None)}', file=f)\n",
    "                    else:\n",
    "                        print(f'f1_score: {f1_score(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_accuracy_score):\n",
    "                    print(f'accuracy_score: {accuracy_score(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_precision_score):\n",
    "                    if multi_class:\n",
    "                        print(f'precision: {precision_score(y_true_num, y_pred_num, average=None)}', file=f)\n",
    "                    else:\n",
    "                        print(f'precision: {precision_score(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_recall_score):\n",
    "                    if multi_class:\n",
    "                        print(f'recall: {recall_score(y_true_num, y_pred_num, average=None)}', file=f)\n",
    "                    else:\n",
    "                        print(f'recall: {recall_score(y_true_num, y_pred_num)}', file=f)\n",
    "                # if(give_hinge_loss):\n",
    "                #     print(f'hinge_loss: {hinge_loss(y_true_num, y_pred)}', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 128\n",
    "hidden_dim = 64\n",
    "embedding_dim = 64\n",
    "label_size = 1\n",
    "seed = 911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_size = 32\n",
    "# hidden_dim = 16\n",
    "# embedding_dim = 16\n",
    "# label_size = 1\n",
    "# seed = 911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "def train_model(epochs=30, dropout=0.25, weight_decay=0.000012, lr=0.0002, amsgrad=False, fused=True, use_token_polarity=[False, True, True]):\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    classifier_torch_model = CNN_for_Text(num_embedding=num_embedding, hidden_dim=hidden_dim, embedding_dim=embedding_dim, pos_emb_size=4096, dropout=dropout, num_out_features=len(class_id), seed=seed, random_edges=4, lattice_edges=8, lattice_step=2, virtual_nodes=0, lattice_start_distance=2, use_token_polarity=use_token_polarity).to(device)\n",
    "    # optimizer = torch.optim.Adam(classifier_torch_model.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=amsgrad, fused=fused)\n",
    "    optimizer = torch.optim.AdamW(classifier_torch_model.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=amsgrad, fused=fused)\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 100, 150, 200, 250, 300, 350],gamma=0.5, verbose=False)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15, 20, 30, 40, 45,50,55],gamma=0.5, verbose=False)\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    classfier_lightning_model = CnnGnnClassifierLightningModel(classifier_torch_model, \n",
    "                                                        num_classes=len(class_id),\n",
    "                                                learning_rate=lr,\n",
    "                                                batch_size=batch_size,\n",
    "                                                optimizer=optimizer,\n",
    "                                                loss_func=loss_func,\n",
    "                                                lr_scheduler=lr_scheduler,\n",
    "                                                user_lr_scheduler=True\n",
    "                                                ).to(device)\n",
    "\n",
    "\n",
    "    model_manager = ClassifierModelManager(classifier_torch_model, classfier_lightning_model, log_name=f'CNN-GNN_{use_token_polarity[0]}_{use_token_polarity[1]}_{use_token_polarity[2]}',device=device, num_train_epoch=epochs, accumulate_grad_batches=1)\n",
    "    # trainer = L.Trainer(\n",
    "    #             # callbacks=callbacks,\n",
    "    #             max_epochs=epochs,\n",
    "    #             accelerator= 'gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    #             logger=CSVLogger(save_dir='logs/', name='log2'), \n",
    "    #             num_sanity_val_steps=0,\n",
    "    #         #     default_root_dir='models\\model2_word_embedding-256-2'\n",
    "    #         )\n",
    "\n",
    "    train_dataset.reset_params()\n",
    "    train_dataset.position_j = 0\n",
    "    test_dataset.reset_params()\n",
    "    test_dataset.position_j = 0\n",
    "    \n",
    "    # train_dataset.section_i = 0\n",
    "    # train_dataset.each_section_i = np.zeros((train_dataset.num_sections, ), dtype=int)\n",
    "    # test_dataset.section_i = 0\n",
    "    # test_dataset.each_section_i = np.zeros((test_dataset.num_sections, ), dtype=int)\n",
    "    \n",
    "    model_manager.fit(train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)\n",
    "    model_manager.save_plot_csv_logger(loss_names=['train_loss_epoch', 'val_loss_epoch'], eval_names=['train_acc_epoch', 'val_acc_epoch'], name_prepend=f'tests_{dropout}_{weight_decay}_{lr}_{amsgrad}_{fused}')\n",
    "    model_manager.torch_model = model_manager.torch_model.to(device)\n",
    "    model_manager.save_evaluation(test_dataloader, f'{dropout}_{weight_decay}_{lr}]',True, True, True, True, True, True, True, multi_class=True)\n",
    "    # trainer.fit(classfier_lightning_model, train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)\n",
    "    classfier_lightning_model.model = classfier_lightning_model.model.eval()\n",
    "    classfier_lightning_model = classfier_lightning_model.eval()\n",
    "    calculate_metrics(classfier_lightning_model, test_dataloader)\n",
    "    model_manager.evaluate_best_models(CnnGnnClassifierLightningModel, test_dataloader,True, True, True, True, True, True, True, multi_class=True, model=classifier_torch_model, num_classes=len(class_id))\n",
    "    return model_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True\n",
      "True True False\n",
      "True False True\n",
      "True False False\n",
      "False True False\n",
      "False False True\n",
      "False False False\n"
     ]
    }
   ],
   "source": [
    "for sp1 in [True, False]:\n",
    "    for sp2 in [True, False]:\n",
    "        for sp3 in [True, False]:\n",
    "            if sp1==False and sp2==True and sp3==True:\n",
    "                continue\n",
    "            print(sp1, sp2, sp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7, 10):\n",
    "    train_dataset.set_active_fold(i)\n",
    "    test_dataset.set_active_fold(i)\n",
    "    # max_token_count = max(train_dataset.max_token_count, test_dataset.max_token_count)\n",
    "    train_dataloader = CharacterandTokenLevelDataLoader(train_dataset, batch_size=batch_size, drop_last=False, shuffle=False)\n",
    "    test_dataloader = CharacterandTokenLevelDataLoader(test_dataset, batch_size=batch_size, drop_last=False, shuffle=False)\n",
    "    # for sp1 in [True, False]:\n",
    "    #     for sp2 in [True, False]:\n",
    "    #         for sp3 in [True, False]:\n",
    "    #             if sp1==False and sp2==True and sp3==True:\n",
    "    #                 continue\n",
    "    for sp1, sp2, sp3  in [[True, True, False], [False, True, False]]:\n",
    "        for j in range(3):\n",
    "            model_manager = train_model(70, 0.2, 0.000012, 0.0032, use_token_polarity=[sp1, sp2, sp3])\n",
    "            time.sleep(20)\n",
    "            torch.cuda.empty_cache()\n",
    "            import gc\n",
    "            gc.collect()\n",
    "            time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculatge_metrics(chpt_path, target_data_loader, use_token_polarity=[False, False, False]):\n",
    "        classifier_torch_model = CNN_for_Text(num_embedding=num_embedding, hidden_dim=hidden_dim, embedding_dim=embedding_dim, pos_emb_size=4096, dropout=0.2, num_out_features=len(class_id), seed=seed, random_edges=4, lattice_edges=8, lattice_step=2, virtual_nodes=0, lattice_start_distance=2, use_token_polarity=use_token_polarity)\n",
    "        classfier_lightning_model = CnnGnnClassifierLightningModel.load_from_checkpoint(chpt_path, model=classifier_torch_model, num_classes=len(class_id)).eval()\n",
    "        mean_infer_acc = []\n",
    "        mean_infer_f1 = []\n",
    "        mean_infer_prec = []\n",
    "        mean_infer_rec = []\n",
    "        for i in range(5):\n",
    "            all_ys = []\n",
    "            all_y_preds = []\n",
    "            for X, y in target_data_loader:\n",
    "                with torch.no_grad():\n",
    "                    y_pred = classfier_lightning_model(X.to(device))\n",
    "                all_ys.append(torch.argmax(y,dim=1))\n",
    "                all_y_preds.append(torch.argmax(y_pred.cpu(), dim=1))\n",
    "            all_ys = torch.concat(all_ys)\n",
    "            all_y_preds = torch.concat(all_y_preds)\n",
    "            \n",
    "            cm = confusion_matrix(all_ys, all_y_preds)\n",
    "            \n",
    "            accuracy = np.sum(np.diag(cm))/ np.sum(cm)\n",
    "            precision = np.mean(np.diag(cm) / np.sum(cm, axis=0))\n",
    "            recall = np.mean(np.diag(cm) / np.sum(cm, axis=1))\n",
    "            f1_score = (2*precision*recall)/(precision + recall)\n",
    "            \n",
    "            mean_infer_acc.append(accuracy)\n",
    "            mean_infer_f1.append(f1_score)\n",
    "            mean_infer_prec.append(precision)\n",
    "            mean_infer_rec.append(recall)\n",
    "        mean_infer_acc = torch.mean(torch.tensor(mean_infer_acc))\n",
    "        mean_infer_f1 = torch.mean(torch.tensor(mean_infer_f1))\n",
    "        mean_infer_prec = torch.mean(torch.tensor(mean_infer_prec))\n",
    "        mean_infer_rec = torch.mean(torch.tensor(mean_infer_rec))\n",
    "        return mean_infer_acc, mean_infer_f1, mean_infer_prec, mean_infer_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "def get_best_chpt(metrics_path, epoch_numbers):\n",
    "    epoch_data = pd.read_csv(metrics_path)\n",
    "    if 'val_acc_epoch' in epoch_data.columns and epoch_data['val_acc_epoch'].notna().any():\n",
    "        best_chpt = epoch_data.loc[epoch_data['val_acc_epoch'].idxmax()]\n",
    "    elif 'val_loss_epoch' in epoch_data.columns and epoch_data['val_loss_epoch'].notna().any():\n",
    "        best_chpt = epoch_data.loc[epoch_data['val_loss_epoch'].idxmin()]\n",
    "    else:\n",
    "        raise ValueError(f\"No valid validation metrics available for epoch {epoch_numbers}.\")\n",
    "    return np.argwhere(np.array(epoch_numbers)==best_chpt['epoch']).item(), best_chpt['val_loss_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_metrics_mean(base_path = 'logs\\CNN-GNN18_mr2k_seeds', use_token_polarity=[False, False, False]):\n",
    "    total_accuracy = []\n",
    "    total_f1 = []\n",
    "    total_prec = []\n",
    "    total_rec = []\n",
    "    total_loss = []\n",
    "\n",
    "    for k_fold_num in range(10):\n",
    "        infer_acc, infer_f1, infer_prec, infer_rec, infer_loss = [], [], [], [], []\n",
    "        for j in range(3):\n",
    "            i = k_fold_num*3 + j\n",
    "            # k_fold_num = i//3\n",
    "            test_dataset.set_active_fold(k_fold_num)\n",
    "            test_dataloader = CharacterandTokenLevelDataLoader(test_dataset, batch_size=batch_size, drop_last=False, shuffle=False)\n",
    "            \n",
    "            version_path = join(base_path, f'version_{i}')\n",
    "            checkpoint_path = join(version_path, f'checkpoints')\n",
    "            onlyfiles  = [f for f in listdir(checkpoint_path) if (isfile(join(checkpoint_path, f)) and 'epoch' in f) ]\n",
    "            epoch_numbers = [int(re.search(r'\\d+', f).group()) for f in onlyfiles]\n",
    "            best_chpt_id, loss = get_best_chpt(join(version_path, 'metrics.csv'), epoch_numbers)\n",
    "            print(onlyfiles[best_chpt_id])\n",
    "            mean_infer_acc, mean_infer_f1, mean_infer_prec, mean_infer_rec = calculatge_metrics(join(checkpoint_path, f'{onlyfiles[best_chpt_id]}'), test_dataloader, use_token_polarity=use_token_polarity)\n",
    "                \n",
    "            infer_acc.append(mean_infer_acc)\n",
    "            infer_f1.append(mean_infer_f1)\n",
    "            infer_prec.append(mean_infer_prec)\n",
    "            infer_rec.append(mean_infer_rec)\n",
    "            infer_loss.append(loss)\n",
    "\n",
    "        total_accuracy.append(torch.mean(torch.tensor(infer_acc)))\n",
    "        total_f1.append(torch.mean(torch.tensor(infer_f1)))\n",
    "        total_prec.append(torch.mean(torch.tensor(infer_prec)))\n",
    "        total_rec.append(torch.mean(torch.tensor(infer_rec)))\n",
    "        total_loss.append(torch.mean(torch.tensor(infer_loss)))\n",
    "\n",
    "    total_accuracy = torch.mean(torch.tensor(total_accuracy))\n",
    "    total_f1 = torch.mean(torch.tensor(total_f1))\n",
    "    total_prec = torch.mean(torch.tensor(total_prec))\n",
    "    total_rec = torch.mean(torch.tensor(total_rec))\n",
    "    total_loss = torch.mean(torch.tensor(total_loss))\n",
    "    print(f'total_accuracy: {total_accuracy}')\n",
    "    print(f'total_f1: {total_f1}')\n",
    "    print(f'total_prec: {total_prec}')\n",
    "    print(f'total_rec: {total_rec}')\n",
    "    print(f'total_loss: {total_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_metrics_max(base_path = 'logs\\CNN-GNN18_mr2k_seeds', use_token_polarity=[False, False, False]):\n",
    "    total_accuracy = []\n",
    "    total_f1 = []\n",
    "    total_prec = []\n",
    "    total_rec = []\n",
    "    total_loss = []\n",
    "\n",
    "    for k_fold_num in range(10):\n",
    "        infer_acc, infer_f1, infer_prec, infer_rec, infer_loss = [], [], [], [], []\n",
    "        for j in range(3):\n",
    "            i = k_fold_num*3 + j\n",
    "            # k_fold_num = i//3\n",
    "            test_dataset.set_active_fold(k_fold_num)\n",
    "            test_dataloader = CharacterandTokenLevelDataLoader(test_dataset, batch_size=batch_size, drop_last=False, shuffle=False)\n",
    "            \n",
    "            version_path = join(base_path, f'version_{i}')\n",
    "            checkpoint_path = join(version_path, f'checkpoints')\n",
    "            onlyfiles  = [f for f in listdir(checkpoint_path) if (isfile(join(checkpoint_path, f)) and 'epoch' in f) ]\n",
    "            epoch_numbers = [int(re.search(r'\\d+', f).group()) for f in onlyfiles]\n",
    "            best_chpt_id, loss = get_best_chpt(join(version_path, 'metrics.csv'), epoch_numbers)\n",
    "            print(onlyfiles[best_chpt_id])\n",
    "            mean_infer_acc, mean_infer_f1, mean_infer_prec, mean_infer_rec = calculatge_metrics(join(checkpoint_path, f'{onlyfiles[best_chpt_id]}'), test_dataloader, use_token_polarity=use_token_polarity)\n",
    "                \n",
    "            infer_acc.append(mean_infer_acc)\n",
    "            infer_f1.append(mean_infer_f1)\n",
    "            infer_prec.append(mean_infer_prec)\n",
    "            infer_rec.append(mean_infer_rec)\n",
    "            infer_loss.append(loss)\n",
    "\n",
    "        total_accuracy.append(torch.max(torch.tensor(infer_acc)))\n",
    "        total_f1.append(torch.max(torch.tensor(infer_f1)))\n",
    "        total_prec.append(torch.max(torch.tensor(infer_prec)))\n",
    "        total_rec.append(torch.max(torch.tensor(infer_rec)))\n",
    "        total_loss.append(torch.min(torch.tensor(infer_loss)))\n",
    "\n",
    "    total_accuracy = torch.mean(torch.tensor(total_accuracy))\n",
    "    total_f1 = torch.mean(torch.tensor(total_f1))\n",
    "    total_prec = torch.mean(torch.tensor(total_prec))\n",
    "    total_rec = torch.mean(torch.tensor(total_rec))\n",
    "    total_loss = torch.mean(torch.tensor(total_loss))\n",
    "    print(f'total_accuracy: {total_accuracy}')\n",
    "    print(f'total_f1: {total_f1}')\n",
    "    print(f'total_prec: {total_prec}')\n",
    "    print(f'total_rec: {total_rec}')\n",
    "    print(f'total_loss: {total_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def give_average_metrics_max(base_path = 'logs\\CNN-GNN18_mr2k_seeds', use_token_polarity=[False, False, False]):\n",
    "    total_accuracy = []\n",
    "    total_f1 = []\n",
    "    total_prec = []\n",
    "    total_rec = []\n",
    "    total_loss = []\n",
    "\n",
    "    for k_fold_num in range(10):\n",
    "        infer_acc, infer_f1, infer_prec, infer_rec, infer_loss = [], [], [], [], []\n",
    "        for j in range(3):\n",
    "            i = k_fold_num*3 + j\n",
    "            # k_fold_num = i//3\n",
    "            test_dataset.set_active_fold(k_fold_num)\n",
    "            \n",
    "            version_path = join(base_path, f'version_{i}')\n",
    "            # checkpoint_path = join(version_path, f'checkpoints')\n",
    "            # onlyfiles  = [f for f in listdir(checkpoint_path) if (isfile(join(checkpoint_path, f)) and 'epoch' in f) ]\n",
    "            # epoch_numbers = [int(re.search(r'\\d+', f).group()) for f in onlyfiles]\n",
    "            # best_chpt_id, loss = get_best_chpt(join(version_path, 'metrics.csv'), epoch_numbers)\n",
    "            # print(onlyfiles[best_chpt_id])\n",
    "            # print(version_path)\n",
    "            with open(path.join(rf'{version_path}', 'best_model_test_metrics.txt'), 'rt') as f:\n",
    "                all_lines = f.readlines()\n",
    "                for l in all_lines:\n",
    "                    if 'precision' in l and 'recall' in l: \n",
    "                        continue\n",
    "                    \n",
    "                    if 'f1_score' in l:\n",
    "                        f1_score = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", l)\n",
    "                        f1_score = [float(fl) for fl in f1_score]\n",
    "                        f1_score = np.array(f1_score).mean()\n",
    "                    elif 'accuracy_score' in l:\n",
    "                        accuracy_score = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", l)\n",
    "                        accuracy_score = [float(acc) for acc in accuracy_score]\n",
    "                        accuracy_score = np.array(accuracy_score).mean()\n",
    "                    elif 'precision' in l:\n",
    "                        precision = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", l)\n",
    "                        precision = [float(pr) for pr in precision]\n",
    "                        precision = np.array(precision).mean()\n",
    "                    elif 'recall' in l:\n",
    "                        recall = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", l)\n",
    "                        recall = [float(rc) for rc in recall]\n",
    "                        recall = np.array(recall).mean()\n",
    "                \n",
    "            infer_acc.append(accuracy_score)\n",
    "            infer_f1.append(f1_score)\n",
    "            infer_prec.append(precision)\n",
    "            infer_rec.append(recall)\n",
    "\n",
    "        total_accuracy.append(torch.max(torch.tensor(infer_acc)))\n",
    "        total_f1.append(torch.max(torch.tensor(infer_f1)))\n",
    "        total_prec.append(torch.max(torch.tensor(infer_prec)))\n",
    "        total_rec.append(torch.max(torch.tensor(infer_rec)))\n",
    "\n",
    "    total_accuracy = torch.mean(torch.tensor(total_accuracy))\n",
    "    total_f1 = torch.mean(torch.tensor(total_f1))\n",
    "    total_prec = torch.mean(torch.tensor(total_prec))\n",
    "    total_rec = torch.mean(torch.tensor(total_rec))\n",
    "    # total_loss = torch.mean(torch.tensor(total_loss))\n",
    "    print(f'total_accuracy: {total_accuracy}')\n",
    "    print(f'total_f1: {total_f1}')\n",
    "    print(f'total_prec: {total_prec}')\n",
    "    print(f'total_rec: {total_rec}')\n",
    "    # print(f'total_loss: {total_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_average_metrics_mean(r'logs\\CNN-GNN_True_True_True', [True, True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_accuracy: 0.8805\n",
      "total_f1: 0.9196889123333332\n",
      "total_prec: 0.8845566805\n",
      "total_rec: 0.8792429759999999\n",
      "total_loss: nan\n"
     ]
    }
   ],
   "source": [
    "give_average_metrics_max(r'logs\\CNN-GNN_True_True_False', [True, True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_average_metrics_mean(r'logs\\CNN-GNN_True_False_True', [True, False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_average_metrics_mean(r'logs\\CNN-GNN_True_False_False', [True, False, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_accuracy: 0.8720000000000001\n",
      "total_f1: 0.9141242829999999\n",
      "total_prec: 0.8752973655\n",
      "total_rec: 0.8713720589999999\n",
      "total_loss: nan\n"
     ]
    }
   ],
   "source": [
    "give_average_metrics_max(r'logs\\CNN-GNN_False_True_True', [False, True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_accuracy: 0.8775000000000001\n",
      "total_f1: 0.9180739506666665\n",
      "total_prec: 0.879140324\n",
      "total_rec: 0.8778226865000001\n",
      "total_loss: nan\n"
     ]
    }
   ],
   "source": [
    "give_average_metrics_max(r'logs\\CNN-GNN_False_True_False', [False, True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_average_metrics_mean(r'logs\\CNN-GNN_False_False_True', [False, False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_average_metrics_mean(r'logs\\CNN-GNN_False_False_False', [False, False, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.91977188081937"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=85.78\n",
    "r=86.06\n",
    "2*p*r/(p+r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
