{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dependencies\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Articles\\CGNet\\venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and prepare tokens vocabulary\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "id_vocab = {v:k for k,v in tokenizer.vocab.items()}\n",
    "all_vocab_str = []\n",
    "for i in range(len(id_vocab)):\n",
    "    all_vocab_str.append(id_vocab[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the polarity and subjectivity of tokens from spacy text blob\n",
    "# all_vocab_polarity = []\n",
    "# all_vocab_subjectivity = []\n",
    "# nlp = spacy.load('en_core_web_sm', disable=['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
    "# nlp.add_pipe('spacytextblob')\n",
    "# for i in range(len(all_vocab_str)):\n",
    "#     text = all_vocab_str[i]\n",
    "#     doc = nlp(text)\n",
    "#     polarity = 0\n",
    "#     subjectivity = 0\n",
    "#     for t in doc:\n",
    "#         polarity+=t._.blob.polarity\n",
    "#         subjectivity+=t._.blob.subjectivity\n",
    "#     all_vocab_polarity.append(polarity)\n",
    "#     all_vocab_subjectivity.append(subjectivity)\n",
    "# all_vocab_polarity = np.array(all_vocab_polarity)\n",
    "# all_vocab_subjectivity = np.array(all_vocab_subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creaete batch requests for OpenAI chat, to get subjectivity and polarity of tokens\n",
    "temp_str = \"\"\" the texts: \"\"\"\n",
    "def create_batch_request_for_openai(tokens_list):\n",
    "    batch_line = temp_str\n",
    "    batch_line += tokens_list[0]\n",
    "    for t in tokens_list[1:]:\n",
    "        batch_line += ' | '\n",
    "        batch_line += t\n",
    "    return batch_line\n",
    "lines = {}\n",
    "interval = 50\n",
    "for i in range(0, len(all_vocab_str), interval):\n",
    "    upper_bound = min(i+interval, len(all_vocab_str))\n",
    "    lines[(i, upper_bound)] = create_batch_request_for_openai(all_vocab_str[i:upper_bound])\n",
    "\n",
    "jsonl_lines = []\n",
    "for k, v in lines.items():\n",
    "    # print(k, len(v.split('|')))\n",
    "    system_prompt = \"\"\"As sentiment classifier, write polarity and subjectivity for the provided texts. For example for the given text 'good' write 'good: polarity=0.7, subjectivity=0.6' and for the text 'bad' write 'bad: polarity=-0.7, subjectivity=0.67' and for the given text 'good | bad' write 'good: polarity=0.7, subjectivity=0.6 | bad: polarity=-0.7, subjectivity=0.67'. I will give you exactly {t_count} texts and you just write the polarity and sensitivity without any other words for exact same {t_count} texts.\"\"\"\n",
    "    jsonl_lines.append({\"custom_id\": f\"{k}\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-4o-mini\", \"messages\": [{\"role\": \"system\", \"content\": system_prompt.format(t_count=len(v.split('|')))},{\"role\": \"user\", \"content\": v}],\"max_tokens\": 1000}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the batch requests into '.jsonl' files\n",
    "with open(r'DataManipulation\\tokens_polarity.jsonl', 'w', encoding=\"utf-8\") as f:\n",
    "    for i in range(len(jsonl_lines)):\n",
    "        f.write(json.dumps(jsonl_lines[i])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read to response to batch requests\n",
    "with open(r'DataManipulation\\token_polarity_mini_2_batch_674650e577508190b2e501b187ce6ed7_output.jsonl', 'r') as f:\n",
    "    all_lines = []\n",
    "    line_stream = f.readlines()\n",
    "    for line in line_stream:\n",
    "        all_lines.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed: [' ']\n",
      "failed: ['ATFORM ']\n"
     ]
    }
   ],
   "source": [
    "# Take the polarity and sensitivy from the responses\n",
    "def get_string_ints(string_sample):\n",
    "    str_arr = re.findall(r'\\d+', string_sample)\n",
    "    int_arr = [int(s) for s in str_arr]\n",
    "    return int_arr\n",
    "\n",
    "def get_string_floats(string_sample):\n",
    "    all_floats = re.findall(r\"[-+]?(?:\\d*\\.*\\d+)\", string_sample)\n",
    "    all_floats = [float(f) for f in all_floats]\n",
    "    return all_floats\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def map_content_to_tokens(content, tokens):\n",
    "    \n",
    "    content_dict = dict()\n",
    "    t_c_dict = dict()\n",
    "    content_keys = []\n",
    "    for c in content:\n",
    "        c = c.split(':')\n",
    "        if len(c) == 3:\n",
    "            c= [':', c[2]]\n",
    "        elif len(c) ==0:\n",
    "            continue\n",
    "        elif len(c)==1 or len(c) >3:\n",
    "            print(f\"failed: {c}\")\n",
    "            continue\n",
    "        # print(f'cccc: {c}')\n",
    "        content_dict[c[0]] = c[1]\n",
    "        content_keys.append(c[0])\n",
    "    tokens_ = copy(tokens)\n",
    "    for c in content_keys:\n",
    "        most_similar_token = tokens_[0]\n",
    "        base_sim = similar(c, most_similar_token)\n",
    "        for t in tokens_[1:]:\n",
    "            new_sim = similar(c, t)\n",
    "            if new_sim > base_sim:\n",
    "                base_sim = new_sim\n",
    "                most_similar_token = t\n",
    "        t_c_dict[most_similar_token] = c\n",
    "        tokens_.remove(most_similar_token)\n",
    "    return t_c_dict, content_dict\n",
    "\n",
    "all_vocab_sentiments = dict()\n",
    "k = 0\n",
    "for j in range(len(all_lines)):\n",
    "    ids_range = get_string_ints(all_lines[j]['custom_id'])\n",
    "    content = all_lines[j]['response']['body']['choices'][0]['message']['content']\n",
    "    content = str.split(content, '|')\n",
    "    # print(ids_range)\n",
    "    # print(len(content), content)\n",
    "    list_to_remove = []\n",
    "    for i in range(len(content)):\n",
    "        if content[i].strip() =='':\n",
    "            list_to_remove.append(i)\n",
    "    for ltr in list_to_remove:\n",
    "        del content[ltr]\n",
    "    t_c_dict, content_dict = map_content_to_tokens(content, all_vocab_str[ids_range[0]: ids_range[1]])\n",
    "            \n",
    "    for i in range(ids_range[0], ids_range[1]):\n",
    "        \n",
    "        if all_vocab_str[i] not in t_c_dict:\n",
    "            k += 1\n",
    "            # print(k, 'failed', i, t, all_vocab_str[i])\n",
    "            all_vocab_sentiments[all_vocab_str[i]] = [0, 0]\n",
    "            continue\n",
    "        \n",
    "        # print(i, all_vocab_str[i], t_c_dict[all_vocab_str[i]], content_dict[t_c_dict[all_vocab_str[i]]])\n",
    "        content_info = content_dict[t_c_dict[all_vocab_str[i]]].split(',')\n",
    "        content_info = [t.strip() for t in content_info]\n",
    "        content_info = sorted(content_info)\n",
    "        polarity = get_string_floats(content_info[0])\n",
    "        # print(content_info)\n",
    "        subjectivity = get_string_floats(content_info[1])\n",
    "        all_vocab_sentiments[all_vocab_str[i]]  = [polarity, subjectivity]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128001, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the token's subjectivity and polarity into a tensor with same order as tokens\n",
    "vocab_sentiments = []\n",
    "for i in range(len(all_vocab_str)):\n",
    "    vocab_sentiments.append(np.array(all_vocab_sentiments[all_vocab_str[i]]).squeeze())\n",
    "vocab_sentiments = np.vstack(vocab_sentiments)\n",
    "vocab_sentiments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sentiment data extracted from GPT\n",
    "np.save(r'Data\\ReducedEmbeddings\\polarity_debertav3_tokens_gpt_mini_emb.npy', vocab_sentiments, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
