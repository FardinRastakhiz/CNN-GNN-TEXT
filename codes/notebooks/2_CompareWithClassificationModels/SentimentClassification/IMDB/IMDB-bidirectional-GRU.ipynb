{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-16T02:25:06.224658Z",
     "iopub.status.busy": "2025-06-16T02:25:06.224500Z",
     "iopub.status.idle": "2025-06-16T02:25:08.306558Z",
     "shell.execute_reply": "2025-06-16T02:25:08.305896Z",
     "shell.execute_reply.started": "2025-06-16T02:25:06.224643Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from nltk import pos_tag\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T02:25:10.622763Z",
     "iopub.status.busy": "2025-06-16T02:25:10.622356Z",
     "iopub.status.idle": "2025-06-16T02:25:10.627896Z",
     "shell.execute_reply": "2025-06-16T02:25:10.627132Z",
     "shell.execute_reply.started": "2025-06-16T02:25:10.622736Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "stem = PorterStemmer()\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^a-zA-Z0-9]',' ',text)\n",
    "    text = text.split()\n",
    "    text = [ i for i in text if i not in stopwords.words('english')]\n",
    "    text = [stem.stem(i) for i in text]\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T02:25:10.628952Z",
     "iopub.status.busy": "2025-06-16T02:25:10.628653Z",
     "iopub.status.idle": "2025-06-16T02:31:45.476813Z",
     "shell.execute_reply": "2025-06-16T02:31:45.476183Z",
     "shell.execute_reply.started": "2025-06-16T02:25:10.628930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# keep_ratio = 1.0\n",
    "# df_test = pd.read_csv(r'data\\TextClassification\\IMDB\\test.csv')\n",
    "# df_test['Topic'] = df_test['label']\n",
    "# df_test['Content'] = df_test['text']\n",
    "# df_test.drop(['label', 'text'], axis=1, inplace=True)\n",
    "# df_test.dropna(inplace=True)\n",
    "# df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
    "# df_test = df_test.iloc[:int(keep_ratio*df_test.shape[0])]\n",
    "# df_train = pd.read_csv(r'data\\TextClassification\\IMDB\\train.csv')\n",
    "# df_train['Topic'] = df_train['label']\n",
    "# df_train['Content'] = df_train['text']\n",
    "# df_train.drop(['label', 'text'], axis=1, inplace=True)\n",
    "# df_train.dropna(inplace=True)\n",
    "# df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "# df_train = df_train.iloc[:int(keep_ratio*df_train.shape[0])]\n",
    "\n",
    "# df_train['clean'] = df_train['Content'].apply(clean)\n",
    "# df_train.to_csv(r\"data\\TextClassification\\IMDB\\train-clean.csv\")\n",
    "\n",
    "# df_test['clean'] = df_test['Content'].apply(clean)\n",
    "# df_test.to_csv(r\"data\\TextClassification\\IMDB\\test-clean.csv\")\n",
    "\n",
    "df_train = pd.read_csv(r\"data\\TextClassification\\IMDB\\train-clean.csv\")\n",
    "df_test = pd.read_csv(r\"data\\TextClassification\\IMDB\\test-clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_classes = [\"Negative\", \"Positive\"]\n",
    "df = pd.concat([df_train, df_test])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T02:25:09.293554Z",
     "iopub.status.busy": "2025-06-16T02:25:09.293333Z",
     "iopub.status.idle": "2025-06-16T02:25:09.298660Z",
     "shell.execute_reply": "2025-06-16T02:25:09.297905Z",
     "shell.execute_reply.started": "2025-06-16T02:25:09.293537Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T02:25:09.218008Z",
     "iopub.status.busy": "2025-06-16T02:25:09.217702Z",
     "iopub.status.idle": "2025-06-16T02:25:09.290546Z",
     "shell.execute_reply": "2025-06-16T02:25:09.289757Z",
     "shell.execute_reply.started": "2025-06-16T02:25:09.217982Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  25000 non-null  int64 \n",
      " 1   Topic       25000 non-null  int64 \n",
      " 2   Content     25000 non-null  object\n",
      " 3   clean       25000 non-null  object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T02:25:09.299791Z",
     "iopub.status.busy": "2025-06-16T02:25:09.299520Z",
     "iopub.status.idle": "2025-06-16T02:25:09.316806Z",
     "shell.execute_reply": "2025-06-16T02:25:09.316070Z",
     "shell.execute_reply.started": "2025-06-16T02:25:09.299766Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Topic\n",
       "1    12500\n",
       "0    12500\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T02:31:59.600977Z",
     "iopub.status.busy": "2025-06-16T02:31:59.600548Z",
     "iopub.status.idle": "2025-06-16T02:31:59.793318Z",
     "shell.execute_reply": "2025-06-16T02:31:59.792751Z",
     "shell.execute_reply.started": "2025-06-16T02:31:59.600960Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1455"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_len1 = df_train['clean'].map(lambda x: len(x.split())).max()\n",
    "sent_len2 = df_test['clean'].map(lambda x: len(x.split())).max()\n",
    "sent_len = max(sent_len1, sent_len2)\n",
    "sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Articles\\CGNet\\venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\", cache_dir=r'cache_dir/tokenizers/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.tensor([tokenizer(doc, padding='max_length', truncation=True, max_length=min(sent_len, 1024)).input_ids for doc in df_train['clean']])\n",
    "train_label = torch.tensor([label for label in df_train['Topic']])\n",
    "train_dataset = TensorDataset(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.tensor([tokenizer(doc, padding='max_length', truncation=True, max_length=min(sent_len, 1024)).input_ids for doc in df_test['clean']])\n",
    "test_label = torch.tensor([label for label in df_test['Topic']])\n",
    "test_dataset = TensorDataset(test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, drop_last=True, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class GRUTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super(GRUTextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.bigru1 = nn.GRU(embed_dim, 128, batch_first=True, bidirectional=True)\n",
    "        self.bigru2 = nn.GRU(128*2, 64, batch_first=True, bidirectional=True)\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc1 = nn.Linear(64*2, 256)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.dropout3 = nn.Dropout(0.25)\n",
    "        self.out = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)                         # [B, L] -> [B, L, E]\n",
    "        x, _ = self.bigru1(x)                          # [B, L, 2*128]\n",
    "        x, _ = self.bigru2(x)                          # [B, L, 2*64]\n",
    "        x = x.permute(0, 2, 1)                         # [B, 2*64, L]\n",
    "        x = self.global_max_pool(x).squeeze(-1)        # [B, 2*64]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        return self.out(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example values (adjust as needed)\n",
    "vocabulary_size = tokenizer.vocab_size\n",
    "embed_size = 256\n",
    "max_len = sent_len\n",
    "num_classes = 4\n",
    "num_epochs = 3\n",
    "lr = 0.0012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 33262532\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model\n",
    "model = GRUTextClassifier(vocabulary_size, embed_size, num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)\n",
    "\n",
    "num_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'number of parameters: {num_total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptflops import get_model_complexity_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module Embedding is treated as a zero-op.\n",
      "Warning: module Dropout is treated as a zero-op.\n",
      "Warning: module GRUTextClassifier is treated as a zero-op.\n",
      "Warning! No positional inputs found for a module, assuming batch size is 1.\n",
      "GRUTextClassifier(\n",
      "  494.53 k, 1.487% Params, 615.41 MMac, 99.970% MACs, \n",
      "  (embedding): Embedding(0, 0.000% Params, 0.0 Mac, 0.000% MACs, 128000, 256)\n",
      "  (bigru1): GRU(296.45 k, 0.891% Params, 433.94 MMac, 70.491% MACs, 256, 128, batch_first=True, bidirectional=True)\n",
      "  (bigru2): GRU(123.65 k, 0.372% Params, 181.21 MMac, 29.437% MACs, 256, 64, batch_first=True, bidirectional=True)\n",
      "  (global_max_pool): AdaptiveMaxPool1d(0, 0.000% Params, 186.24 KMac, 0.030% MACs, output_size=1)\n",
      "  (fc1): Linear(33.02 k, 0.099% Params, 33.02 KMac, 0.005% MACs, in_features=128, out_features=256, bias=True)\n",
      "  (dropout1): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.25, inplace=False)\n",
      "  (fc2): Linear(32.9 k, 0.099% Params, 32.9 KMac, 0.005% MACs, in_features=256, out_features=128, bias=True)\n",
      "  (dropout2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.25, inplace=False)\n",
      "  (fc3): Linear(8.26 k, 0.025% Params, 8.26 KMac, 0.001% MACs, in_features=128, out_features=64, bias=True)\n",
      "  (dropout3): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.25, inplace=False)\n",
      "  (out): Linear(260, 0.001% Params, 260.0 Mac, 0.000% MACs, in_features=64, out_features=4, bias=True)\n",
      ")\n",
      "MACs: 615598084, Params: 33262532\n"
     ]
    }
   ],
   "source": [
    "def input_constructor(input_res):\n",
    "    # input_res is (batch_size, sent_len)\n",
    "    return {'x': torch.ones(input_res, dtype=torch.long)}\n",
    "\n",
    "macs, params = get_model_complexity_info(\n",
    "    model, \n",
    "    (1, sent_len), \n",
    "    as_strings=False, \n",
    "    backend='pytorch', \n",
    "    print_per_layer_stat=True, \n",
    "    verbose=True,\n",
    "    input_constructor=input_constructor\n",
    ")\n",
    "print(f\"MACs: {macs}, Params: {params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/195 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:11<00:00, 16.34it/s]\n",
      "100%|██████████| 195/195 [00:05<00:00, 37.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 125.1410, Val Loss: 76.6097, Val Acc: 0.8258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:11<00:00, 17.05it/s]\n",
      "100%|██████████| 195/195 [00:04<00:00, 45.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 65.4229, Val Loss: 64.0929, Val Acc: 0.8589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:11<00:00, 17.69it/s]\n",
      "100%|██████████| 195/195 [00:04<00:00, 42.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Loss: 46.2216, Val Loss: 61.9259, Val Acc: 0.8750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.00001)\n",
    "# Training Loop\n",
    "model = model.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in tqdm(train_dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)                     # [B, num_classes]\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in tqdm(test_dataloader):\n",
    "            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "            val_outputs = model(val_inputs)\n",
    "            v_loss = loss_fn(val_outputs, val_labels)\n",
    "            val_loss += v_loss.item()\n",
    "            preds = val_outputs.argmax(dim=1)\n",
    "            correct += (preds == val_labels).sum().item()\n",
    "            total += val_labels.size(0)\n",
    "    val_acc = correct / total if total > 0 else 0\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from torchmetrics import ConfusionMatrix\n",
    "\n",
    "def calculate_metrics(cl_model, dataloader, num_classes):\n",
    "    cm = ConfusionMatrix(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    cl_model = cl_model.eval()\n",
    "    cl_model.to(device)\n",
    "    for X, y in tqdm(dataloader):\n",
    "        X = X.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_p = cl_model(X)\n",
    "            y_p = y_p.cpu()\n",
    "        y_pred.append(y_p)\n",
    "        y_true.append(y)\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "    y_true = torch.cat(y_true, dim=0)\n",
    "    y_pred2 = torch.argmax(y_pred, dim=1)\n",
    "    y_true2 = torch.argmax(y_true, dim=1) if len(y_true.shape)>1 else y_true\n",
    "    print(f'classification report: \\n {classification_report(y_true2, y_pred2, digits=4)}')\n",
    "    print(f'confusion matrix:\\n {cm(y_pred2, y_true2)}')\n",
    "    print('================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:04<00:00, 46.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8634    0.8910    0.8770     12481\n",
      "           1     0.8873    0.8590    0.8730     12479\n",
      "\n",
      "    accuracy                         0.8750     24960\n",
      "   macro avg     0.8754    0.8750    0.8750     24960\n",
      "weighted avg     0.8754    0.8750    0.8750     24960\n",
      "\n",
      "confusion matrix:\n",
      " tensor([[11120,  1361,     0,     0],\n",
      "        [ 1759, 10720,     0,     0],\n",
      "        [    0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0]])\n",
      "================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "calculate_metrics(model, test_dataloader, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 612351,
     "sourceId": 1095715,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
