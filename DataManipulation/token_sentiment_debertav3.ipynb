{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_vocab = {v:k for k,v in tokenizer.vocab.items()}\n",
    "all_vocab_str = []\n",
    "for i in range(len(id_vocab)):\n",
    "    all_vocab_str.append(id_vocab[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "all_vocab_polarity = []\n",
    "all_vocab_subjectivity = []\n",
    "nlp = spacy.load('en_core_web_sm', disable=['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
    "nlp.add_pipe('spacytextblob')\n",
    "for i in range(len(all_vocab_str)):\n",
    "    text = all_vocab_str[i]\n",
    "    doc = nlp(text)\n",
    "    polarity = 0\n",
    "    subjectivity = 0\n",
    "    for t in doc:\n",
    "        polarity+=t._.blob.polarity\n",
    "        subjectivity+=t._.blob.subjectivity\n",
    "    all_vocab_polarity.append(polarity)\n",
    "    all_vocab_subjectivity.append(subjectivity)\n",
    "all_vocab_polarity = np.array(all_vocab_polarity)\n",
    "all_vocab_subjectivity = np.array(all_vocab_subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_str = \"\"\" the texts: \"\"\"\n",
    "def create_batch_request_for_openai(tokens_list):\n",
    "    batch_line = temp_str\n",
    "    batch_line += tokens_list[0]\n",
    "    for t in tokens_list[1:]:\n",
    "        batch_line += ' | '\n",
    "        batch_line += t\n",
    "    return batch_line\n",
    "lines = {}\n",
    "interval = 50\n",
    "for i in range(0, len(all_vocab_str), interval):\n",
    "    upper_bound = min(i+interval, len(all_vocab_str))\n",
    "    lines[(i, upper_bound)] = create_batch_request_for_openai(all_vocab_str[i:upper_bound])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 50)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256.002"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(all_vocab_str)/500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_content = \"\"\"As sentiment classifier, write polarity and subjectivity for the provided texts. For example for the given text 'good' write 'good: polarity=0.7, subjectivity=0.6' and for the text 'bad' write 'bad: polarity=-0.7, subjectivity=0.67' and for the given text 'good | bad' write 'good: polarity=0.7, subjectivity=0.6 | bad: polarity=-0.7, subjectivity=0.67'. I will give you exactly {t_count} texts and you just write the polarity and sensitivity without any other words for exact same {t_count} texts.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As sentiment classifier, write polarity and subjectivity for the provided texts. For example for the given text 'good' write 'good: polarity=0.7, subjectivity=0.6' and for the text 'bad' write 'bad: polarity=-0.7, subjectivity=0.67' and for the given text 'good | bad' write 'good: polarity=0.7, subjectivity=0.6 | bad: polarity=-0.7, subjectivity=0.67'. I will give you exactly 50 texts and you just write the polarity and sensitivity without any other words for exact same 50 texts.\""
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_lines = []\n",
    "for k, v in lines.items():\n",
    "    # print(k, len(v.split('|')))\n",
    "    system_content = \"\"\"As sentiment classifier, write polarity and subjectivity for the provided texts. For example for the given text 'good' write 'good: polarity=0.7, subjectivity=0.6' and for the text 'bad' write 'bad: polarity=-0.7, subjectivity=0.67' and for the given text 'good | bad' write 'good: polarity=0.7, subjectivity=0.6 | bad: polarity=-0.7, subjectivity=0.67'. I will give you exactly {t_count} texts and you just write the polarity and sensitivity without any other words for exact same {t_count} texts.\"\"\"\n",
    "    jsonl_lines.append({\"custom_id\": f\"{k}\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-4o-mini\", \"messages\": [{\"role\": \"system\", \"content\": system_content.format(t_count=len(v.split('|')))},{\"role\": \"user\", \"content\": v}],\"max_tokens\": 1000}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonl_lines2 = []\n",
    "# for k, v in lines.items():\n",
    "#     jsonl_lines2.append(system_content + v)\n",
    "    # jsonl_lines2.append(str.join(system_content, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'DataManipulation\\tokens_polarity.jsonl', 'w', encoding=\"utf-8\") as f:\n",
    "    for i in range(len(jsonl_lines)):\n",
    "        f.write(json.dumps(jsonl_lines[i])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'DataManipulation\\tokens_polarity3.jsonl', 'w', encoding=\"utf-8\") as f:\n",
    "    for i in range(len(jsonl_lines2)):\n",
    "        f.write(json.dumps(jsonl_lines2[i])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'DataManipulation\\tokens_polarity2.jsonl', 'w', encoding=\"utf-8\") as f:\n",
    "    for i in range(len(jsonl_lines2)):\n",
    "        f.write(json.dumps(jsonl_lines2[i])+'\\n')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'DataManipulation\\token_polarity_mini_2_batch_674650e577508190b2e501b187ce6ed7_output.jsonl', 'r') as f:\n",
    "    all_lines = []\n",
    "    line_stream = f.readlines()\n",
    "    for line in line_stream:\n",
    "        all_lines.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = all_lines[51]['response']['body']['choices'][0]['message']['content']\n",
    "content = str.split(content, '|')\n",
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', ' ', ' polarity=0.0, subjectivity=0.0 ']"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' : : polarity=0.0, subjectivity=0.0 '.split(':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed: [' ']\n",
      "failed: ['ATFORM ']\n"
     ]
    }
   ],
   "source": [
    "from copy import copy\n",
    "\n",
    "def get_string_ints(string_sample):\n",
    "    str_arr = re.findall(r'\\d+', string_sample)\n",
    "    int_arr = [int(s) for s in str_arr]\n",
    "    return int_arr\n",
    "\n",
    "def get_string_floats(string_sample):\n",
    "    all_floats = re.findall(r\"[-+]?(?:\\d*\\.*\\d+)\", string_sample)\n",
    "    all_floats = [float(f) for f in all_floats]\n",
    "    return all_floats\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def map_content_to_tokens(content, tokens):\n",
    "    \n",
    "    content_dict = dict()\n",
    "    t_c_dict = dict()\n",
    "    content_keys = []\n",
    "    for c in content:\n",
    "        c = c.split(':')\n",
    "        if len(c) == 3:\n",
    "            c= [':', c[2]]\n",
    "        elif len(c) ==0:\n",
    "            continue\n",
    "        elif len(c)==1 or len(c) >3:\n",
    "            print(f\"failed: {c}\")\n",
    "            continue\n",
    "        # print(f'cccc: {c}')\n",
    "        content_dict[c[0]] = c[1]\n",
    "        content_keys.append(c[0])\n",
    "    tokens_ = copy(tokens)\n",
    "    for c in content_keys:\n",
    "        most_similar_token = tokens_[0]\n",
    "        base_sim = similar(c, most_similar_token)\n",
    "        for t in tokens_[1:]:\n",
    "            new_sim = similar(c, t)\n",
    "            if new_sim > base_sim:\n",
    "                base_sim = new_sim\n",
    "                most_similar_token = t\n",
    "        t_c_dict[most_similar_token] = c\n",
    "        tokens_.remove(most_similar_token)\n",
    "    return t_c_dict, content_dict\n",
    "\n",
    "all_vocab_sentiments = dict()\n",
    "k = 0\n",
    "for j in range(len(all_lines)):\n",
    "    ids_range = get_string_ints(all_lines[j]['custom_id'])\n",
    "    content = all_lines[j]['response']['body']['choices'][0]['message']['content']\n",
    "    content = str.split(content, '|')\n",
    "    # print(ids_range)\n",
    "    # print(len(content), content)\n",
    "    list_to_remove = []\n",
    "    for i in range(len(content)):\n",
    "        if content[i].strip() =='':\n",
    "            list_to_remove.append(i)\n",
    "    for ltr in list_to_remove:\n",
    "        del content[ltr]\n",
    "    t_c_dict, content_dict = map_content_to_tokens(content, all_vocab_str[ids_range[0]: ids_range[1]])\n",
    "            \n",
    "    for i in range(ids_range[0], ids_range[1]):\n",
    "        \n",
    "        if all_vocab_str[i] not in t_c_dict:\n",
    "            k += 1\n",
    "            # print(k, 'failed', i, t, all_vocab_str[i])\n",
    "            all_vocab_sentiments[all_vocab_str[i]] = [0, 0]\n",
    "            continue\n",
    "        \n",
    "        # print(i, all_vocab_str[i], t_c_dict[all_vocab_str[i]], content_dict[t_c_dict[all_vocab_str[i]]])\n",
    "        content_info = content_dict[t_c_dict[all_vocab_str[i]]].split(',')\n",
    "        content_info = [t.strip() for t in content_info]\n",
    "        content_info = sorted(content_info)\n",
    "        polarity = get_string_floats(content_info[0])\n",
    "        # print(content_info)\n",
    "        subjectivity = get_string_floats(content_info[1])\n",
    "        all_vocab_sentiments[all_vocab_str[i]]  = [polarity, subjectivity]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sentiments = []\n",
    "for i in range(len(all_vocab_str)):\n",
    "    vocab_sentiments.append(np.array(all_vocab_sentiments[all_vocab_str[i]]).squeeze())\n",
    "vocab_sentiments = np.vstack(vocab_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128001, 2)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_sentiments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4838 [-0.7   0.67]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_vocab_str)):\n",
    "    if 'terrible' in all_vocab_str[i]:\n",
    "        print(i, vocab_sentiments[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(r'Data\\ReducedEmbeddings\\polarity_debertav3_tokens_gpt_mini_emb.npy', vocab_sentiments, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128001, 2)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack(vocab_sentiments).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'batch_req_67461a2c0660819092aed4151fd0baaf',\n",
       " 'custom_id': '(0, 50)',\n",
       " 'response': {'status_code': 200,\n",
       "  'request_id': 'a2db5f65ff33cb9c21142e0ebc71b861',\n",
       "  'body': {'id': 'chatcmpl-AXv2UjwijaVpPfAhw9CXFPjLbgBoN',\n",
       "   'object': 'chat.completion',\n",
       "   'created': 1732647446,\n",
       "   'model': 'gpt-4o-2024-08-06',\n",
       "   'choices': [{'index': 0,\n",
       "     'message': {'role': 'assistant',\n",
       "      'content': '[PAD]: polarity=0.0, subjectivity=0.0 | [CLS]: polarity=0.0, subjectivity=0.0 | [SEP]: polarity=0.0, subjectivity=0.0 | [UNK]: polarity=0.0, subjectivity=0.0 | <0x00>: polarity=0.0, subjectivity=0.0 | <0x01>: polarity=0.0, subjectivity=0.0 | <0x02>: polarity=0.0, subjectivity=0.0 | <0x03>: polarity=0.0, subjectivity=0.0 | <0x04>: polarity=0.0, subjectivity=0.0 | <0x05>: polarity=0.0, subjectivity=0.0 | <0x06>: polarity=0.0, subjectivity=0.0 | <0x07>: polarity=0.0, subjectivity=0.0 | <0x08>: polarity=0.0, subjectivity=0.0 | <0x09>: polarity=0.0, subjectivity=0.0 | <0x0A>: polarity=0.0, subjectivity=0.0 | <0x0B>: polarity=0.0, subjectivity=0.0 | <0x0C>: polarity=0.0, subjectivity=0.0 | <0x0D>: polarity=0.0, subjectivity=0.0 | <0x0E>: polarity=0.0, subjectivity=0.0 | <0x0F>: polarity=0.0, subjectivity=0.0 | <0x10>: polarity=0.0, subjectivity=0.0 | <0x11>: polarity=0.0, subjectivity=0.0 | <0x12>: polarity=0.0, subjectivity=0.0 | <0x13>: polarity=0.0, subjectivity=0.0 | <0x14>: polarity=0.0, subjectivity=0.0 | <0x15>: polarity=0.0, subjectivity=0.0 | <0x16>: polarity=0.0, subjectivity=0.0 | <0x17>: polarity=0.0, subjectivity=0.0 | <0x18>: polarity=0.0, subjectivity=0.0 | <0x19>: polarity=0.0, subjectivity=0.0 | <0x1A>: polarity=0.0, subjectivity=0.0 | <0x1B>: polarity=0.0, subjectivity=0.0 | <0x1C>: polarity=0.0, subjectivity=0.0 | <0x1D>: polarity=0.0, subjectivity=0.0 | <0x1E>: polarity=0.0, subjectivity=0.0 | <0x1F>: polarity=0.0, subjectivity=0.0 | <0x20>: polarity=0.0, subjectivity=0.0 | <0x21>: polarity=0.0, subjectivity=0.0 | <0x22>: polarity=0.0, subjectivity=0.0 | <0x23>: polarity=0.0, subjectivity=0.0 | <0x24>: polarity=0.0, subjectivity=0.0 | <0x25>: polarity=0.0, subjectivity=0.0 | <0x26>: polarity=0.0, subjectivity=0.0 | <0x27>: polarity=0.0, subjectivity=0.0 | <0x28>: polarity=0.0, subjectivity=0.0 | <0x29>: polarity=0.0, subjectivity=0.0 | <0x2A>: polarity=0.0, subjectivity=0.0 | <0x2B>: polarity=0.0, subjectivity=0.0 | <0x2C>: polarity=0.0, subjectivity=0.0 | <0x2D>: polarity=0.0, subjectivity=0.0',\n",
       "      'refusal': None},\n",
       "     'logprobs': None,\n",
       "     'finish_reason': 'stop'}],\n",
       "   'usage': {'prompt_tokens': 446,\n",
       "    'completion_tokens': 907,\n",
       "    'total_tokens': 1353,\n",
       "    'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
       "    'completion_tokens_details': {'reasoning_tokens': 0,\n",
       "     'audio_tokens': 0,\n",
       "     'accepted_prediction_tokens': 0,\n",
       "     'rejected_prediction_tokens': 0}},\n",
       "   'system_fingerprint': 'fp_7f6be3efb0'}},\n",
       " 'error': None}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lines[0]['response']['body']['choices'][0]['message']['content']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
