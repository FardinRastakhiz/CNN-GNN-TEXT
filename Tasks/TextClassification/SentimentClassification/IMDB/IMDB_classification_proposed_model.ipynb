{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classification Using Proposed CNN-GNN on IMDB binary dataset + Token Sentiment Injection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Results:\n",
    "| metric | test 1 | test 2 | test 3 | test 4 | test 5 | Average |\n",
    "|:---|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "| f1-score |  |  |  |  |  |  |\n",
    "| accuracy |  |  |  |  |  |  |\n",
    "| precision |  |  |  |  |  |  |\n",
    "| recall |  |  |  |  |  |  |\n",
    "| loss |  |  |  |  |  |  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch_scatter import scatter_max, scatter_mean, scatter_sum, scatter_std\n",
    "import torchmetrics\n",
    "import lightning as L\n",
    "from torch_geometric.data import Batch, Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from copy import copy\n",
    "import spacy\n",
    "from umap import UMAP\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import PyPDF2\n",
    "from bs4 import BeautifulSoup\n",
    "# import textract\n",
    "import json\n",
    "import gzip\n",
    "import re\n",
    "import collections\n",
    "import csv\n",
    "import string\n",
    "import textwrap\n",
    "import random\n",
    "import itertools\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "771064"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(r'data\\reduced_embeddings\\spacy_lg_reduced_embeddings.npy', 'rb') as f:\n",
    "    embeddings = np.load(f)\n",
    "nlp = spacy.load('en_core_web_lg', disable=['tok2vec','tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
    "t_tokenizer = TweetTokenizer()\n",
    "nlp.max_length = len(' '.join(list(nlp.vocab.strings)))+1\n",
    "all_vocab_doc = nlp(' '.join(list(nlp.vocab.strings)))\n",
    "all_vocab_str = [f'{t}' for t in all_vocab_doc]\n",
    "embeddings = torch.from_numpy(embeddings)\n",
    "embeddings = (embeddings - torch.min(embeddings)) / (torch.max(embeddings)-torch.min(embeddings))\n",
    "token_vocab_dict = dict(zip(all_vocab_str, embeddings))\n",
    "token_vocab_dict['<n>'] = token_vocab_dict['newline']\n",
    "len(token_vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "771064"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(r'data\\reduced_embeddings\\spacy_polarities_subjectivities.npy', 'rb') as f:\n",
    "    polarities_subjectivities= np.load(f)\n",
    "polarities_subjectivities = torch.from_numpy(polarities_subjectivities)\n",
    "polarity_vocab_dict = dict(zip(all_vocab_str, polarities_subjectivities))\n",
    "polarity_vocab_dict['<n>'] = torch.tensor([0.0, 0.0])\n",
    "len(token_vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 224\n",
    "folder_path = r'data\\TextClassification\\IMDB'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r'data\\TextClassification\\IMDB\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I have seen this movie twice and it's theme is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I happened to catch this on community TV a few...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  I have seen this movie twice and it's theme is...\n",
       "1      1  I happened to catch this on community TV a few..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_ratio = 1.0\n",
    "test_df = pd.read_csv(r'data\\TextClassification\\IMDB\\test.csv')\n",
    "test_df['Topic'] = test_df['label']\n",
    "test_df['Content'] = test_df['text']\n",
    "test_df.drop(['label', 'text'], axis=1, inplace=True)\n",
    "test_df.dropna(inplace=True)\n",
    "test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "test_df = test_df.iloc[:int(keep_ratio*test_df.shape[0])]\n",
    "train_df = pd.read_csv(r'data\\TextClassification\\IMDB\\train.csv')\n",
    "train_df['Topic'] = train_df['label']\n",
    "train_df['Content'] = train_df['text']\n",
    "train_df.drop(['label', 'text'], axis=1, inplace=True)\n",
    "train_df.dropna(inplace=True)\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "train_df = train_df.iloc[:int(keep_ratio*train_df.shape[0])]\n",
    "sst_classes = [\"Negative\", \"Positive\"]\n",
    "df = pd.concat([train_df, test_df])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lengths = np.array([len(tx) for tx in df['Content'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a histogram\n",
    "plt.hist(doc_lengths, bins='auto', edgecolor='black')\n",
    "# plt.xlim([0,138000])\n",
    "# Add labels and title\n",
    "plt.xlabel('Document Lengths')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Document Lengths')\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = df.Topic.unique()\n",
    "class_id = {sst_classes[i]:i for i in class_list}\n",
    "id_class = {i:sst_classes[i] for i in class_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_set = set()\n",
    "for doc in df.Content.values:\n",
    "    char_set.update(set(' '.join(t_tokenizer.tokenize(doc))))\n",
    "len(char_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_chars = string.ascii_letters + string.digits + string.punctuation + ' '\n",
    "all_chars = set(''.join(char_set).join(allowed_chars))\n",
    "vocab_dict = {c:i for i, c in enumerate(all_chars)}\n",
    "if '\\x01' not in vocab_dict:\n",
    "    vocab_dict['\\x01'] = len(vocab_dict)\n",
    "char_Set = set(vocab_dict.keys())\n",
    "num_embedding = len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterandTokenLevelCustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, num_classes, char_dict, token_dict, sentiment_dict, tokenizer, shuffle=True, batch_size=128) -> None:\n",
    "        super().__init__()\n",
    "        # y = y[indices].values\n",
    "        # y = torch.from_numpy(np.array([class_id[c] for c in y], dtype=np.longlong))\n",
    "        \n",
    "        # print(f'self.num_sections1: {len(y) // batch_size}')\n",
    "        \n",
    "        if len(y) % batch_size != 0:\n",
    "            self.shortage = ((len(y) // batch_size)+1)*batch_size - len(y)\n",
    "            empty_labels = [i%2 for i in range(self.shortage)]\n",
    "            empty_strings = [id_class[l] for l in empty_labels]\n",
    "            \n",
    "            # print(f'y1 - {y.shape}: {y}')\n",
    "            y = np.concatenate([y, empty_labels])\n",
    "            # print(f'y2 - {y.shape}: {y}')\n",
    "            # print(f'X1 - {X.shape}: {X}')\n",
    "            X = np.concatenate([X, empty_strings])\n",
    "        #     print(f'X2 - {X.shape}: {X}')\n",
    "        \n",
    "        # print(f'self.num_sections2: {len(y) // batch_size}')\n",
    "        \n",
    "        y = torch.from_numpy(y)\n",
    "        self.shuffle = shuffle\n",
    "        self.y = torch.nn.functional.one_hot(y, num_classes=num_classes).float()\n",
    "        self.X = X\n",
    "        self.char_dict = char_dict\n",
    "        self.char_Set = set(char_dict.keys())\n",
    "        self.vocab_size = len(self.char_dict)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_dict = token_dict\n",
    "        self.sentiment_dict = sentiment_dict\n",
    "        self.max_token_count = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.all_data = []\n",
    "        self.token_lengths = []\n",
    "        self.token_embeddign_ids = []\n",
    "        \n",
    "        self.sum_a = 0\n",
    "        \n",
    "        for doc in tqdm(self.X):\n",
    "            # tokens = self.tokenizer(''.join(c for c in doc if c in self.char_Set))\n",
    "            # tokens = [t.text for t in tokens]\n",
    "            tokens = self.tokenizer(doc)\n",
    "            if len(tokens) == 0:\n",
    "                tokens = ['empty']\n",
    "                            \n",
    "            token_lengths = [len(t) for t in tokens]\n",
    "            tokens.append('\\x01')\n",
    "            \n",
    "            token_lengths.append(len(tokens[-1])-1)\n",
    "            token_lengths = torch.from_numpy(np.array(token_lengths, dtype=np.longlong))+1\n",
    "            token_embs = [self.token_dict[t] if t in self.token_dict else torch.zeros((64, ), dtype=torch.float32) for t in tokens]\n",
    "            token_sentiments = [self.sentiment_dict[t] if t in self.sentiment_dict else (0.0, 0.0) for t in tokens]\n",
    "            token_embs = torch.from_numpy(np.array(token_embs, dtype=np.float32))\n",
    "            token_sentiments = torch.from_numpy(np.array(token_sentiments, dtype=np.float32))\n",
    "            doc = ' '.join(tokens)\n",
    "            characters = torch.from_numpy(np.array([self.char_dict[t] for t in doc], dtype=np.longlong))\n",
    "            token_positions = torch.arange(len(token_lengths), dtype=torch.long)\n",
    "            token_indices = torch.repeat_interleave(token_positions, token_lengths)\n",
    "            num_tokens = len(token_lengths)\n",
    "            if num_tokens > self.max_token_count:\n",
    "                self.max_token_count = num_tokens\n",
    "            g_data = Data(x=characters,\n",
    "                            token_positions=token_positions,\n",
    "                            character_length = len(characters),\n",
    "                            num_tokens = num_tokens,\n",
    "                            token_indices=token_indices,\n",
    "                            token_lengths=token_lengths,\n",
    "                            token_embeddings=token_embs,\n",
    "                            token_sentiments=token_sentiments)\n",
    "            self.all_data.append(g_data)\n",
    "        \n",
    "        \n",
    "        self.num_sections = len(y) // batch_size\n",
    "        self.x_lengths = np.array([self.all_data[i].character_length for i in range(len(self.all_data))])\n",
    "        self.x_len_args = np.argsort(self.x_lengths)[::-1]\n",
    "        \n",
    "        self.section_ranges = np.linspace(0, len(self.x_len_args), self.num_sections+1)\n",
    "        self.section_ranges = [(int(self.section_ranges[i-1]), int(self.section_ranges[i])) for i in range(1, len(self.section_ranges))]\n",
    "\n",
    "        self.position_j = 0\n",
    "        self.section_i = 0\n",
    "        self.epoch = 0\n",
    "        self.each_section_i = np.zeros((self.num_sections, ), dtype=int)\n",
    "        \n",
    "        self.sections, self.section_size = self.split_into_k_groups(self.x_len_args, self.x_lengths, self.num_sections)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        index = self.get_section_index()\n",
    "        return self.all_data[index], self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def get_section_index(self):\n",
    "        # if self.shuffle:\n",
    "            \n",
    "        #     t_range = self.section_ranges[self.section_i]\n",
    "        #     target_index = np.random.randint(t_range[0], t_range[1])\n",
    "        # else:\n",
    "        #     t_range = self.section_ranges[self.section_i]\n",
    "        #     target_index = t_range[0] + self.each_section_i[self.section_i]\n",
    "        #     self.each_section_i[self.section_i] = (self.each_section_i[self.section_i] + 1) % (t_range[1] - t_range[0])\n",
    "        # print()\n",
    "        # print(f'self.section_i: {self.section_i},   self.position_j: {self.position_j}')\n",
    "        target_index = self.sections[self.section_i, self.position_j]\n",
    "        \n",
    "        self.position_j = (self.position_j + 1) % self.section_size\n",
    "        if self.position_j == 0:\n",
    "            self.section_i = (self.section_i + 1) % self.num_sections\n",
    "            if self.shuffle and self.section_i == 0:\n",
    "                self.sections, self.section_size = self.split_into_k_groups(self.x_len_args, self.x_lengths, self.num_sections)\n",
    "                # random_positions = np.random.choice(np.arange(0, self.section_size), size=self.section_size, replace=False)\n",
    "        # return self.x_len_args[target_index]\n",
    "        return target_index\n",
    "\n",
    "    def reset_params(self):\n",
    "        self.section_i = 0\n",
    "        self.position_j = 0\n",
    "        self.each_section_i = np.zeros((self.num_sections, ), dtype=int)\n",
    "        \n",
    "    def split_into_k_groups(self, len_sorted_args, lengths:np.array, k):\n",
    "        if self.shuffle and self.epoch > 0:\n",
    "            randomize_sections = np.concatenate([np.random.choice(np.arange(r[0], r[1]), size=r[1]-r[0], replace=False) for r in self.section_ranges])\n",
    "            len_sorted_args = len_sorted_args[randomize_sections]\n",
    "        \n",
    "        nums = lengths[len_sorted_args]\n",
    "        groups_size = len(len_sorted_args) // k\n",
    "        \n",
    "        \n",
    "        groups = [[] for _ in range(k)]\n",
    "        group_sums = np.zeros(k, dtype=int)\n",
    "        group_sizes = np.zeros(k, dtype=int)\n",
    "        \n",
    "        # print(f'groups_size: {groups_size}')\n",
    "        # print(f'len(len_sorted_args): {len(len_sorted_args)}')\n",
    "        # print(f'k: {k}')\n",
    "        for i, num in enumerate(nums):\n",
    "            candidate_indices = np.where(group_sizes<groups_size)[0]\n",
    "            # print(f'candidate_indices: {candidate_indices}')\n",
    "            min_group_idx = candidate_indices[np.argmin(group_sums[candidate_indices])]\n",
    "            groups[min_group_idx].append(len_sorted_args[i])\n",
    "            group_sums[min_group_idx] += num\n",
    "            group_sizes[min_group_idx] += 1\n",
    "        self.epoch += 1\n",
    "        \n",
    "        groups = np.array(groups)\n",
    "        group_sums_argsort = np.argsort(group_sums)[::-1]\n",
    "        groups = groups[group_sums_argsort]\n",
    "        \n",
    "        # check_x = self.X[groups]\n",
    "        # check_x_lens = [np.sum(np.array([len(sx) for sx in rx])) for rx in check_x]\n",
    "        # print(f'check_x: {check_x}')\n",
    "        \n",
    "        \n",
    "        return np.array(groups), groups_size\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "class CharacterandTokenLevelDataLoader(DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size: int = 1,\n",
    "        shuffle: bool = False,\n",
    "        follow_batch: List[str] | None = None,\n",
    "        exclude_keys: List[str] | None = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(CharacterandTokenLevelDataLoader, self).__init__(\n",
    "            dataset, batch_size, shuffle, follow_batch, exclude_keys, **kwargs\n",
    "        )\n",
    "\n",
    "    def __iter__(self):\n",
    "        base_iterator = super(CharacterandTokenLevelDataLoader, self).__iter__()\n",
    "        for batch in base_iterator:\n",
    "            cumsum_vals = torch.cumsum(batch[0].num_tokens, dim=0).roll(1)\n",
    "            cumsum_vals[0] = 0\n",
    "            additions = torch.repeat_interleave(cumsum_vals, batch[0].character_length)\n",
    "            batch[0].cumulative_token_indices = batch[0].token_indices + additions\n",
    "            yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_tokenizer(doc):\n",
    "    tokens = t_tokenizer.tokenize(doc)\n",
    "    tokens = nlp(' '.join(tokens))\n",
    "    tokens = [t.text for t in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25088/25088 [05:10<00:00, 80.74it/s] \n",
      "100%|██████████| 25088/25088 [05:05<00:00, 82.18it/s] \n"
     ]
    }
   ],
   "source": [
    "train_dataset = CharacterandTokenLevelCustomDataset(train_df.Content.values, train_df.Topic.values, len(class_id), vocab_dict, token_vocab_dict, polarity_vocab_dict, double_tokenizer, batch_size=batch_size)\n",
    "test_dataset = CharacterandTokenLevelCustomDataset(test_df.Content.values, test_df.Topic.values, len(class_id), vocab_dict, token_vocab_dict, polarity_vocab_dict, double_tokenizer, batch_size=batch_size)\n",
    "max_token_count = max(train_dataset.max_token_count, test_dataset.max_token_count)\n",
    "train_dataloader = CharacterandTokenLevelDataLoader(train_dataset, batch_size=batch_size, drop_last=False, shuffle=False)\n",
    "test_dataloader = CharacterandTokenLevelDataLoader(test_dataset, batch_size=batch_size, drop_last=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2851\n",
      "2986\n"
     ]
    }
   ],
   "source": [
    "train_lengths = np.array([train_dataset[i][0].num_tokens for i in range(len(train_dataset))])\n",
    "test_lengths = np.array([test_dataset[i][0].num_tokens for i in range(len(test_dataset))])\n",
    "print(np.max(train_lengths))\n",
    "print(np.max(test_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader = CharacterandTokenLevelDataLoader(train_dataset, batch_size=batch_size, drop_last=False, shuffle=True)\n",
    "# test_dataloader = CharacterandTokenLevelDataLoader(test_dataset, batch_size=batch_size, drop_last=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Data(x=[5688], token_positions=[1185], character_length=5688, num_tokens=1185, token_indices=[5688], token_lengths=[1185], token_embeddings=[1185, 64], token_sentiments=[1185, 2]),\n",
       " tensor([0., 1.]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset.reset_params()\n",
    "# test_dataset.position_j = 0\n",
    "# for X, y in test_dataloader:\n",
    "#     print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATv2Conv, SimpleConv\n",
    "\n",
    "class GCNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, *args, **kwargs):\n",
    "        super(GCNN, self).__init__(*args, **kwargs)\n",
    "        # self.hidden_dim = hidden_dim\n",
    "        self.gnn = GATv2Conv(hidden_dim, hidden_dim//8, heads=4)\n",
    "        # self.gnn = SimpleConv(aggr='mean')\n",
    "        self.conv = nn.Conv1d(hidden_dim, hidden_dim//2, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(hidden_dim//2, hidden_dim)\n",
    "        # self.out_fc = nn.Linear(2*hidden_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim//2)\n",
    "        # self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim)\n",
    "        # self.bn4 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "    def forward(self, x, graph, total_token_count, return_attention_weights = False):\n",
    "        x1, edge_weights = self.gnn(x, graph.edge_index, return_attention_weights=return_attention_weights) \n",
    "        x2 = F.relu(self.conv(x[:total_token_count].T).T)#self.bn2(self.conv(x[:total_token_count].T).T))\n",
    "        # x3 =  F.leaky_relu_(self.fc(x1[total_token_count:]))\n",
    "        x3 =  F.leaky_relu_(self.bn3(self.fc(x1[total_token_count:])))\n",
    "        x1 = F.leaky_relu_(self.bn1(x1[:total_token_count]))\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        x = torch.cat([x, x3], dim=0)\n",
    "        # x = self.bn4(x)\n",
    "        return x, edge_weights #F.leaky_relu_(self.bn4(self.out_fc(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx, to_undirected\n",
    "\n",
    "class GenGraph(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, virtual_nodes, lattice_step, *args, **kwargs):\n",
    "        super(GenGraph, self).__init__(*args, **kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.virtual_nodes = virtual_nodes\n",
    "        self.lattice_step = lattice_step\n",
    "        self.virtual_node_embeddings = nn.Embedding(self.virtual_nodes, hidden_dim)\n",
    "        \n",
    "    def gen_graph(self, x, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance=2):\n",
    "        random_links, lattice_links, tc_range = self.calculate_graph(x, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance)\n",
    "        v_n_e_counts = total_token_coutns*self.virtual_nodes\n",
    "        base_numel = random_links.numel() + lattice_links.numel()*2\n",
    "        edge_index = torch.empty((2, base_numel + v_n_e_counts*2), dtype=torch.int64, device=x.device)\n",
    "        self.fill_lattice_and_random_edges(edge_index, random_links, lattice_links, tc_range)\n",
    "            \n",
    "        if self.virtual_nodes > 0:\n",
    "            virtual_nodes_range = torch.arange(self.virtual_nodes, device=x.device).view(1, -1)\n",
    "            virtual_nodes_ids = torch.repeat_interleave(virtual_nodes_range, len(token_counts), dim=0)\n",
    "            v_n_idx = (virtual_nodes_ids + torch.arange(0, len(token_counts)*self.virtual_nodes, self.virtual_nodes, device=x.device).view(-1, 1) + total_token_coutns )\n",
    "            virtual_edge_ids = torch.repeat_interleave(v_n_idx.view(-1), token_counts.view(-1, 1).expand(len(token_counts), self.virtual_nodes).reshape(-1), dim=0).view(1, -1)\n",
    "            \n",
    "            embs = self.virtual_node_embeddings(virtual_nodes_ids.T).view(-1, self.hidden_dim)\n",
    "            x_extended = torch.cat([x, embs], dim=0)\n",
    "            x_index = torch.arange(total_token_coutns, device=x.device).repeat(self.virtual_nodes).view(1, -1)\n",
    "            edge_index[:, base_numel:base_numel+v_n_e_counts] = torch.cat([x_index, virtual_edge_ids], dim=0)\n",
    "            edge_index[:, base_numel+v_n_e_counts:] = torch.cat([virtual_edge_ids, x_index], dim=0)\n",
    "            x = x_extended\n",
    "        \n",
    "        return Batch.from_data_list([Data(x=x, edge_index=edge_index)])\n",
    "        \n",
    "    def re_gen_graph(self, x, edge_index, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance=2):\n",
    "        random_links, lattice_links, tc_range = self.calculate_graph(x, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance)\n",
    "        base_numel = random_links.numel() + lattice_links.numel()*2\n",
    "        \n",
    "        self.fill_lattice_and_random_edges(edge_index, random_links, lattice_links, tc_range)\n",
    "        # for i in range(base.shape[1]):\n",
    "        #     edge_index[:, i*base.shape[0]:(i+1)*base.shape[0]] = torch.cat([tc_range, base[:,i].view(1,-1)], dim=0)\n",
    "            \n",
    "        return Batch.from_data_list([Data(x=x, edge_index=edge_index)])\n",
    "    \n",
    "    def replace_unimportant_edges(self, edge_weights, x, edge_index, total_token_coutns, token_counts, random_edges, lattice_edges, p_keep=1, lattice_start_distance=2):\n",
    "        v_n_e_counts = total_token_coutns*self.virtual_nodes\n",
    "        if v_n_e_counts>0:\n",
    "            important_indices = torch.topk(edge_weights[:-2*v_n_e_counts].view(-1, total_token_coutns), p_keep, dim=0).indices\n",
    "        else:\n",
    "            important_indices = torch.topk(edge_weights.view(-1, total_token_coutns), p_keep, dim=0).indices\n",
    "        # important_indices = torch.topk(edge_weights[:-1*total_token_coutns].view(-1, total_token_coutns), 1, dim=0).indices.squeeze()\n",
    "\n",
    "        important_indices = torch.arange(total_token_coutns, dtype=torch.int64, device=x.device) + important_indices*total_token_coutns\n",
    "        important_indices = important_indices.view(-1)\n",
    "        random_links, lattice_links, tc_range = self.calculate_graph(x, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance)\n",
    "        base_numel = random_links.numel() + lattice_links.numel()*2\n",
    "        \n",
    "        new_edge_index = torch.empty((2, base_numel + important_indices.shape[0] + 2*v_n_e_counts), dtype=torch.int64, device=x.device)\n",
    "        self.fill_lattice_and_random_edges(new_edge_index, random_links, lattice_links, tc_range)\n",
    "        new_edge_index[:, base_numel:base_numel+important_indices.shape[0]] = edge_index[:, important_indices]\n",
    "        if(self.virtual_nodes>0):\n",
    "            new_edge_index[:, -2*v_n_e_counts:] = edge_index[:, -2*v_n_e_counts:]\n",
    "            \n",
    "        # for i in range(base.shape[1]):\n",
    "        #     new_edge_index[:, i*base.shape[0]:(i+1)*base.shape[0]] = torch.cat([tc_range, base[:,i].view(1,-1)], dim=0)\n",
    "        \n",
    "        return Batch.from_data_list([Data(x=x, edge_index=new_edge_index)])\n",
    "         \n",
    "    def calculate_graph(self, x, total_token_coutns, token_counts, random_edges, lattice_edges, lattice_start_distance):\n",
    "        tc_extended = torch.repeat_interleave(token_counts, token_counts, dim=0).view(-1,1)\n",
    "        tc_lower_bound = torch.empty((len(token_counts)+1), dtype=torch.long, device=x.device) #torch.cuda.IntTensor(len(token_counts)+1) #\n",
    "        tc_lower_bound[0] = 0\n",
    "        tc_lower_bound[1:] = torch.cumsum(token_counts, dim=0)\n",
    "        tc_lower_bound_extended = torch.repeat_interleave(tc_lower_bound[:-1], token_counts, dim=0).view(-1,1)\n",
    "        tc_range = torch.arange(tc_lower_bound[-1], device=x.device).view(-1,1)\n",
    "        # torch.arange(tc_lower_bound[-1], dtype=torch.int32, device=x.device).view(-1,1)\n",
    "        random_ints = torch.randint(0, 2*total_token_coutns, (total_token_coutns, random_edges), device=x.device) # torch.cuda.IntTensor(len(token_lengths), random_edges).random_()\n",
    "        lattice = torch.arange(lattice_start_distance, self.lattice_step*lattice_edges+1, self.lattice_step, device=x.device).view(1, -1)\n",
    "\n",
    "        # exponentials = torch.pow(2, torch.arange(1, self.exp_edges+1, device=x.device)).view(1, -1)\n",
    "        tc_local_range = tc_range - tc_lower_bound_extended\n",
    "        random_links = (((random_ints % (tc_extended - 1))+1 + tc_local_range) % tc_extended)+tc_lower_bound_extended\n",
    "        lattice_links = ((lattice + tc_local_range) % tc_extended)+tc_lower_bound_extended\n",
    "        # base = torch.cat([base1, base2], dim=1)\n",
    "        tc_range = tc_range.view(1,-1)\n",
    "        return random_links, lattice_links, tc_range\n",
    "    \n",
    "    def fill_lattice_and_random_edges(self, edge_index, random_links, lattice_links, tc_range):\n",
    "        for i in range(0, lattice_links.shape[1]*2, 2):\n",
    "            edge_index[:, i*lattice_links.shape[0]:(i+1)*lattice_links.shape[0]] = torch.cat([lattice_links[:,i//2].view(1,-1), tc_range], dim=0)\n",
    "            edge_index[:, (i+1)*lattice_links.shape[0]:(i+2)*lattice_links.shape[0]] = edge_index[:, i*lattice_links.shape[0]:(i+1)*lattice_links.shape[0]][[1, 0]]\n",
    "            \n",
    "        for i in range(random_links.shape[1]):\n",
    "            j = i + lattice_links.shape[1]*2\n",
    "            edge_index[:, j*random_links.shape[0]:(j+1)*random_links.shape[0]] = torch.cat([random_links[:,i].view(1,-1), tc_range], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN_for_Text(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embedding, pos_emb_size=8192, embedding_dim=64, hidden_dim=64, dropout=0.3, num_out_features=4, seed=-1, random_edges=4, lattice_edges=10, virtual_nodes=1, lattice_step=2, lattice_start_distance=2, inject_embedding_dim=64, *args, **kwargs) -> None:\n",
    "        super(CNN_for_Text, self).__init__(*args, **kwargs)\n",
    "        self.pos_emb_size = pos_emb_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.virtual_nodes = virtual_nodes\n",
    "        self.base_random_edges = random_edges\n",
    "        self.base_lattice_edges = lattice_edges\n",
    "        self.lattice_start_distance = lattice_start_distance\n",
    "        if seed>-1:\n",
    "            torch.manual_seed(seed)\n",
    " \n",
    "        self.embedding = nn.Embedding(num_embedding, embedding_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.positional_encoding = nn.Embedding(pos_emb_size, embedding_dim)\n",
    "        self.positional_encoding.weight = self.create_positional_encoding()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(embedding_dim, hidden_dim, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv1d(2*hidden_dim + 2, hidden_dim, kernel_size=3, padding=1)\n",
    "               \n",
    "        self.gcnn1 = GCNN(hidden_dim)\n",
    "        self.gcnn2 = GCNN(hidden_dim + inject_embedding_dim)\n",
    "        self.graph_generator = GenGraph(hidden_dim, virtual_nodes, lattice_step)\n",
    "        \n",
    "        k = 32\n",
    "        self.fc0 =  nn.Linear(hidden_dim , hidden_dim + inject_embedding_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim + inject_embedding_dim , hidden_dim * k)\n",
    "        self.fc2 = nn.Linear(hidden_dim * (2+virtual_nodes) * k , 32)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(32, num_out_features)\n",
    "        self.max_length = 0\n",
    "    \n",
    "    def forward(self, g_data):\n",
    "        # self.max_length = max(self.max_length, len(g_data.x))\n",
    "        if len(g_data.x) > self.max_length:\n",
    "            self.max_length = len(g_data.x)\n",
    "            print(self.max_length)\n",
    "        x = self.embedding(g_data.x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.T\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        x1 = scatter_max(x, g_data.cumulative_token_indices, dim=1)[0]\n",
    "        x2 = scatter_mean(x, g_data.cumulative_token_indices, dim=1)\n",
    "\n",
    "        x = torch.cat([x1, x2, g_data.token_sentiments.T], dim=0)\n",
    "        x = F.relu(self.conv3(x)).T\n",
    "        x = x + self.positional_encoding(g_data.token_positions)\n",
    "\n",
    "        rand_edges, lattice_edges = self.base_random_edges, self.base_lattice_edges\n",
    "            \n",
    "        graph = self.graph_generator.gen_graph(x, len(g_data.token_lengths), g_data.num_tokens, rand_edges, lattice_edges, lattice_start_distance=self.lattice_start_distance)\n",
    "        rand_edges = rand_edges-1\n",
    "        lattice_edges = lattice_edges-1\n",
    "        \n",
    "        x, edge_weights = self.gcnn1(graph.x, graph, len(g_data.token_lengths), return_attention_weights = True)\n",
    "        edge_weights = edge_weights[1][:graph.edge_index.shape[1], 0]\n",
    "        \n",
    "        graph = self.graph_generator.replace_unimportant_edges(edge_weights, x, graph.edge_index, len(g_data.token_lengths), g_data.num_tokens, rand_edges, lattice_edges, p_keep=2, lattice_start_distance=self.lattice_start_distance+1)\n",
    "                \n",
    "        x = torch.cat([x, g_data.token_embeddings], dim=1)\n",
    "        x1 = F.relu(self.fc0(graph.x[g_data.token_embeddings.shape[0]:]))\n",
    "        x = torch.cat([x, x1], dim=0)\n",
    "        \n",
    "        x, edge_weights = self.gcnn2(x, graph, len(g_data.token_lengths))\n",
    "\n",
    "        x = F.elu_(self.fc1(x))\n",
    "        doc_token_index = torch.repeat_interleave(torch.arange(len(g_data.num_tokens), device=x.device), g_data.num_tokens)\n",
    "        x1 = scatter_max(x[:len(g_data.token_lengths)], doc_token_index, dim=0)[0]\n",
    "        x2 = scatter_mean(x[:len(g_data.token_lengths)], doc_token_index, dim=0)\n",
    "        vn_embs = x[len(g_data.token_lengths):]\n",
    "        x_for_cat = [x1, x2]\n",
    "        x_for_cat.extend([vn_embs[i*x1.shape[0]:(i+1)*x1.shape[0]] for i in range(self.virtual_nodes)])\n",
    "        x = torch.cat(x_for_cat, dim=1)\n",
    "        \n",
    "        x = F.elu_(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "    def create_positional_encoding(self):\n",
    "        position = torch.arange(self.pos_emb_size).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.hidden_dim, 2) * (-math.log(10000.0) / self.hidden_dim))\n",
    "        pe = torch.zeros(self.pos_emb_size, self.hidden_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return torch.nn.Parameter(pe, requires_grad=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_torch_model = CNN_for_Text(num_embedding=num_embedding, hidden_dim=64, embedding_dim=64, pos_emb_size=3072, dropout=0.2, num_out_features=len(class_id), seed=911, random_edges=4, lattice_edges=4, lattice_step=2, virtual_nodes=0, lattice_start_distance=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = classifier_torch_model(X.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CnnGnnClassifierLightningModel(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        num_classes,\n",
    "        optimizer=None,\n",
    "        loss_func=None,\n",
    "        learning_rate=0.01,\n",
    "        batch_size=64,\n",
    "        lr_scheduler=None,\n",
    "        user_lr_scheduler=False,\n",
    "        min_lr=0.0,\n",
    "    ):\n",
    "        super(CnnGnnClassifierLightningModel, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "        self.min_lr = min_lr\n",
    "        # self.save_hyperparameters(ignore=[\"model\"])\n",
    "        self.save_hyperparameters(logger=False)\n",
    "        self.optimizer = self._get_optimizer(optimizer)\n",
    "        self.lr_scheduler = (\n",
    "            self._get_lr_scheduler(lr_scheduler) if user_lr_scheduler else None\n",
    "        )\n",
    "        self.loss_func = loss_func\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.model(x)\n",
    "\n",
    "    def on_train_epoch_start(self) -> None:\n",
    "        param_groups = next(iter(self.optimizer.param_groups))\n",
    "        if \"lr\" in param_groups and param_groups[\"lr\"] is not None:\n",
    "            current_learning_rate = float(param_groups[\"lr\"])\n",
    "            self.log(\n",
    "                \"lr\",\n",
    "                current_learning_rate,\n",
    "                batch_size=self.batch_size,\n",
    "                on_epoch=True,\n",
    "                on_step=False,\n",
    "            )\n",
    "\n",
    "    def training_step(self, batch, *args, **kwargs):\n",
    "        X, y = batch\n",
    "        X.to(self.device)\n",
    "        y.to(self.device)\n",
    "        \n",
    "        self.model.train()\n",
    "        y_out = self(X)\n",
    "\n",
    "        loss = self.loss_func(y_out.view(y.shape), y )\n",
    "        self.train_losses.append(loss.detach().item())\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            batch_size=self.batch_size,\n",
    "            on_epoch=True,\n",
    "            on_step=True,\n",
    "        )\n",
    "        \n",
    "        self.train_acc(torch.argmax(y_out, dim=1), torch.argmax(y, dim=1))\n",
    "        self.log('train_acc', self.train_acc, prog_bar=True, on_epoch=True, on_step=True, batch_size=self.batch_size)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, *args, **kwargs):\n",
    "        X, y = batch\n",
    "        X.to(self.device)\n",
    "        y.to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        y_out = self(X)\n",
    "        loss = self.loss_func(y_out.view(y.shape), y )\n",
    "        self.val_losses.append(loss.detach().item())\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            batch_size=self.batch_size,\n",
    "            on_epoch=True,\n",
    "            on_step=True,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.val_acc(torch.argmax(y_out, dim=1), torch.argmax(y, dim=1))\n",
    "        self.log('val_acc', self.val_acc, prog_bar=True, on_epoch=True, on_step=True, batch_size=self.batch_size)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.lr_scheduler is None:\n",
    "            return self.optimizer\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": self.optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": self.lr_scheduler,\n",
    "                \"monitor\": \"train_loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def update_learning_rate(self, learning_rate: float):\n",
    "        self.learning_rate = learning_rate\n",
    "        for g in self.optimizer.param_groups:\n",
    "            g[\"lr\"] = learning_rate\n",
    "\n",
    "    def _get_optimizer(self, optimizer):\n",
    "        return (\n",
    "            optimizer\n",
    "            if optimizer is not None\n",
    "            else torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        )\n",
    "\n",
    "    def _get_lr_scheduler(self, lr_scheduler):\n",
    "        return (\n",
    "            lr_scheduler\n",
    "            if lr_scheduler is not None\n",
    "            else torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer, patience=5, factor=0.5, mode=\"min\", min_lr=self.min_lr\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from torchmetrics import ConfusionMatrix\n",
    "\n",
    "\n",
    "def calculate_metrics(cl_model, dataloader):\n",
    "    cm = ConfusionMatrix(task=\"multiclass\", num_classes=len(class_id))\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    cl_model = cl_model.eval()\n",
    "    cl_model.to(device)\n",
    "    for X, y in tqdm(dataloader):\n",
    "        X = X.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_p = cl_model(X)\n",
    "            y_p = y_p.cpu()\n",
    "        y_pred.append(y_p)\n",
    "        y_true.append(y)\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "    y_true = torch.cat(y_true, dim=0)\n",
    "    y_pred2 = torch.argmax(y_pred, dim=1)\n",
    "    y_true2 = torch.argmax(y_true, dim=1)\n",
    "    print(f'classification report: \\n {classification_report(y_true2, y_pred2, digits=4)}')\n",
    "    print(f'confusion matrix:\\n {cm(y_pred2, y_true2)}')\n",
    "    print('================================')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 128\n",
    "hidden_dim = 64\n",
    "embedding_dim = 64\n",
    "label_size = 1\n",
    "seed = 911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fardin Rastakhiz @ 2023\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from lightning.pytorch.loggers import Logger, CSVLogger\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from typing import List\n",
    "from pytorch_lightning.core.saving import save_hparams_to_yaml\n",
    "\n",
    "class ModelManager(ABC):\n",
    "\n",
    "    def __init__(self,\n",
    "                 torch_model: torch.nn.Module,\n",
    "                 lightning_model,\n",
    "                 model_save_dir: str = '~/Desktop',\n",
    "                 log_dir: str = 'logs/',\n",
    "                 log_name: str = 'model_logs',\n",
    "                 device='cpu',\n",
    "                 max_epochs = 100,\n",
    "                 ckpt_path: str|None=None,\n",
    "                 accumulate_grad_batches=1):\n",
    "        self.torch_model = torch_model\n",
    "        self.lightning_model = lightning_model\n",
    "        self.log_dir = log_dir\n",
    "        self.log_name = log_name\n",
    "        self.model_save_dir = model_save_dir\n",
    "        self.device = device\n",
    "        self.accelerator = 'cpu' if self.device=='cpu' else 'gpu'\n",
    "        self.max_epochs = max_epochs\n",
    "        self.ckpt_path = ckpt_path\n",
    "\n",
    "        self.logger = self._create_logger()\n",
    "        self.callbacks = self._create_callbacks()\n",
    "        self.trainer: L.Trainer = self._create_trainer(accumulate_grad_batches)\n",
    "        self.tuner = Tuner(self.trainer)\n",
    "        self.tuning_result = None\n",
    "\n",
    "    def tune(self, data_manager=None, train_dataloaders=None, val_dataloaders=None, datamodule=None, draw_result=True, min_lr=0.0000001, max_lr=0.1):\n",
    "        self.tuning_result = self.tuner.lr_find(self.lightning_model, datamodule=data_manager, train_dataloaders=train_dataloaders,\n",
    "                         val_dataloaders=val_dataloaders, min_lr=min_lr,max_lr=max_lr, num_training=150)\n",
    "        if draw_result:\n",
    "            fig = self.tuning_result.plot(suggest=True)\n",
    "            fig.show()\n",
    "        self.update_learning_rate(self.tuning_result.suggestion())\n",
    "        return self.tuning_result.suggestion()\n",
    "    \n",
    "    def update_learning_rate(self, lr):\n",
    "        self.lightning_model.update_learning_rate(lr)\n",
    "\n",
    "    def fit(self, train_dataloaders=None, val_dataloaders=None, datamodule=None, max_epochs = -1, ckpt_path=None):\n",
    "        if ckpt_path is not None and ckpt_path != '':\n",
    "            self.ckpt_path = ckpt_path\n",
    "        if max_epochs>0:\n",
    "            self.trainer.fit_loop.max_epochs = max_epochs\n",
    "            # self.max_epochs = max_epochs\n",
    "            # self.trainer = self._create_trainer()\n",
    "        self.trainer.fit(self.lightning_model,\n",
    "                         datamodule=datamodule,\n",
    "                         train_dataloaders=train_dataloaders,\n",
    "                         val_dataloaders=val_dataloaders,\n",
    "                         ckpt_path = self.ckpt_path\n",
    "                         )\n",
    "\n",
    "    def validate(self, dataloaders=None, datamodule=None):\n",
    "        return self.trainer.validate(self.lightning_model,\n",
    "                             datamodule=datamodule,\n",
    "                             dataloaders=dataloaders)\n",
    "\n",
    "    def predict(self, dataloaders=None, datamodule=None):\n",
    "        return self.trainer.predict(self.lightning_model,\n",
    "                             datamodule=datamodule,\n",
    "                             dataloaders=dataloaders)\n",
    "\n",
    "    def _create_trainer(self, accumulate_grad_batches) -> L.Trainer:\n",
    "        return L.Trainer(\n",
    "            callbacks=self.callbacks,\n",
    "            max_epochs=self.max_epochs,\n",
    "            accelerator=self.accelerator,\n",
    "            logger=self.logger,\n",
    "            num_sanity_val_steps=0,\n",
    "            default_root_dir=self.model_save_dir,\n",
    "            accumulate_grad_batches=accumulate_grad_batches\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def _create_callbacks(self) -> List[Callback]:\n",
    "        pass\n",
    "\n",
    "    def _create_logger(self) -> Logger:\n",
    "        return CSVLogger(save_dir=self.log_dir, name=self.log_name)\n",
    "\n",
    "    @abstractmethod\n",
    "    def draw_summary(self, dataloader):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def plot_csv_logger(self, loss_names, eval_names):\n",
    "        pass\n",
    "    \n",
    "    def save_hyper_parameters(self):\n",
    "        mhparams = {\n",
    "            'start_lr': 0.045,\n",
    "            'ckpt_lrs' :  {51: 0.002, 65: 0.00058},\n",
    "            'last_lr' : 0.0003,\n",
    "            'ac_loss_factor': 0.0002,\n",
    "            'weight_decay': 0.0012\n",
    "        }\n",
    "        save_hparams_to_yaml(config_yaml=r'logs\\hetero_model_17_AG\\version_12\\hparams.yaml',\n",
    "                     hparams=mhparams)\n",
    "        \n",
    "    # def find_best_settings(data_manager,\n",
    "    #                        lrs: List[float]=[0.001], dropouts: List[float]=[0.2], \n",
    "    #                        weight_decays: List[float]=[0.00055], emb_factors: List[float]=[0.1], \n",
    "    #                        batch_sizes: List[int]=[128], log_name='find_best_settings'):\n",
    "    #     for lr in lrs:\n",
    "    #         for dropout in dropouts:\n",
    "    #             for wd in weight_decays:\n",
    "    #                 for emb_factor in emb_factors:\n",
    "    #                     for bs in batch_sizes:\n",
    "    #                         data_manager.update_batch_size(bs)\n",
    "    #                         torch_model = HeteroGcnGatModel1(300, 1, X1.metadata(), 128, dropout=dropout)\n",
    "    #                         lightning_model = HeteroBinaryLightningModel(torch_model,\n",
    "    #                                         torch.optim.Adam(torch_model.parameters(), lr=lr, weight_decay=wd),\n",
    "    #                                             loss_func=HeteroLoss1(exception_keys='word', enc_factor=emb_factor),\n",
    "    #                                             learning_rate=lr,\n",
    "    #                                             batch_size=bs,\n",
    "    #                                             user_lr_scheduler=True\n",
    "    #                                             ).to(device)\n",
    "    #                         model_manager = ClassifierModelManager(torch_model, lightning_model, log_name=log_name, device=device, num_train_epoch=10)\n",
    "    #                         model_manager.fit(datamodule=data_manager)\n",
    "    #                         model_manager.save_plot_csv_logger(name_prepend=f'{lr}_{dropout}_{wd}_{emb_factor}_{bs}', loss_names=['train_loss', 'val_loss'], eval_names=['train_acc_epoch', 'val_acc_epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fardin Rastakhiz @ 2023\n",
    "import torch\n",
    "# from scripts.managers.ModelManager import ModelManager\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from torch_geometric.nn import summary\n",
    "from lightning.pytorch.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from os import path\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, hinge_loss\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "class ClassifierModelManager(ModelManager):\n",
    "\n",
    "    def __init__(self,\n",
    "                 torch_model: torch.nn.Module,\n",
    "                 lightning_model,\n",
    "                 model_save_dir: str = '~/Desktop',\n",
    "                 log_dir: str = 'logs/',\n",
    "                 log_name: str = 'model_logs',\n",
    "                 device='cpu',\n",
    "                 num_train_epoch = 100,\n",
    "                 accumulate_grad_batches=1):\n",
    "        super(ClassifierModelManager, self).__init__(torch_model, lightning_model, model_save_dir, log_dir, log_name, device, num_train_epoch, accumulate_grad_batches=accumulate_grad_batches)\n",
    "\n",
    "    def _create_callbacks(self) -> List[Callback]:\n",
    "        return [\n",
    "            ModelCheckpoint(save_top_k=2, mode='max', monitor='val_acc', save_last=True),\n",
    "            # EarlyStopping(patience=50, mode='max', monitor='val_acc')\n",
    "        ]\n",
    "\n",
    "    def draw_summary(self, dataloader):\n",
    "        X, y = next(iter(dataloader))\n",
    "        print(summary(self.torch_model, X.to(self.device)))\n",
    "\n",
    "    def plot_csv_logger(self, loss_names=['train_loss', 'val_loss'], eval_names=['train_acc', 'val_acc']):\n",
    "        csv_path = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', 'metrics.csv')\n",
    "        metrics = pd.read_csv(csv_path)\n",
    "\n",
    "        aggregation_metrics = []\n",
    "        agg_col = 'epoch'\n",
    "        for i, dfg in metrics.groupby(agg_col):\n",
    "            agg = dict(dfg.mean())\n",
    "            agg[agg_col] = i\n",
    "            aggregation_metrics.append(agg)\n",
    "\n",
    "        df_metrics = pd.DataFrame(aggregation_metrics)\n",
    "        df_metrics[loss_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='loss')\n",
    "        df_metrics[eval_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='accuracy')\n",
    "        plt.show()\n",
    "\n",
    "    def save_plot_csv_logger(self, loss_names=['train_loss', 'val_loss'], eval_names=['train_acc', 'val_acc'], name_prepend: str=\"\"):\n",
    "        csv_path = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', 'metrics.csv')\n",
    "        metrics = pd.read_csv(csv_path)\n",
    "\n",
    "        aggregation_metrics = []\n",
    "        agg_col = 'epoch'\n",
    "        for i, dfg in metrics.groupby(agg_col):\n",
    "            agg = dict(dfg.mean())\n",
    "            agg[agg_col] = i\n",
    "            aggregation_metrics.append(agg)\n",
    "\n",
    "        df_metrics = pd.DataFrame(aggregation_metrics)\n",
    "        df_metrics[loss_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='loss')\n",
    "        \n",
    "        loss_png = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', f'{name_prepend}_loss_metric.png')\n",
    "        plt.savefig(loss_png)\n",
    "        \n",
    "        df_metrics[eval_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='accuracy')\n",
    "        \n",
    "        acc_png = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', f'{name_prepend}_acc_metric.png')\n",
    "        plt.savefig(acc_png)\n",
    "        \n",
    "        plt.close()\n",
    "    \n",
    "    def evaluate(self, eval_dataloader,\n",
    "                 give_confusion_matrix: bool=True, \n",
    "                 give_report: bool=True, \n",
    "                 give_f1_score: bool=False, \n",
    "                 give_accuracy_score: bool=False, \n",
    "                 give_precision_score: bool=False, \n",
    "                 give_recall_score: bool=False, \n",
    "                 give_hinge_loss: bool=False):\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        self.lightning_model.eval()\n",
    "        for X, y in eval_dataloader:\n",
    "            y_p = self.lightning_model(X.to(self.device))\n",
    "            if type(y_p) is tuple:\n",
    "                y_p = y_p[0]\n",
    "            y_pred.append((y_p>0).to(torch.int32).detach().to(y.device))\n",
    "            y_true.append(y.to(torch.int32))\n",
    "        y_true = torch.concat(y_true)\n",
    "        y_pred = torch.concat(y_pred)\n",
    "        if(give_confusion_matrix):\n",
    "            print(f'confusion_matrix: \\n{confusion_matrix(y_true, y_pred)}')\n",
    "        if(give_report):\n",
    "            print(classification_report(y_true, y_pred))\n",
    "        if(give_f1_score):\n",
    "            print(f'f1_score: {f1_score(y_true, y_pred)}')\n",
    "        if(give_accuracy_score):\n",
    "            print(f'accuracy_score: {accuracy_score(y_true, y_pred)}')\n",
    "        if(give_precision_score):\n",
    "            print(f'precision_score: {precision_score(y_true, y_pred)}')\n",
    "        if(give_recall_score):\n",
    "            print(f'recall_score: {recall_score(y_true, y_pred)}')\n",
    "        # if(give_hinge_loss):\n",
    "        #     print(f'hinge_loss: {hinge_loss(y_true, y_pred)}')\n",
    "                \n",
    "    def evaluate_best_models(self, lightning_type: L.LightningModule, eval_dataloader,\n",
    "                             give_confusion_matrix: bool=True, \n",
    "                             give_report: bool=True, \n",
    "                             give_f1_score: bool=False, \n",
    "                             give_accuracy_score: bool=False, \n",
    "                             give_precision_score: bool=False, \n",
    "                             give_recall_score: bool=False, \n",
    "                             give_hinge_loss: bool=False,\n",
    "                             multi_class: bool=False, **kwargs):\n",
    "        self.lightning_model = lightning_type.load_from_checkpoint(rf'{self.trainer.checkpoint_callback.best_model_path}', map_location=None, hparams_file=None, strict=True, **kwargs).eval()\n",
    "        self.save_evaluation(eval_dataloader, 'best_model', give_confusion_matrix, give_report,\n",
    "                             give_f1_score, give_accuracy_score, give_precision_score, give_recall_score, give_hinge_loss, multi_class)\n",
    "            \n",
    "    def save_evaluation(self, eval_dataloader, name_prepend: str='',\n",
    "                    give_confusion_matrix: bool=True, \n",
    "                    give_report: bool=True, \n",
    "                    give_f1_score: bool=False, \n",
    "                    give_accuracy_score: bool=False, \n",
    "                    give_precision_score: bool=False, \n",
    "                    give_recall_score: bool=False, \n",
    "                    give_hinge_loss: bool=False,\n",
    "                    multi_class: bool=False\n",
    "                    ):\n",
    "            \n",
    "            test_metrics_path = path.join(self.log_dir, self.log_name, f'version_{self.logger.version}', f'{name_prepend}_test_metrics.txt')\n",
    "            \n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            self.lightning_model.eval()\n",
    "            self.lightning_model.model.eval()\n",
    "            self.torch_model.eval()\n",
    "            self.trainer.model.eval()\n",
    "            for X, y in eval_dataloader:\n",
    "                with torch.no_grad():\n",
    "                    y_p = self.lightning_model(X.to(self.device))\n",
    "                if type(y_p) is tuple:\n",
    "                    y_p = y_p[0]\n",
    "                \n",
    "                if multi_class:\n",
    "                    y_pred.append(y_p.detach().to(y.device))\n",
    "                    y_true.append(y)\n",
    "                else:\n",
    "                    y_pred.append((y_p>0).to(torch.int32).detach().to(y.device))\n",
    "                    y_true.append(y.to(torch.int32))\n",
    "                    \n",
    "            y_true = torch.concat(y_true)\n",
    "            y_pred = torch.concat(y_pred)\n",
    "            print(y_true.shape)\n",
    "            print(y_pred.shape)\n",
    "            if multi_class:\n",
    "                y_true_num = torch.argmax(y_true, dim=1)\n",
    "                y_pred_num = torch.argmax(y_pred, dim=1)\n",
    "            else:\n",
    "                y_true_num = y_true\n",
    "                y_pred_num = y_pred\n",
    "                \n",
    "            print(y_true_num.shape)\n",
    "            print(y_pred_num.shape)\n",
    "            with open(test_metrics_path, 'at+') as f:\n",
    "                if(give_confusion_matrix):\n",
    "                    print(f'confusion_matrix: \\n{confusion_matrix(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_report):\n",
    "                    print(classification_report(y_true_num, y_pred_num), file=f)\n",
    "                if(give_f1_score):\n",
    "                    if multi_class:\n",
    "                        print(f'f1_score: {f1_score(y_true_num, y_pred_num, average=None)}', file=f)\n",
    "                    else:\n",
    "                        print(f'f1_score: {f1_score(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_accuracy_score):\n",
    "                    print(f'accuracy_score: {accuracy_score(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_precision_score):\n",
    "                    if multi_class:\n",
    "                        print(f'precision: {precision_score(y_true_num, y_pred_num, average=None)}', file=f)\n",
    "                    else:\n",
    "                        print(f'precision: {precision_score(y_true_num, y_pred_num)}', file=f)\n",
    "                if(give_recall_score):\n",
    "                    if multi_class:\n",
    "                        print(f'recall: {recall_score(y_true_num, y_pred_num, average=None)}', file=f)\n",
    "                    else:\n",
    "                        print(f'recall: {recall_score(y_true_num, y_pred_num)}', file=f)\n",
    "                # if(give_hinge_loss):\n",
    "                #     print(f'hinge_loss: {hinge_loss(y_true_num, y_pred)}', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "def train_model(epochs=30, dropout=0.2, weight_decay=0.000012, lr=0.0002, amsgrad=False, fused=True):\n",
    "    classifier_torch_model = CNN_for_Text(num_embedding=num_embedding, hidden_dim=hidden_dim, embedding_dim=embedding_dim, pos_emb_size=3072, dropout=dropout, num_out_features=len(class_id), seed=seed, random_edges=4, lattice_edges=4, lattice_step=2, virtual_nodes=0, lattice_start_distance=2).to(device)\n",
    "    # optimizer = torch.optim.Adam(classifier_torch_model.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=amsgrad, fused=fused)\n",
    "    optimizer = torch.optim.AdamW(classifier_torch_model.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=amsgrad, fused=fused)\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 100, 150, 200, 250, 300, 350],gamma=0.5, verbose=False)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15, 20, 30, 40, 45,50,55],gamma=0.5, verbose=False)\n",
    "    loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "    classfier_lightning_model = CnnGnnClassifierLightningModel(classifier_torch_model, \n",
    "                                                        num_classes=len(class_id),\n",
    "                                                learning_rate=lr,\n",
    "                                                batch_size=batch_size,\n",
    "                                                optimizer=optimizer,\n",
    "                                                loss_func=loss_func,\n",
    "                                                lr_scheduler=lr_scheduler,\n",
    "                                                user_lr_scheduler=True\n",
    "                                                ).to(device)\n",
    "\n",
    "\n",
    "    model_manager = ClassifierModelManager(classifier_torch_model, classfier_lightning_model, log_name='CNN-GNN15_imdb_sentiments',device=device, num_train_epoch=epochs, accumulate_grad_batches=1)\n",
    "    # trainer = L.Trainer(\n",
    "    #             # callbacks=callbacks,\n",
    "    #             max_epochs=epochs,\n",
    "    #             accelerator= 'gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    #             logger=CSVLogger(save_dir='logs/', name='log2'), \n",
    "    #             num_sanity_val_steps=0,\n",
    "    #         #     default_root_dir='models\\model2_word_embedding-256-2'\n",
    "    #         )\n",
    "\n",
    "    train_dataset.reset_params()\n",
    "    train_dataset.position_j = 0\n",
    "    test_dataset.reset_params()\n",
    "    test_dataset.position_j = 0\n",
    "    \n",
    "    # train_dataset.section_i = 0\n",
    "    # train_dataset.each_section_i = np.zeros((train_dataset.num_sections, ), dtype=int)\n",
    "    # test_dataset.section_i = 0\n",
    "    # test_dataset.each_section_i = np.zeros((test_dataset.num_sections, ), dtype=int)\n",
    "    \n",
    "    model_manager.fit(train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)\n",
    "    model_manager.save_plot_csv_logger(loss_names=['train_loss_epoch', 'val_loss_epoch'], eval_names=['train_acc_epoch', 'val_acc_epoch'], name_prepend=f'tests_{dropout}_{weight_decay}_{lr}_{amsgrad}_{fused}')\n",
    "    model_manager.torch_model = model_manager.torch_model.to(device)\n",
    "    model_manager.save_evaluation(test_dataloader, f'{dropout}_{weight_decay}_{lr}]',True, True, True, True, True, True, True, multi_class=True)\n",
    "    # trainer.fit(classfier_lightning_model, train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)\n",
    "    classfier_lightning_model.model = classfier_lightning_model.model.eval()\n",
    "    classfier_lightning_model = classfier_lightning_model.eval()\n",
    "    calculate_metrics(classfier_lightning_model, test_dataloader)\n",
    "    model_manager.evaluate_best_models(CnnGnnClassifierLightningModel, test_dataloader,True, True, True, True, True, True, True, multi_class=True, model=classifier_torch_model, num_classes=len(class_id))\n",
    "    return model_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(2):\n",
    "#     time.sleep(30)\n",
    "#     torch.cuda.empty_cache()\n",
    "#     import gc\n",
    "#     gc.collect()\n",
    "#     train_model(60, 0.2, 0.000012, 0.0032, amsgrad=False, fused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(10, 0.2, 0.000012, 0.0032, amsgrad=False, fused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(3):\n",
    "#     train_model(60, 0.2, 0.000012, 0.0032, amsgrad=False, fused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(5):\n",
    "#     train_model(60, 0.2, 0.000012, 0.0032, amsgrad=False, fused=True)\n",
    "#     time.sleep(30)\n",
    "#     torch.cuda.empty_cache()\n",
    "#     import gc\n",
    "#     gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sp1 in [True, False]:\n",
    "    for sp2 in [True, False]:\n",
    "        for sp3 in [True, False]:\n",
    "            for i in range(3):\n",
    "                train_model(30, 0.2, 0.000012, 0.0032, amsgrad=False, fused=True, use_token_polarity=[sp1, sp2, sp3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(2):\n",
    "#     train_model(70, 0.2, 0.000012, 0.0032, amsgrad=False, fused=True)\n",
    "#     time.sleep(60)\n",
    "#     torch.cuda.empty_cache()\n",
    "#     import gc\n",
    "#     gc.collect()\n",
    "#     time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     train_model(40, 0.2, 0.000012, 0.0032, amsgrad=False, fused=False)\n",
    "#     time.sleep(60)\n",
    "#     torch.cuda.empty_cache()\n",
    "#     import gc\n",
    "#     gc.collect()\n",
    "#     time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from os import path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_results(execution_number=0, base_path='logs/CNN-GNN15_imdb_sentiments/'):\n",
    "    base_path = path.join(base_path, rf'version_{execution_number}')\n",
    "    best_ckpts = [f for f in os.listdir(path.join(base_path, 'checkpoints'))][:2]\n",
    "    best_epochs = [int(re.search(r'\\d+', ckpt).group()) for ckpt in best_ckpts ]\n",
    "    \n",
    "    metrics = pd.read_csv(path.join(base_path, 'metrics.csv'))\n",
    "\n",
    "    aggregation_metrics = []\n",
    "    agg_col = 'epoch'\n",
    "    for i, dfg in metrics.groupby(agg_col):\n",
    "        agg = dict(dfg.mean())\n",
    "        agg[agg_col] = i\n",
    "        aggregation_metrics.append(agg)\n",
    "    df_metrics = pd.DataFrame(aggregation_metrics)\n",
    "    \n",
    "    print(f\"{df_metrics['val_acc_epoch'][best_epochs]}\")\n",
    "    print(f\"{df_metrics['val_loss_epoch'][best_epochs]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30    0.906091\n",
      "54    0.905851\n",
      "Name: val_acc_epoch, dtype: float64\n",
      "30    0.311653\n",
      "54    0.345440\n",
      "Name: val_loss_epoch, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "get_best_results(50, 'logs\\CNN-GNN15_imdb_sentiments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoints = {'49': r'logs\\CNN-GNN15_imdb_sentiments\\version_49\\checkpoints\\epoch=42-step=4214.ckpt', \n",
    "          '50': r'logs\\CNN-GNN15_imdb_sentiments\\version_50\\checkpoints\\epoch=30-step=3038.ckpt',\n",
    "          '59': r'logs\\CNN-GNN15_imdb_sentiments\\version_59\\checkpoints\\epoch=42-step=4816.ckptt',\n",
    "          '60': r'logs\\CNN-GNN15_imdb_sentiments\\version_60\\checkpoints\\epoch=25-step=2912.ckpt',\n",
    "          '61': r'logs\\CNN-GNN15_imdb_sentiments\\version_61\\checkpoints\\epoch=51-step=5824.ckpt'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Their Result: </br>\n",
    "{'losses': 0.31136547923088076, </br>\n",
    " 'accuracies': 0.9054926658163265, </br>\n",
    " 'precisions': 0.9056822870000001, </br>\n",
    " 'recalls': 0.9054926640000002, </br>\n",
    " 'f1_scores': 0.9055874557407388}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18    0.90504\n",
      "30    0.90492\n",
      "Name: val_acc_epoch, dtype: float64\n",
      "18    0.252355\n",
      "30    0.317930\n",
      "Name: val_loss_epoch, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "get_best_results(27, 'logs/CNN-GNN14_imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_float_numbers(line_str):\n",
    "    line_str = str.split(line_str, sep=\":\")[-1]\n",
    "    all_floats = re.findall(r\"[-+]?(?:\\d*\\.*\\d+)\", line_str)\n",
    "    all_floats = np.array([float(fs) for fs in all_floats])\n",
    "    return all_floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_results(execution_number=0, base_path='logs/CNN-GNN15_imdb_sentiments/'):\n",
    "    base_path = os.path.join(base_path, rf'version_{execution_number}')\n",
    "    \n",
    "    best_ckpts = [f for f in os.listdir(path.join(base_path, 'checkpoints'))][:2]\n",
    "    best_epochs = [int(re.search(r'\\d+', ckpt).group()) for ckpt in best_ckpts ]\n",
    "    \n",
    "    metrics = pd.read_csv(path.join(base_path, 'metrics.csv'))\n",
    "\n",
    "    aggregation_metrics = []\n",
    "    agg_col = 'epoch'\n",
    "    for i, dfg in metrics.groupby(agg_col):\n",
    "        agg = dict(dfg.mean())\n",
    "        agg[agg_col] = i\n",
    "        aggregation_metrics.append(agg)\n",
    "    df_metrics = pd.DataFrame(aggregation_metrics)\n",
    "    \n",
    "    max_acc_idx = df_metrics['val_acc_epoch'][best_epochs].argmax()\n",
    "    # print(max_acc_idx)\n",
    "    # print(f\"{df_metrics['val_acc_epoch'][best_epochs]}\")\n",
    "    # print(f\"{df_metrics['val_loss_epoch'][best_epochs]}\")\n",
    "    \n",
    "    \n",
    "    with open(path.join(base_path, 'best_model_test_metrics.txt'), 'rt') as f:\n",
    "        all_lines = f.readlines()\n",
    "\n",
    "    accuracy = np.mean(get_float_numbers(all_lines[13]))\n",
    "    # print(f\"accuracy: {accuracy}\")\n",
    "    precision = np.mean(get_float_numbers(all_lines[14]))\n",
    "    # print(f\"precision: {precision}\")\n",
    "    recall = np.mean(get_float_numbers(all_lines[15]))\n",
    "    # print(f\"recall: {recall}\")\n",
    "    f1_score = 2*precision*recall/(precision+recall)\n",
    "    # print(f\"f1_score: {f1_score}\")\n",
    "    loss = df_metrics['val_loss_epoch'][best_epochs[max_acc_idx]]\n",
    "    # print(f\"loss: {loss}\")\n",
    "    return loss, accuracy, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean(execution_numbers, base_path='logs/CNN-GNN15_imdb_sentiments/'):\n",
    "    losses, accuracies, precisions, recalls, f1_scores = [], [], [], [], []\n",
    "    for i in execution_numbers:\n",
    "        loss, accuracy, precision, recall, f1_score = get_best_results(i, base_path)\n",
    "        losses.append(loss)\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1_score)\n",
    "    losses, accuracies, precisions, recalls, f1_scores = np.mean(losses),  np.mean(accuracies),  np.mean(precisions),  np.mean(recalls),  np.mean(f1_scores)\n",
    "    return {'losses': losses, 'accuracies':accuracies, 'precisions':precisions, 'recalls':recalls, 'f1_scores':f1_scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'losses': 0.31136547923088076,\n",
       " 'accuracies': 0.9054926658163265,\n",
       " 'precisions': 0.9056822870000001,\n",
       " 'recalls': 0.9054926640000002,\n",
       " 'f1_scores': 0.9055874557407388}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_mean([49, 50, 59, 60, 61], base_path='logs/CNN-GNN15_imdb_sentiments/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    si_all = {'0': r'logs\\CNN-GNN15_imdb_sentiments\\version_0\\checkpoints\\epoch=15-step=3136.ckpt', \n",
    "            #'0': r'logs\\CNN-GNN15_imdb_sentiments\\version_0\\checkpoints\\epoch=23-step=4704.ckpt',\n",
    "            '1': r'logs\\CNN-GNN15_imdb_sentiments\\version_1\\checkpoints\\epoch=20-step=4116.ckpt',\n",
    "            '2': r'logs\\CNN-GNN15_imdb_sentiments\\version_2\\checkpoints\\epoch=16-step=3332.ckpt'}\n",
    "\n",
    "    si_p1_p2 = {'3': r'logs\\CNN-GNN15_imdb_sentiments\\version_3\\checkpoints\\epoch=15-step=3136.ckpt',\n",
    "                # '4': r'logs\\CNN-GNN15_imdb_sentiments\\version_4\\checkpoints\\epoch=25-step=5096.ckpt',\n",
    "                '4': r'logs\\CNN-GNN15_imdb_sentiments\\version_4\\checkpoints\\epoch=27-step=5488.ckpt',\n",
    "                '5': r'logs\\CNN-GNN15_imdb_sentiments\\version_5\\checkpoints\\epoch=27-step=5488.ckpt',\n",
    "                # '5': r'logs\\CNN-GNN15_imdb_sentiments\\version_5\\checkpoints\\epoch=18-step=3724.ckpt'\n",
    "                }\n",
    "\n",
    "    si_p1_p3 = {'6': r'logs\\CNN-GNN15_imdb_sentiments\\version_6\\checkpoints\\epoch=25-step=5096.ckpt',\n",
    "                '7': r'logs\\CNN-GNN15_imdb_sentiments\\version_7\\checkpoints\\epoch=28-step=5684.ckpt',\n",
    "                '8': r'logs\\CNN-GNN15_imdb_sentiments\\version_8\\checkpoints\\epoch=20-step=4116.ckpt',\n",
    "                '8': r'logs\\CNN-GNN15_imdb_sentiments\\version_8\\checkpoints\\epoch=22-step=4508.ckpt'\n",
    "                }\n",
    "\n",
    "    si_p1 = {'9': r'logs\\CNN-GNN15_imdb_sentiments\\version_9\\checkpoints\\epoch=25-step=5096.ckpt',\n",
    "            '10': r'logs\\CNN-GNN15_imdb_sentiments\\version_10\\checkpoints\\epoch=15-step=3136.ckpt',\n",
    "            '11': r'logs\\CNN-GNN15_imdb_sentiments\\version_11\\checkpoints\\epoch=21-step=4312.ckpt'}\n",
    "\n",
    "    si_p2_p3 = {'12': r'logs\\CNN-GNN15_imdb_sentiments\\version_12\\checkpoints\\epoch=27-step=5488.ckpt',\n",
    "                '13': r'logs\\CNN-GNN15_imdb_sentiments\\version_13\\checkpoints\\epoch=27-step=5488.ckpt',\n",
    "                '14': r'logs\\CNN-GNN15_imdb_sentiments\\version_14\\checkpoints\\epoch=20-step=4116.ckpt'}\n",
    "\n",
    "    si_p2 = {'15': r'logs\\CNN-GNN15_imdb_sentiments\\version_15\\checkpoints\\epoch=21-step=4312.ckpt',\n",
    "            '16': r'logs\\CNN-GNN15_imdb_sentiments\\version_16\\checkpoints\\epoch=21-step=4312.ckpt',\n",
    "            '17': r'logs\\CNN-GNN15_imdb_sentiments\\version_17\\checkpoints\\epoch=19-step=3920.ckpt'}\n",
    "\n",
    "    si_p3 = {'18': r'logs\\CNN-GNN15_imdb_sentiments\\version_18\\checkpoints\\epoch=14-step=2940.ckpt',\n",
    "            '19': r'logs\\CNN-GNN15_imdb_sentiments\\version_19\\checkpoints\\epoch=16-step=3332.ckpt',\n",
    "            '20': r'logs\\CNN-GNN15_imdb_sentiments\\version_20\\checkpoints\\epoch=15-step=3136.ckpt'}\n",
    "\n",
    "    si_none = {'24': r'logs\\CNN-GNN15_imdb_sentiments\\version_24\\checkpoints\\epoch=24-step=4900.ckpt',\n",
    "           '25': r'logs\\CNN-GNN15_imdb_sentiments\\version_25\\checkpoints\\epoch=20-step=4116.ckpt',\n",
    "           '26': r'logs\\CNN-GNN15_imdb_sentiments\\version_26\\checkpoints\\epoch=25-step=5096.ckpt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_types = {'si_all': [True, True, True], 'si_p1_p2':[True, True, False], 'si_p1_p3':[True, False, True], 'si_p1':[True, False, False], 'si_p2_p3':[False, True, True], 'si_p2':[False, True, False], 'si_p3':[False, False, True], 'si_none':[False, False, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:198: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:198: Attribute 'loss_func' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_func'])`.\n"
     ]
    }
   ],
   "source": [
    "classifier_torch_model = CNN_for_Text(num_embedding=num_embedding, hidden_dim=hidden_dim, embedding_dim=embedding_dim, pos_emb_size=3072, dropout=0.2, num_out_features=len(class_id), seed=seed, random_edges=4, lattice_edges=4, lattice_step=2, virtual_nodes=0, lattice_start_distance=2, use_token_polarity=si_types['si_all'])\n",
    "classfier_lightning_model = CnnGnnClassifierLightningModel.load_from_checkpoint(si_all['2'], model=classifier_torch_model, num_classes=len(class_id)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpt = torch.load(si_all['2'], map_location=lambda storage, loc: storage)\n",
    "model_state_dict = classifier_torch_model.state_dict()\n",
    "for k in checkpt['state_dict']:\n",
    "    if k[6:] in model_state_dict:\n",
    "        model_state_dict[k[6:]] = checkpt['state_dict'][k]\n",
    "    else:\n",
    "        print(f\"Error: it doesn't have the key: {k[6:]}\")\n",
    "classifier_torch_model.load_state_dict(model_state_dict)\n",
    "classifier_torch_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_model = CnnGnnClassifierLightningModel.load_from_checkpoint(r\"logs\\CNN-GNN15_imdb_sentiments\\version_0\\checkpoints\\epoch=15-step=3136.ckpt\", map_location=None, hparams_file=None, strict=True, model=classifier_torch_model, num_classes=len(class_id)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    total_accuracy = []\n",
    "    total_f1 = []\n",
    "    total_prec = []\n",
    "    total_rec = []\n",
    "    seed=911\n",
    "\n",
    "    for k in si_all:    \n",
    "        classifier_torch_model = CNN_for_Text(num_embedding=num_embedding, hidden_dim=hidden_dim, embedding_dim=embedding_dim, pos_emb_size=3072, dropout=0.2, num_out_features=len(class_id), seed=seed, random_edges=4, lattice_edges=4, lattice_step=2, virtual_nodes=0, lattice_start_distance=2, use_token_polarity=si_types['si_all'])\n",
    "        \n",
    "        checkpt = torch.load(si_all[k], map_location=lambda storage, loc: storage)\n",
    "        model_state_dict = classifier_torch_model.state_dict()\n",
    "        for k in checkpt['state_dict']:\n",
    "            if k[6:] in model_state_dict:\n",
    "                model_state_dict[k[6:]] = checkpt['state_dict'][k]\n",
    "            else:\n",
    "                print(f\"Error: it doesn't have the key: {k[6:]}\")\n",
    "        classifier_torch_model.load_state_dict(model_state_dict)\n",
    "        classifier_torch_model.to(device)\n",
    "        classifier_torch_model.eval()\n",
    "        \n",
    "        # classfier_lightning_model = CnnGnnClassifierLightningModel.load_from_checkpoint(si_all[k], map_location=None, hparams_file=None, strict=True, model=classifier_torch_model, num_classes=len(class_id)).eval()\n",
    "        mean_infer_acc = []\n",
    "        mean_infer_f1 = []\n",
    "        mean_infer_prec = []\n",
    "        mean_infer_rec = []\n",
    "        for i in range(5):\n",
    "            all_ys = []\n",
    "            all_y_preds = []\n",
    "            for X, y in test_dataloader:\n",
    "                with torch.no_grad():\n",
    "                    y_pred = classifier_torch_model(X.to(device))\n",
    "                all_ys.append(torch.argmax(y,dim=1))\n",
    "                all_y_preds.append(torch.argmax(y_pred.cpu(), dim=1))\n",
    "            all_ys = torch.concat(all_ys)\n",
    "            all_y_preds = torch.concat(all_y_preds)\n",
    "            \n",
    "            cm = confusion_matrix(all_ys, all_y_preds)\n",
    "            \n",
    "            accuracy = np.sum(np.diag(cm))/ np.sum(cm)\n",
    "            precision = np.mean(np.diag(cm) / np.sum(cm, axis=0))\n",
    "            recall = np.mean(np.diag(cm) / np.sum(cm, axis=1))\n",
    "            f1_score = (2*precision*recall)/(precision + recall)\n",
    "            \n",
    "            mean_infer_acc.append(accuracy)\n",
    "            mean_infer_f1.append(f1_score)\n",
    "            mean_infer_prec.append(precision)\n",
    "            mean_infer_rec.append(recall)\n",
    "        mean_infer_acc = torch.mean(torch.tensor(mean_infer_acc))\n",
    "        mean_infer_f1 = torch.mean(torch.tensor(mean_infer_f1))\n",
    "        mean_infer_prec = torch.mean(torch.tensor(mean_infer_prec))\n",
    "        mean_infer_rec = torch.mean(torch.tensor(mean_infer_rec))\n",
    "        \n",
    "        print(f'accuracy {k}: {mean_infer_acc}')\n",
    "        print(f'f1 {k}: {mean_infer_f1}')\n",
    "        print(f'prec {k}: {mean_infer_prec}')\n",
    "        print(f'rec {k}: {mean_infer_rec}')\n",
    "        total_accuracy.append(mean_infer_acc)\n",
    "        total_f1.append(mean_infer_f1)\n",
    "        total_prec.append(mean_infer_prec)\n",
    "        total_rec.append(mean_infer_rec)\n",
    "\n",
    "    total_accuracy = torch.mean(torch.tensor(total_accuracy))\n",
    "    total_f1 = torch.mean(torch.tensor(total_f1))\n",
    "    total_prec = torch.mean(torch.tensor(total_prec))\n",
    "    total_rec = torch.mean(torch.tensor(total_rec))\n",
    "    print(f'total_accuracy: {total_accuracy}')\n",
    "    print(f'total_f1: {total_f1}')\n",
    "    print(f'total_prec: {total_prec}')\n",
    "    print(f'total_rec: {total_rec}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoints = {'0': r'logs\\CNN-GNN14_imdb\\version_0\\checkpoints\\epoch=21-step=4312.ckpt',\n",
    "                    '1': r'logs\\CNN-GNN14_imdb\\version_1\\checkpoints\\epoch=16-step=3332.ckpt',\n",
    "                    '2': r'logs\\CNN-GNN14_imdb\\version_2\\checkpoints\\epoch=15-step=3136.ckpt',\n",
    "                    '3': r'logs\\CNN-GNN14_imdb\\version_3\\checkpoints\\epoch=21-step=4312.ckpt',\n",
    "                    '4': r'logs\\CNN-GNN14_imdb\\version_4\\checkpoints\\epoch=21-step=4312.ckpt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:198: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:198: Attribute 'loss_func' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_func'])`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301590\n",
      "301592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fardin\\AppData\\Local\\Temp\\ipykernel_1844\\2901102002.py:29: RuntimeWarning: invalid value encountered in divide\n",
      "  precision = np.mean(np.diag(cm) / np.sum(cm, axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fardin\\AppData\\Local\\Temp\\ipykernel_1844\\2901102002.py:29: RuntimeWarning: invalid value encountered in divide\n",
      "  precision = np.mean(np.diag(cm) / np.sum(cm, axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 49: 0.5000239158163265\n",
      "f1 49: nan\n",
      "prec 49: nan\n",
      "rec 49: 0.5000239158163265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:198: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:198: Attribute 'loss_func' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_func'])`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301571\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\Form&ColorIntelligence\\notebooks\\Task1_X_3\\text_classification\\text_classification_test_2_imdb.ipynb Cell 59\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Form%26ColorIntelligence/notebooks/Task1_X_3/text_classification/text_classification_test_2_imdb.ipynb#Y106sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m all_ys \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Form%26ColorIntelligence/notebooks/Task1_X_3/text_classification/text_classification_test_2_imdb.ipynb#Y106sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m all_y_preds \u001b[39m=\u001b[39m []\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Form%26ColorIntelligence/notebooks/Task1_X_3/text_classification/text_classification_test_2_imdb.ipynb#Y106sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;49;00m X, y \u001b[39min\u001b[39;49;00m test_dataloader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Form%26ColorIntelligence/notebooks/Task1_X_3/text_classification/text_classification_test_2_imdb.ipynb#Y106sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mwith\u001b[39;49;00m torch\u001b[39m.\u001b[39;49mno_grad():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Form%26ColorIntelligence/notebooks/Task1_X_3/text_classification/text_classification_test_2_imdb.ipynb#Y106sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         y_pred \u001b[39m=\u001b[39;49m classfier_lightning_model(X\u001b[39m.\u001b[39;49mto(device))\n",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\Form&ColorIntelligence\\notebooks\\Task1_X_3\\text_classification\\text_classification_test_2_imdb.ipynb Cell 59\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Form%26ColorIntelligence/notebooks/Task1_X_3/text_classification/text_classification_test_2_imdb.ipynb#Y106sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Form%26ColorIntelligence/notebooks/Task1_X_3/text_classification/text_classification_test_2_imdb.ipynb#Y106sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     base_iterator \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(CharacterandTokenLevelDataLoader, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Form%26ColorIntelligence/notebooks/Task1_X_3/text_classification/text_classification_test_2_imdb.ipynb#Y106sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mfor\u001b[39;49;00m batch \u001b[39min\u001b[39;49;00m base_iterator:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Form%26ColorIntelligence/notebooks/Task1_X_3/text_classification/text_classification_test_2_imdb.ipynb#Y106sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         cumsum_vals \u001b[39m=\u001b[39;49m torch\u001b[39m.\u001b[39;49mcumsum(batch[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mnum_tokens, dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mroll(\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Form%26ColorIntelligence/notebooks/Task1_X_3/text_classification/text_classification_test_2_imdb.ipynb#Y106sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         cumsum_vals[\u001b[39m0\u001b[39;49m] \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch_geometric\\loader\\dataloader.py:55\u001b[0m, in \u001b[0;36mCollater.collate_fn\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset, OnDiskDataset):\n\u001b[0;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mmulti_get(batch))\n\u001b[1;32m---> 55\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(batch)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch_geometric\\loader\\dataloader.py:48\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(elem)(\u001b[39m*\u001b[39m(\u001b[39mself\u001b[39m(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch)))\n\u001b[0;32m     47\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, Sequence) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mstr\u001b[39m):\n\u001b[1;32m---> 48\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39;49m(s) \u001b[39mfor\u001b[39;49;00m s \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(\u001b[39m*\u001b[39;49mbatch)]\n\u001b[0;32m     50\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDataLoader found invalid type: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(elem)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch_geometric\\loader\\dataloader.py:48\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(elem)(\u001b[39m*\u001b[39m(\u001b[39mself\u001b[39m(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch)))\n\u001b[0;32m     47\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, Sequence) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mstr\u001b[39m):\n\u001b[1;32m---> 48\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39;49m(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch)]\n\u001b[0;32m     50\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDataLoader found invalid type: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(elem)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch_geometric\\loader\\dataloader.py:28\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     26\u001b[0m elem \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m]\n\u001b[0;32m     27\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, BaseData):\n\u001b[1;32m---> 28\u001b[0m     \u001b[39mreturn\u001b[39;00m Batch\u001b[39m.\u001b[39;49mfrom_data_list(\n\u001b[0;32m     29\u001b[0m         batch,\n\u001b[0;32m     30\u001b[0m         follow_batch\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfollow_batch,\n\u001b[0;32m     31\u001b[0m         exclude_keys\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexclude_keys,\n\u001b[0;32m     32\u001b[0m     )\n\u001b[0;32m     33\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m     34\u001b[0m     \u001b[39mreturn\u001b[39;00m default_collate(batch)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch_geometric\\data\\batch.py:93\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[1;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m     82\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_data_list\u001b[39m(\u001b[39mcls\u001b[39m, data_list: List[BaseData],\n\u001b[0;32m     83\u001b[0m                    follow_batch: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     84\u001b[0m                    exclude_keys: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     85\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[39m    Python list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[39m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[39m    :obj:`follow_batch`.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[39m    Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m     batch, slice_dict, inc_dict \u001b[39m=\u001b[39m collate(\n\u001b[0;32m     94\u001b[0m         \u001b[39mcls\u001b[39;49m,\n\u001b[0;32m     95\u001b[0m         data_list\u001b[39m=\u001b[39;49mdata_list,\n\u001b[0;32m     96\u001b[0m         increment\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     97\u001b[0m         add_batch\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(data_list[\u001b[39m0\u001b[39;49m], Batch),\n\u001b[0;32m     98\u001b[0m         follow_batch\u001b[39m=\u001b[39;49mfollow_batch,\n\u001b[0;32m     99\u001b[0m         exclude_keys\u001b[39m=\u001b[39;49mexclude_keys,\n\u001b[0;32m    100\u001b[0m     )\n\u001b[0;32m    102\u001b[0m     batch\u001b[39m.\u001b[39m_num_graphs \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data_list)\n\u001b[0;32m    103\u001b[0m     batch\u001b[39m.\u001b[39m_slice_dict \u001b[39m=\u001b[39m slice_dict\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch_geometric\\data\\collate.py:92\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[39m# Collate attributes into a unified representation:\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m value, slices, incs \u001b[39m=\u001b[39m _collate(attr, values, data_list, stores,\n\u001b[0;32m     93\u001b[0m                                increment)\n\u001b[0;32m     95\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, Tensor) \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mis_cuda:\n\u001b[0;32m     96\u001b[0m     device \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mdevice\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch_geometric\\data\\collate.py:139\u001b[0m, in \u001b[0;36m_collate\u001b[1;34m(key, values, data_list, stores, increment)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mif\u001b[39;00m cat_dim \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m elem\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    138\u001b[0m     values \u001b[39m=\u001b[39m [value\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m values]\n\u001b[1;32m--> 139\u001b[0m sizes \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([value\u001b[39m.\u001b[39;49msize(cat_dim \u001b[39mor\u001b[39;49;00m \u001b[39m0\u001b[39;49m) \u001b[39mfor\u001b[39;49;00m value \u001b[39min\u001b[39;49;00m values])\n\u001b[0;32m    140\u001b[0m slices \u001b[39m=\u001b[39m cumsum(sizes)\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m increment:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    total_accuracy = []\n",
    "    total_f1 = []\n",
    "    total_prec = []\n",
    "    total_rec = []\n",
    "\n",
    "\n",
    "    for k in best_checkpoints:\n",
    "        classifier_torch_model = CNN_for_Text(num_embedding=num_embedding, hidden_dim=hidden_dim, embedding_dim=embedding_dim, pos_emb_size=3072, dropout=0.2, num_out_features=len(class_id), seed=seed, random_edges=4, lattice_edges=4, lattice_step=2, virtual_nodes=0, lattice_start_distance=2)\n",
    "        classfier_lightning_model = CnnGnnClassifierLightningModel.load_from_checkpoint(best_checkpoints[k], model=classifier_torch_model, num_classes=len(class_id)).eval()\n",
    "        mean_infer_acc = []\n",
    "        mean_infer_f1 = []\n",
    "        mean_infer_prec = []\n",
    "        mean_infer_rec = []\n",
    "        for i in range(5):\n",
    "            all_ys = []\n",
    "            all_y_preds = []\n",
    "            for X, y in test_dataloader:\n",
    "                with torch.no_grad():\n",
    "                    y_pred = classfier_lightning_model(X.to(device))\n",
    "                all_ys.append(torch.argmax(y,dim=1))\n",
    "                all_y_preds.append(torch.argmax(y_pred.cpu(), dim=1))\n",
    "            all_ys = torch.concat(all_ys)\n",
    "            all_y_preds = torch.concat(all_y_preds)\n",
    "            \n",
    "            cm = confusion_matrix(all_ys, all_y_preds)\n",
    "            \n",
    "            accuracy = np.sum(np.diag(cm))/ np.sum(cm)\n",
    "            precision = np.mean(np.diag(cm) / np.sum(cm, axis=0))\n",
    "            recall = np.mean(np.diag(cm) / np.sum(cm, axis=1))\n",
    "            f1_score = (2*precision*recall)/(precision + recall)\n",
    "            \n",
    "            mean_infer_acc.append(accuracy)\n",
    "            mean_infer_f1.append(f1_score)\n",
    "            mean_infer_prec.append(precision)\n",
    "            mean_infer_rec.append(recall)\n",
    "        mean_infer_acc = torch.mean(torch.tensor(mean_infer_acc))\n",
    "        mean_infer_f1 = torch.mean(torch.tensor(mean_infer_f1))\n",
    "        mean_infer_prec = torch.mean(torch.tensor(mean_infer_prec))\n",
    "        mean_infer_rec = torch.mean(torch.tensor(mean_infer_rec))\n",
    "        \n",
    "        print(f'accuracy {k}: {mean_infer_acc}')\n",
    "        print(f'f1 {k}: {mean_infer_f1}')\n",
    "        print(f'prec {k}: {mean_infer_prec}')\n",
    "        print(f'rec {k}: {mean_infer_rec}')\n",
    "        total_accuracy.append(mean_infer_acc)\n",
    "        total_f1.append(mean_infer_f1)\n",
    "        total_prec.append(mean_infer_prec)\n",
    "        total_rec.append(mean_infer_rec)\n",
    "\n",
    "    total_accuracy = torch.mean(torch.tensor(total_accuracy))\n",
    "    total_f1 = torch.mean(torch.tensor(total_f1))\n",
    "    total_prec = torch.mean(torch.tensor(total_prec))\n",
    "    total_rec = torch.mean(torch.tensor(total_rec))\n",
    "    print(f'total_accuracy: {total_accuracy}')\n",
    "    print(f'total_f1: {total_f1}')\n",
    "    print(f'total_prec: {total_prec}')\n",
    "    print(f'total_rec: {total_rec}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN-GNN14_imdb\n",
    "# fused on:\n",
    "best_checkpoints_fused_on = {'18': r'logs\\CNN-GNN14_imdb\\version_18\\checkpoints\\epoch=23-step=4704.ckpt',\n",
    "                             '19': r'logs\\CNN-GNN14_imdb\\version_19\\checkpoints\\epoch=21-step=4312.ckpt',\n",
    "                             '20': r'logs\\CNN-GNN14_imdb\\version_20\\checkpoints\\epoch=30-step=6076.ckpt',\n",
    "                             '21': r'logs\\CNN-GNN14_imdb\\version_21\\checkpoints\\epoch=21-step=4312.ckpt',\n",
    "                             '22': r'logs\\CNN-GNN14_imdb\\version_22\\checkpoints\\epoch=25-step=5096.ckpt',\n",
    "                             }\n",
    "\n",
    "# fused on:\n",
    "best_checkpoints_fused_off = {'23': r'logs\\CNN-GNN14_imdb\\version_23\\checkpoints\\epoch=21-step=4312.ckpt',\n",
    "                              '24': r'logs\\CNN-GNN14_imdb\\version_24\\checkpoints\\epoch=34-step=6860.ckpt',\n",
    "                              '25': r'logs\\CNN-GNN14_imdb\\version_25\\checkpoints\\epoch=34-step=6860.ckpt',\n",
    "                              '26': r'logs\\CNN-GNN14_imdb\\version_26\\checkpoints\\epoch=32-step=6468.ckpt',\n",
    "                              '27': r'logs\\CNN-GNN14_imdb\\version_27\\checkpoints\\epoch=18-step=3724.ckpt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'losses': 0.2982066869735718,\n",
       " 'accuracies': 0.903208,\n",
       " 'precisions': 0.9034025120000001,\n",
       " 'recalls': 0.903208,\n",
       " 'f1_scores': 0.9033052366785561}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_mean([23, 24, 25, 26, 27], 'logs/CNN-GNN14_imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'losses': 0.3015179574489594,\n",
       " 'accuracies': 0.9037200000000001,\n",
       " 'precisions': 0.9037939429999999,\n",
       " 'recalls': 0.9037200000000001,\n",
       " 'f1_scores': 0.9037569696843178}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_mean([18, 19, 20, 21, 22], 'logs/CNN-GNN14_imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CNN_for_Text:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([96, 64]) from checkpoint, the shape in current model is torch.Size([207, 64]).\n\tsize mismatch for conv3.weight: copying a param with shape torch.Size([64, 130, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 3]).\n\tsize mismatch for conv5_sentiment.weight: copying a param with shape torch.Size([64, 64, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 3]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\Form&ColorIntelligence\\notebooks\\Task1_X_3\\text_classification\\text_classification_test_2_imdb_sentiment_injection.ipynb Cell 61\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Form%26ColorIntelligence/notebooks/Task1_X_3/text_classification/text_classification_test_2_imdb_sentiment_injection.ipynb#X64sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Form%26ColorIntelligence/notebooks/Task1_X_3/text_classification/text_classification_test_2_imdb_sentiment_injection.ipynb#X64sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError: it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have the key: \u001b[39m\u001b[39m{\u001b[39;00mk[\u001b[39m6\u001b[39m:]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Form%26ColorIntelligence/notebooks/Task1_X_3/text_classification/text_classification_test_2_imdb_sentiment_injection.ipynb#X64sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m classifier_torch_model\u001b[39m.\u001b[39;49mload_state_dict(model_state_dict)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Form%26ColorIntelligence/notebooks/Task1_X_3/text_classification/text_classification_test_2_imdb_sentiment_injection.ipynb#X64sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m classifier_torch_model\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/Form%26ColorIntelligence/notebooks/Task1_X_3/text_classification/text_classification_test_2_imdb_sentiment_injection.ipynb#X64sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m classifier_torch_model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2147\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[0;32m   2148\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2149\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2151\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2152\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2153\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2154\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CNN_for_Text:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([96, 64]) from checkpoint, the shape in current model is torch.Size([207, 64]).\n\tsize mismatch for conv3.weight: copying a param with shape torch.Size([64, 130, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 3]).\n\tsize mismatch for conv5_sentiment.weight: copying a param with shape torch.Size([64, 64, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 3])."
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    total_accuracy = []\n",
    "    total_f1 = []\n",
    "    total_prec = []\n",
    "    total_rec = []\n",
    "\n",
    "    for k in best_checkpoints_fused_on:\n",
    "        classifier_torch_model = CNN_for_Text(num_embedding=num_embedding, hidden_dim=hidden_dim, embedding_dim=embedding_dim, pos_emb_size=3072, dropout=0.2, num_out_features=len(class_id), seed=seed, random_edges=4, lattice_edges=4, lattice_step=2, virtual_nodes=0, lattice_start_distance=2)\n",
    "        \n",
    "        checkpt = torch.load(best_checkpoints_fused_on[k], map_location=lambda storage, loc: storage)\n",
    "        model_state_dict = classifier_torch_model.state_dict()\n",
    "        for k in checkpt['state_dict']:\n",
    "            if k[6:] in model_state_dict:\n",
    "                model_state_dict[k[6:]] = checkpt['state_dict'][k]\n",
    "            else:\n",
    "                print(f\"Error: it doesn't have the key: {k[6:]}\")\n",
    "        classifier_torch_model.load_state_dict(model_state_dict)\n",
    "        classifier_torch_model.to(device)\n",
    "        classifier_torch_model.eval()\n",
    "        \n",
    "        \n",
    "        # classifier_torch_model = CNN_for_Text(num_embedding=num_embedding, hidden_dim=hidden_dim, embedding_dim=embedding_dim, pos_emb_size=8192, dropout=0.2, num_out_features=len(class_id), seed=seed, random_edges=4, lattice_edges=4, lattice_step=2, virtual_nodes=0, lattice_start_distance=2)\n",
    "        # classfier_lightning_model = CnnGnnClassifierLightningModel.load_from_checkpoint(best_checkpoints_fused_on[k], model=classifier_torch_model, num_classes=len(class_id)).eval()\n",
    "        mean_infer_acc = []\n",
    "        mean_infer_f1 = []\n",
    "        mean_infer_prec = []\n",
    "        mean_infer_rec = []\n",
    "        for i in range(5):\n",
    "            all_ys = []\n",
    "            all_y_preds = []\n",
    "            for X, y in test_dataloader:\n",
    "                with torch.no_grad():\n",
    "                    y_pred = classfier_lightning_model(X.to(device))\n",
    "                all_ys.append(torch.argmax(y,dim=1))\n",
    "                all_y_preds.append(torch.argmax(y_pred.cpu(), dim=1))\n",
    "            all_ys = torch.concat(all_ys)\n",
    "            all_y_preds = torch.concat(all_y_preds)\n",
    "            \n",
    "            cm = confusion_matrix(all_ys, all_y_preds)\n",
    "            \n",
    "            accuracy = np.sum(np.diag(cm))/ np.sum(cm)\n",
    "            precision = np.mean(np.diag(cm) / np.sum(cm, axis=0))\n",
    "            recall = np.mean(np.diag(cm) / np.sum(cm, axis=1))\n",
    "            f1_score = (2*precision*recall)/(precision + recall)\n",
    "            \n",
    "            mean_infer_acc.append(accuracy)\n",
    "            mean_infer_f1.append(f1_score)\n",
    "            mean_infer_prec.append(precision)\n",
    "            mean_infer_rec.append(recall)\n",
    "        mean_infer_acc = torch.mean(torch.tensor(mean_infer_acc))\n",
    "        mean_infer_f1 = torch.mean(torch.tensor(mean_infer_f1))\n",
    "        mean_infer_prec = torch.mean(torch.tensor(mean_infer_prec))\n",
    "        mean_infer_rec = torch.mean(torch.tensor(mean_infer_rec))\n",
    "        \n",
    "        print(f'accuracy {k}: {mean_infer_acc}')\n",
    "        print(f'f1 {k}: {mean_infer_f1}')\n",
    "        print(f'prec {k}: {mean_infer_prec}')\n",
    "        print(f'rec {k}: {mean_infer_rec}')\n",
    "        total_accuracy.append(mean_infer_acc)\n",
    "        total_f1.append(mean_infer_f1)\n",
    "        total_prec.append(mean_infer_prec)\n",
    "        total_rec.append(mean_infer_rec)\n",
    "\n",
    "    total_accuracy = torch.mean(torch.tensor(total_accuracy))\n",
    "    total_f1 = torch.mean(torch.tensor(total_f1))\n",
    "    total_prec = torch.mean(torch.tensor(total_prec))\n",
    "    total_rec = torch.mean(torch.tensor(total_rec))\n",
    "    print(f'total_accuracy: {total_accuracy}')\n",
    "    print(f'total_f1: {total_f1}')\n",
    "    print(f'total_prec: {total_prec}')\n",
    "    print(f'total_rec: {total_rec}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoint = r'logs\\CNN-GNN14_imdb\\version_35\\checkpoints\\epoch=29-step=5880.ckpt'\n",
    "classifier_torch_model = CNN_for_Text(num_embedding=num_embedding, hidden_dim=hidden_dim, embedding_dim=embedding_dim, pos_emb_size=3072, dropout=0.2, num_out_features=len(class_id), seed=seed, random_edges=4, lattice_edges=4, lattice_step=2, virtual_nodes=0, lattice_start_distance=2)\n",
    "classfier_lightning_model = CnnGnnClassifierLightningModel.load_from_checkpoint(best_checkpoint, model=classifier_torch_model, num_classes=len(class_id)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = CharacterandTokenLevelDataLoader(test_dataset, batch_size=batch_size, drop_last=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ys = []\n",
    "all_y_preds = []\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    with torch.no_grad():\n",
    "        y_pred = classfier_lightning_model(X.to(device))\n",
    "    all_ys.append(torch.argmax(y,dim=1))\n",
    "    all_y_preds.append(torch.argmax(y_pred.cpu(), dim=1))\n",
    "all_ys = torch.concat(all_ys)\n",
    "all_y_preds = torch.concat(all_y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_answers = torch.where(all_ys!=all_y_preds)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Negative', 1: 'Positive'}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_vocab_dict['monument']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: Negative\n",
      "predict:  Positive\n",
      "Great music, but ain't these people PATHETIC?!? A true period piece of The Trippy Sixties, and it left me depressed. The director paints the wrong side of the jetset life and it stings as a hornets nest. If the culture of the time led people to do these things, it appears to me that it was all a journey of no discovery, only despair. I tried, really tried, to like this film, but these people aren't anywhere on my page. Yes, it would be nice to see the world, go away for awhile, but I always plan to come BACK. Drugs aren't the cause of these characters' downfalls, it's their lousy attitudes  these guys passionately drink their cup of poison. They cheapen their lives, and in the end, cheapen the journey that is life. Has romance ever been so dark?<br /><br />Cheers: Interesting scenery. Wonderful soundtrack by Pink Floyd.<br /><br />Caveats: Dated. Drugs. Depressing. Thoroughly unlikable characters; they aren't flower children.<br /><br />Only for the curious, since most packages swoon The Pink Floyd connection. ( Rare Floyd tracks many will have never heard before, as FM ain't what it was. )<br /><br />Rating: Two Stars.\n"
     ]
    }
   ],
   "source": [
    "target = wrong_answers[10]\n",
    "print(f'original: {id_class[all_ys[target].item()]}')\n",
    "print(f'predict:  {id_class[all_y_preds[target].item()]}')\n",
    "print(test_df.Content.values[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\"\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_tokenizer.tokenize(test_df.Content.values[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_tokenizer.tokenize(test_df.Content.values[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [t for t in nlp(test_df.Content.values[target])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Data(x=[1638], token_positions=[324], character_length=1638, num_tokens=324, token_indices=[1638], token_lengths=[324], token_embeddings=[324, 64], token_sentiments=[324, 2]),\n",
       " tensor([0., 1.]))"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[wrong_answers[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5690, 0.4279, 0.4346, 0.4461, 0.3818, 0.4812, 0.4366, 0.4414, 0.4344,\n",
       "        0.4238, 0.4482, 0.4549, 0.4248, 0.4391, 0.4586, 0.4327, 0.4505, 0.4512,\n",
       "        0.4434, 0.4419, 0.4308, 0.4346, 0.4399, 0.4611, 0.4297, 0.4528, 0.4385,\n",
       "        0.4429, 0.4572, 0.4548, 0.4501, 0.4404, 0.4428, 0.4398, 0.4401, 0.4505,\n",
       "        0.4367, 0.4604, 0.4323, 0.4415, 0.4319, 0.4389, 0.4170, 0.4280, 0.4440,\n",
       "        0.4366, 0.4440, 0.4463, 0.4444, 0.4423, 0.4420, 0.4493, 0.4414, 0.4523,\n",
       "        0.4494, 0.4485, 0.4456, 0.4471, 0.4572, 0.3837, 0.4540, 0.4410, 0.4355,\n",
       "        0.4449])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token_vocab_dict['Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2380])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_answers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.flop_counter import FlopCounterMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module                          FLOP    % Total\n",
      "------------------------  ----------  ---------\n",
      "CNN_for_Text              45366.445M    100.00%\n",
      " - aten.convolution       21007.046M     46.31%\n",
      " - aten.addmm             24359.399M     53.69%\n",
      " CNN_for_Text.conv1        8175.821M     18.02%\n",
      "  - aten.convolution       8175.821M     18.02%\n",
      " CNN_for_Text.conv2        8175.821M     18.02%\n",
      "  - aten.convolution       8175.821M     18.02%\n",
      " CNN_for_Text.conv3        2086.906M      4.60%\n",
      "  - aten.convolution       2086.906M      4.60%\n",
      " CNN_for_Text.gcnn1         856.166M      1.89%\n",
      "  - aten.addmm              342.467M      0.75%\n",
      "  - aten.convolution        513.700M      1.13%\n",
      " CNN_for_Text.sentiment2    695.635M      1.53%\n",
      "  - aten.addmm              695.635M      1.53%\n",
      " CNN_for_Text.fc0             0.000M      0.00%\n",
      "  - aten.addmm                0.000M      0.00%\n",
      " CNN_for_Text.gcnn2        3424.666M      7.55%\n",
      "  - aten.addmm             1369.866M      3.02%\n",
      "  - aten.convolution       2054.799M      4.53%\n",
      " CNN_for_Text.fc1         21917.860M     48.31%\n",
      "  - aten.addmm            21917.860M     48.31%\n",
      " CNN_for_Text.fc2            33.554M      0.07%\n",
      "  - aten.addmm               33.554M      0.07%\n",
      " CNN_for_Text.fc_out          0.016M      0.00%\n",
      "  - aten.addmm                0.016M      0.00%\n"
     ]
    }
   ],
   "source": [
    "classifier_torch_model = CNN_for_Text(num_embedding=num_embedding, hidden_dim=hidden_dim, embedding_dim=embedding_dim, pos_emb_size=3072, dropout=0.2, num_out_features=len(class_id), seed=seed, random_edges=4, lattice_edges=4, lattice_step=2, virtual_nodes=0, lattice_start_distance=2, use_token_polarity=[True, False, True]).to(device)\n",
    "classifier_torch_model.eval()\n",
    "flopt_counter = FlopCounterMode(classifier_torch_model)\n",
    "with flopt_counter:\n",
    "    classifier_torch_model(X.to(device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
